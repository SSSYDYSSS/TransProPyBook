[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TransProPyBook",
    "section": "",
    "text": "Preface\nTo learn more about TransProPy visit https://github.com/SSSYDYSSS/TransProPy."
  },
  {
    "objectID": "index.html#utilsfunction",
    "href": "index.html#utilsfunction",
    "title": "TransProPyBook",
    "section": "UtilsFunction*",
    "text": "UtilsFunction*\n\n\n\n\n\n\nNote\n\n\n\n\nThese functions usually contain various utilities and helper functions, which can sometimes be considered as low-level functionalities.\n\n\n\n\n\nUtilsFunction1\n\n\n\nAuc.py\nAutoNorm.py\nFeatureRanking.py\nNewFeatureRanking.py\nLoadData.py\nPrintResults.py\nFilterSamples.py\nGeneNames.py\nGeneToFeatureMapping.py\n\n\n\nUtilsFunction2\n\n\n\nsplitdata.py\nLogTransform.py\n\n\n\nUtilsFunction3\n\n\n\nLoadFilterTranspose.py\nLoadEncodeLabels.py\nExtractCommonSamples.py\nLoadAndPreprocessData.py\nSetupLoggingAndProgressBar.py\nUpdateProgressBar.py\nLoggingCustomScorer.py\nTqdmCustomScorer.py\nTrainModel.py\nEnsembleForRFE.py\nSetupFeatureSelection.py\nPrintBoxedText.py\nExtractAndSaveResults.py"
  },
  {
    "objectID": "index.html#main-function",
    "href": "index.html#main-function",
    "title": "TransProPyBook",
    "section": "*main Function",
    "text": "*main Function\n\n\n\n\n\n\nTip\n\n\n\n\nThe main function constructed with the help of auxiliary functions.\n\n\n\n\n\nMACFCmain\n\n\n\nParameters\nReturns\nFunction Principle Explanation\nUsage Workflow\nUsage of MACFCmain (Significant correlation)\nUsage of MACFCmain (Insignificant correlation)\nReferences\n\n\n\nNewMACFCmain\n\n\n\nParameters\nReturns\nFunction Principle Explanation\nUsage Workflow\nUsage of New_MACFCmain (four_methods_degs_union)\nUsage of New_MACFCmain (all_count_exp)\nReferences\n\n\n\nAutogluonTimeLimit\n\n\n\nParameters\nReturns\nUsage of Autogluon_TimeLimit\nInsignificant Correlation\nSignificant Correlation\nReferences\n\n\n\nAutoGluonSelectML\n\n\n\nParameters\nReturns\nUsage of Autogluon_SelectML\nInsignificant Correlation\nSignificant Correlation\nReferences\n\n\n\nAutoFeatureSelection\n\n\n\nParameters\nPlease note\nDescription\nUsage"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "TransProPyBook",
    "section": "Citation",
    "text": "Citation\nYu Dongyue (2023). TransProPy: A python package that integrate algorithms and various machine learning approaches to extract features (genes) effective for classification and attribute them accordingly. https://github.com/SSSYDYSSS/TransProPy, https://sssydysss.gitbook.io/transpropy-manual/."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "TransProPyBook",
    "section": "License",
    "text": "License\nThis project is licensed under the BSD 3-Clause License - see the https://github.com/SSSYDYSSS/TransProPy/blob/main/LICENSE.txt file for details."
  },
  {
    "objectID": "UtilsFunction1.html#auc.py",
    "href": "UtilsFunction1.html#auc.py",
    "title": "1  UtilsFunction1",
    "section": "1.1 Auc.py",
    "text": "1.1 Auc.py\nAssists the MACFCmain function in calculating AUC, obtaining Feature Frequency, and performing sorting.\n\n1.1.1 Introduction\n\n\nIn this function, features that appear with high frequency indicate their presence in multiple optimal feature sets.\nEach optimal feature set is determined by calculating its Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), which is a common measure for evaluating classifier performance.\nDuring each iteration of the loop, an optimal feature set with the highest average AUC value is selected.\nFeatures from this set are then added to a rank list, known as ‘ranklist,’ and when necessary, also to a set named ‘rankset’.\n\n\n\n\n1.1.2 Usage\nauc(tlofe, ne, n0, n1)"
  },
  {
    "objectID": "UtilsFunction1.html#autonorm.py",
    "href": "UtilsFunction1.html#autonorm.py",
    "title": "1  UtilsFunction1",
    "section": "1.2 AutoNorm.py",
    "text": "1.2 AutoNorm.py\nNormalization Function The auto_norm function is designed to normalize a two-dimensional array (matrix). The purpose of normalization is generally to bring all features into the same numerical range, facilitating subsequent analysis or model training.\n\n1.2.1 Parameters\n\n\ndata: ndarray\n\nOrder Requirements for Input Data：\n1.This function does indeed have specific requirements for the row and column order of the input matrix data. Rows should represent individual samples, and columns should represent different features. In other words, each row vector represents a sample containing multiple features.\n2.Each column of the matrix will be independently normalized, so different features should be placed in separate columns.\n\n\n\n\n\n1.2.2 Returns\n\n\nnorm_data: ndarray\n\nIt is the normalized data.\n\n\n\n\n\n1.2.3 Usage\nauto_norm(data)"
  },
  {
    "objectID": "UtilsFunction1.html#featureranking.py",
    "href": "UtilsFunction1.html#featureranking.py",
    "title": "1  UtilsFunction1",
    "section": "1.3 FeatureRanking.py",
    "text": "1.3 FeatureRanking.py\n\n1.3.1 Introduction\n\n\nHigh-Frequency Features and Performance: Because features in each set are chosen based on their contribution to classifier performance, high-frequency features are likely to perform well. In other words, if a feature appears in multiple optimal feature sets, it may have a significant impact on the performance of the classifier.\nNote on Low-Frequency Features: However, it’s important to note that a low frequency of a feature does not necessarily mean it is unimportant. The importance of a feature may depend on how it combines with other features. Additionally, the outcome of feature selection may be influenced by the characteristics of the dataset and random factors. Therefore, the frequency provided by this function should only be used as a reference and is not an absolute indicator of feature performance.\n\n\n\n\n1.3.2 Returns\n\n\nFName\nFauc\nrankset\nranklist\n\n\n\n\n1.3.3 Usage\nfeature_ranking(f, c, max_rank, pos, neg, n0, n1)"
  },
  {
    "objectID": "UtilsFunction1.html#newfeatureranking.py",
    "href": "UtilsFunction1.html#newfeatureranking.py",
    "title": "1  UtilsFunction1",
    "section": "1.4 NewFeatureRanking.py",
    "text": "1.4 NewFeatureRanking.py\n\n1.4.1 Change Summary\n\n\nTo store features with AUC greater than AUC_threshold and their AUC values\nExclude features with AUC greater than AUC_threshold from the original set.\n Sort and process the remaining features\n\n\n\n\n1.4.2 Returns\n\n\nhigh_auc_features\nFName\nFauc\nrankset\nranklist\n\n\n\n\n1.4.3 Usage\nfeature_ranking(f, c, AUC_threshold, max_rank, pos, neg, n0, n1)"
  },
  {
    "objectID": "UtilsFunction1.html#loaddata.py",
    "href": "UtilsFunction1.html#loaddata.py",
    "title": "1  UtilsFunction1",
    "section": "1.5 LoadData.py",
    "text": "1.5 LoadData.py\nData Reading and Transformation.\n\n1.5.1 Introduction\n\n\nData normalization for constant value.\nExtract matrix data and categorical data.\n\n\n\n\n1.5.2 Parameters\n\n\nlable_name: string\n\nFor example: gender, age, altitude, temperature, quality, and other categorical variable names.\n\ndata_path: string\n\nFor example: ‘../data/gene_tpm.csv’\nPlease note: Preprocess the input data in advance to remove samples that contain too many missing values or zeros.\nThe input data matrix should have genes as rows and samples as columns.\n\nlabel_path: string\n\nFor example: ‘../data/tumor_class.csv’\nPlease note: The input CSV data should have rows representing sample names and columns representing class names.\nThe input sample categories must be in a numerical binary format, such as: 1,2,1,1,2,2,1.\nIn this case, the numerical values represent the following classifications: 1: male; 2: female.\n\nthreshold: float\n\nFor example: 0.9\nThe set threshold indicates the proportion of non-zero value samples to all samples in each feature.\n\n\n\n\n\n1.5.3 Returns\n\n\ntranspose(f): ndarray\n\nA transposed feature-sample matrix.\n\nc: ndarray\n\nA NumPy array containing classification labels.\n\n\n\n\n\n1.5.4 Usage\nload_data(\n    lable_name, \n    threshold, \n    data_path='../data/gene_tpm.csv', \n    label_path='../data/tumor_class.csv'\n    )"
  },
  {
    "objectID": "UtilsFunction1.html#printresults.py",
    "href": "UtilsFunction1.html#printresults.py",
    "title": "1  UtilsFunction1",
    "section": "1.6 PrintResults.py",
    "text": "1.6 PrintResults.py\n\n1.6.1 Returns\n\n\nfr: list of strings\n\nRepresenting ranked features.\n\nfre1: dictionary\n\nFeature names as keys and their frequencies as values.\n\nfrequency: list of tuples\n\nFeature names and their frequencies.\n\nlen(FName): integer\n\nCount of AUC values greater than 0.5.\n\nFName: array of strings\n\nFeature names after ranking with AUC &gt; 0.5.\n\nFauc: array of floats\n\nAUC values corresponding to the ranked feature names.\n\n\n\n\n\n1.6.2 Usage\n print_results(fr, fre1, frequency, len_FName, FName, Fauc)"
  },
  {
    "objectID": "UtilsFunction1.html#filtersamples.py",
    "href": "UtilsFunction1.html#filtersamples.py",
    "title": "1  UtilsFunction1",
    "section": "1.7 FilterSamples.py",
    "text": "1.7 FilterSamples.py\nRemove samples with high zero expression.\n\n1.7.1 Parameters\n\n\ndata_path: string\n\nFor example: ‘../data/gene_tpm.csv’\nPlease note: The input data matrix should have genes as rows and samples as columns.\n\nthreshold: float\n\nFor example: 0.9\nThe set threshold indicates the proportion of non-zero value samples to all samples in each feature.\n\n\n\n\n\n1.7.2 Return\n\n\nX: pandas.core.frame.DataFrame\n\n\n\n\n1.7.3 Usage\nfilter_samples(threshold, data_path='../data/gene_tpm.csv')"
  },
  {
    "objectID": "UtilsFunction1.html#genenames.py",
    "href": "UtilsFunction1.html#genenames.py",
    "title": "1  UtilsFunction1",
    "section": "1.8 GeneNames.py",
    "text": "1.8 GeneNames.py\nExtract gene_names data.\n\n1.8.1 Parameters\n\n\ndata_path: string\n\nFor example: ‘../data/gene_tpm.csv’\nPlease note: Preprocess the input data in advance to remove samples that contain too many missing values or zeros.\nThe input data matrix should have genes as rows and samples as columns.\n\n\n\n\n\n1.8.2 Return\n\n\ngene_names: list\n\n\n\n\n1.8.3 Usage\ngene_name(data_path='../data/gene_tpm.csv')"
  },
  {
    "objectID": "UtilsFunction1.html#genetofeaturemapping.py",
    "href": "UtilsFunction1.html#genetofeaturemapping.py",
    "title": "1  UtilsFunction1",
    "section": "1.9 GeneToFeatureMapping.py",
    "text": "1.9 GeneToFeatureMapping.py\ngene map feature.\n\n1.9.1 Parameters\n\n\ngene_names: list\n\nFor example: [‘GeneA’, ‘GeneB’, ‘GeneC’, ‘GeneD’, ‘GeneE’]\ncontaining strings\n\nranked_features: list\n\nFor example: [2, 0, 1]\ncontaining integers\n\n\n\n\n\n1.9.2 Return\n\n\ngene_to_feature_mapping: dictionary\n\ngene_to_feature_mapping is a Python dictionary type. It is used to map gene names to their corresponding feature (or ranked feature) names.\n\n\n\n\n\n1.9.3 Usage\ngene_map_feature(gene_names, ranked_features)"
  },
  {
    "objectID": "UtilsFunction1.html#references",
    "href": "UtilsFunction1.html#references",
    "title": "1  UtilsFunction1",
    "section": "1.10 References",
    "text": "1.10 References\n\n\nSu,Y., Du,K., Wang,J., Wei,J. and Liu,J. (2022) Multi-variable AUC for sifting complementary features and its biomedical application. Briefings in Bioinformatics, 23, bbac029."
  },
  {
    "objectID": "UtilsFunction2.html#splitdata.py",
    "href": "UtilsFunction2.html#splitdata.py",
    "title": "2  UtilsFunction2",
    "section": "2.1 splitdata.py",
    "text": "2.1 splitdata.py\nReads the gene expression and class data, processes it, and splits it into training and testing sets.\n\n2.1.1 Parameters\n\n\ngene_data_path (str):\n\nPath to the CSV file containing the gene expression data.\nFor example: ‘../data/gene_tpm.csv’\n\nclass_data_path (str):\n\nPath to the CSV file containing the class data.\nFor example: ‘../data/tumor_class.csv’\n\nclass_name (str):\n\nThe name of the class column in the class data.\n\ntest_size (float, optional):\n\nThe proportion of the data to be used as the testing set.\nDefault is 0.2.\n\nrandom_state (int, optional):\n\nThe seed used by the random number generator.\nDefault is 42.\n\nthreshold (float, optional):\n\nThe threshold used to filter out rows based on the proportion of non-zero values.\nDefault is 0.9.\n\nrandom_feature (int, optional):\n\nThe number of random feature to select. If None, no random feature selection is performed.\nDefault is None.\n\n\n\n\n\n2.1.2 Returns\n\n\ntrain_data (pd.DataFrame):\n\nThe training data.\n\ntest_data (pd.DataFrame):\n\nThe testing data.\n\n\n\n\n\n2.1.3 Usage\nsplit_data(\n    gene_data_path='../data/gene_tpm.csv', \n    class_data_path='../data/tumor_class.csv', \n    class_name, \n    test_size=0.2, \n    random_state=42, \n    threshold=0.9, \n    random_feature=None\n    )"
  },
  {
    "objectID": "UtilsFunction2.html#logtransform.py",
    "href": "UtilsFunction2.html#logtransform.py",
    "title": "2  UtilsFunction2",
    "section": "2.2 LogTransform.py",
    "text": "2.2 LogTransform.py\nEvaluate and potentially apply log2 transformation to data. - This function checks data against a set of criteria to determine if a log2 transformation is needed, applying the transformation if necessary.\n\n2.2.1 Parameters\n\n\ndata (np.ndarray):\n\nA numerical numpy array.\n\n\n\n\n\n2.2.2 Returns\n\n\nresult np.ndarray\n\nThe original data or the data transformed with log2.\n\n\n\n\n\n2.2.3 Usage\nlog_transform(\n    data\n    )"
  },
  {
    "objectID": "UtilsFunction3.html#loadfiltertranspose.py",
    "href": "UtilsFunction3.html#loadfiltertranspose.py",
    "title": "3  UtilsFunction3",
    "section": "3.1 LoadFilterTranspose.py",
    "text": "3.1 LoadFilterTranspose.py\nRemove samples with high zero expression.\n\n3.1.1 Parameters\n\n\ndata_path: string:\n\nFor example: ‘../data/gene_tpm.csv’\nPlease note: The input data matrix should have genes as rows and samples as columns.\n\nthreshold: float:\n\nFor example: 0.9\nThe set threshold indicates the proportion of non-zero value samples to all samples in each feature.\n\n\n\n\n\n3.1.2 Returns\n\n\nX (pandas.core.frame.DataFrame):\n\n\n\n\n3.1.3 Usage\nload_filter_transpose(\n    threshold=0.9, \n    data_path='../data/gene_tpm.csv'\n    )"
  },
  {
    "objectID": "UtilsFunction3.html#loadencodelabels.py",
    "href": "UtilsFunction3.html#loadencodelabels.py",
    "title": "3  UtilsFunction3",
    "section": "3.2 LoadEncodeLabels.py",
    "text": "3.2 LoadEncodeLabels.py\nReads a CSV file containing labels and encodes categorical labels in the specified column to numeric labels.\n\n3.2.1 Parameters\n\n\nfile_path (str):\n\nPath to the CSV file containing labels.\n\ncolumn_name (str):\n\nName of the column to be encoded.\n\n\n\n\n\n3.2.2 Returns\n\n\nY (pd.DataFrame):\n\nA DataFrame containing the encoded numeric labels.\n\n\n\n\n\n3.2.3 Usage\nload_encode_labels(\n    file_path='../data/class.csv', \n    column_name='class'\n    )"
  },
  {
    "objectID": "UtilsFunction3.html#extractcommonsamples.py",
    "href": "UtilsFunction3.html#extractcommonsamples.py",
    "title": "3  UtilsFunction3",
    "section": "3.3 ExtractCommonSamples.py",
    "text": "3.3 ExtractCommonSamples.py\nExtracts common samples (rows) from two DataFrames based on their indices.\n\n3.3.1 Parameters\n\n\nX (pd.DataFrame):\n\nFirst DataFrame.\n\nY (pd.DataFrame):\n\nSecond DataFrame.\n\n\n\n\n\n3.3.2 Returns\n\n\nX_common, Y_common (pd.DataFrame):\n\nTwo DataFrames containing only the rows that are common in both.\n\n\n\n\n\n3.3.3 Usage\nextract_common_samples(\n    X, \n    Y\n    )"
  },
  {
    "objectID": "UtilsFunction3.html#loadandpreprocessdata.py",
    "href": "UtilsFunction3.html#loadandpreprocessdata.py",
    "title": "3  UtilsFunction3",
    "section": "3.4 LoadAndPreprocessData.py",
    "text": "3.4 LoadAndPreprocessData.py\nLoad and preprocess the data.\n\n3.4.1 Parameters\n\n\nfeature_file: str:\n\nPath to the feature data file.\n\nlabel_file: str:\n\nPath to the label data file.\n\nlabel_column: str:\n\nColumn name of the labels in the label file.\n\nthreshold: float:\n\nThreshold for filtering in load_filter_transpose function.\n\n\n\n\n\n3.4.2 Returns\n\n\nX (DataFrame):\n\nPreprocessed feature data.\n\nY (ndarray):\n\nPreprocessed label data.\n\n\n\n\n\n3.4.3 Usage\nload_and_preprocess_data(\n    feature_file, \n    label_file, \n    label_column, \n    threshold\n    )"
  },
  {
    "objectID": "UtilsFunction3.html#setuploggingandprogressbar.py",
    "href": "UtilsFunction3.html#setuploggingandprogressbar.py",
    "title": "3  UtilsFunction3",
    "section": "3.5 SetupLoggingAndProgressBar.py",
    "text": "3.5 SetupLoggingAndProgressBar.py\nSet up logging and initialize a tqdm progress bar.\n\n3.5.1 Parameters\n\n\nn_iter (int):\n\nNumber of iterations for RandomizedSearchCV.\n\nn_cv (int):\n\nNumber of cross-validation folds.\n\n\n\n\n\n3.5.2 Returns\n\n\ntqdm object\n\nAn initialized tqdm progress bar.\n\n\n\n\n\n3.5.3 Usage\nsetup_logging_and_progress_bar(\n    n_iter, \n    n_cv\n    )"
  },
  {
    "objectID": "UtilsFunction3.html#updateprogressbar.py",
    "href": "UtilsFunction3.html#updateprogressbar.py",
    "title": "3  UtilsFunction3",
    "section": "3.6 UpdateProgressBar.py",
    "text": "3.6 UpdateProgressBar.py\nRead the number of log entries in the log file and update the tqdm progress bar.\n\n3.6.1 Parameters\n\n\npbar (tqdm):\n\nThe tqdm progress bar object.\n\nlog_file (str):\n\nPath to the log file, default is ‘progress.log’.\n\n\n\n\n\n3.6.2 Usage\nupdate_progress_bar(\n    pbar, \n    log_file='progress.log'\n    )"
  },
  {
    "objectID": "UtilsFunction3.html#loggingcustomscorer.py",
    "href": "UtilsFunction3.html#loggingcustomscorer.py",
    "title": "3  UtilsFunction3",
    "section": "3.7 LoggingCustomScorer.py",
    "text": "3.7 LoggingCustomScorer.py\nCreates a custom scorer function for use in model evaluation processes. This scorer logs both the accuracy score and the time taken for each call.\n\n3.7.1 Parameters\n\n\nn_iter (int):\n\nNumber of iterations for the search process. Default is 10.\n\nn_cv (int):\n\nNumber of cross-validation splits. Default is 5.\n\n\n\n\n\n3.7.2 Returns\n\n\ncustom_scorer(function)\n\nA custom scorer function that logs the accuracy score and time taken for each call.\n\n\n\n\n\n3.7.3 Usage\nlogging_custom_scorer(\n    n_iter=10, \n    n_cv=5\n    )"
  },
  {
    "objectID": "UtilsFunction3.html#tqdmcustomscorer.py",
    "href": "UtilsFunction3.html#tqdmcustomscorer.py",
    "title": "3  UtilsFunction3",
    "section": "3.8 TqdmCustomScorer.py",
    "text": "3.8 TqdmCustomScorer.py\n\nCreates a custom scorer for model evaluation, integrating a progress bar with tqdm.\n\n\n3.8.1 Parameters\n\n\nn_iter: int (optional):\n\nNumber of iterations for the search process. Default is 10.\n\nn_cv: int (optional):\n\nNumber of cross-validation splits. Default is 5.\n\n\n\n\n\n3.8.2 Returns\n\n\nfunction:\n\nA custom scorer function that can be used with model evaluation methods like RandomizedSearchCV.\n\n\n\n\n\n3.8.3 Description\n\nThe tqdm_custom_scorer function creates a scorer for model evaluation, incorporating a tqdm progress bar to monitor the evaluation process. This scorer is especially useful in processes like RandomizedSearchCV, where it provides real-time feedback on the number of iterations and cross-validation steps completed.\n\n\n\n3.8.4 Usage\ncustom_scorer = tqdm_custom_scorer(n_iter=10, n_cv=5)\n# Use this scorer in RandomizedSearchCV or similar methods"
  },
  {
    "objectID": "UtilsFunction3.html#trainmodel.py",
    "href": "UtilsFunction3.html#trainmodel.py",
    "title": "3  UtilsFunction3",
    "section": "3.9 TrainModel.py",
    "text": "3.9 TrainModel.py\nSet up and run the model training process.\n\n3.9.1 Parameters\n\n\nX: DataFrame:\n\nfeature data.\n\nY: ndarray:\n\nlabel data.\n\nfeature_selection:\n\nFeatureUnion, the feature selection process.\n\nparameters: dict:\n\nparameters for RandomizedSearchCV.\n\nn_iter: int:\n\nnumber of iterations for RandomizedSearchCV.\n\nn_cv: int:\n\nnumber of cross-validation folds.\n\nn_jobs: int:\n\nnumber of jobs to run in parallel (default is 9).\n\n\n\n\n\n3.9.2 Returns\n\n\nclf\n\nRandomizedSearchCV object after fitting.\n\n\n\n\n\n3.9.3 Usage\ntrain_model(\n    X, \n    Y, \n    feature_selection, \n    parameters, \n    n_iter, \n    n_cv, \n    n_jobs=9\n    )"
  },
  {
    "objectID": "UtilsFunction3.html#ensembleforrfe.py",
    "href": "UtilsFunction3.html#ensembleforrfe.py",
    "title": "3  UtilsFunction3",
    "section": "3.10 EnsembleForRFE.py",
    "text": "3.10 EnsembleForRFE.py\nSet up and run the Ensemble model for Recursive Feature Elimination.\n\n3.10.1 Parameters\n\n\nsvm_C: float:\nRegularization parameter for SVM.\ntree_max_depth: int:\nMaximum depth of the decision tree.\ntree_min_samples_split: int:\nMinimum number of samples required to split an internal node.\ngbm_learning_rate: float:\nLearning rate for gradient boosting.\ngbm_n_estimators: int:\nNumber of boosting stages for gradient boosting.\n\n\n\n\n3.10.2 Attributes\n\n\nfeature_importances_:\nArray of feature importances after fitting the model.\n\n\n\n\n3.10.3 Methods\n\n\nfit(X, y):\nFit the model to data matrix X and target(s) y.\npredict(X):\nPredict class labels for samples in X.\nset_params(**params):\nSet parameters for the ensemble estimator."
  },
  {
    "objectID": "UtilsFunction3.html#setupfeatureselection.py",
    "href": "UtilsFunction3.html#setupfeatureselection.py",
    "title": "3  UtilsFunction3",
    "section": "3.11 SetupFeatureSelection.py",
    "text": "3.11 SetupFeatureSelection.py\n\nSet up the feature selection process in TransProPy.UtilsFunction3. This function is particularly useful for setting up a feature selection pipeline, especially in models that benefit from ensemble methods and mutual information-based feature selection.\n\n\n3.11.1 Returns\n\n\nfeature_selection: FeatureUnion:\n\nA combined feature selection process.\n\n\n\n\n\n3.11.2 Description\n\nThe setup_feature_selection function initializes and returns a FeatureUnion object for feature selection. This union includes: - RFECV: Utilizes an EnsembleForRFE estimator with StratifiedKFold(5) for cross-validation, focusing on accuracy. - SelectKBest: Applies mutual_info_classif for feature scoring. The combination of these techniques provides a robust approach to feature selection in machine learning models.\n\n\n\n3.11.3 Usage\n\nfeature_selection = setup_feature_selection()"
  },
  {
    "objectID": "UtilsFunction3.html#printboxedtext.py",
    "href": "UtilsFunction3.html#printboxedtext.py",
    "title": "3  UtilsFunction3",
    "section": "3.12 PrintBoxedText.py",
    "text": "3.12 PrintBoxedText.py\n\nPrints a title in a boxed format in the console output.\n\n\n3.12.1 Parameters\n\n\ntitle: str:\n\nThe text to be displayed inside the box.\n\n\n\n\n\n3.12.2 Returns\n\n\nNone. This function directly prints the formatted title to the console.\n\n\n\n\n3.12.3 Description\n\nThis function creates a box around the given title text using hash (#) and equals (=) symbols. It prints the title with a border on the top and bottom, making it stand out in the console output. The border line consists of a hash symbol, followed by equals symbols the length of the title plus two (for padding), and then another hash symbol.\n\n\n\n3.12.4 Usage Example\n\nprint_boxed_text(\"Example Title\")"
  },
  {
    "objectID": "UtilsFunction3.html#extractandsaveresults.py",
    "href": "UtilsFunction3.html#extractandsaveresults.py",
    "title": "3  UtilsFunction3",
    "section": "3.13 ExtractAndSaveResults.py",
    "text": "3.13 ExtractAndSaveResults.py\n\nThe function uses matplotlib for plotting, pandas for data handling, and a custom print_boxed_text function for formatted output.\n\n\n3.13.1 Parameters\n\n\nclf: trained model (RandomizedSearchCV object):\n\nThe classifier object after training.\n\nX: DataFrame:\n\nFeature data used for training.\n\nsave_path: str:\n\nBase path for saving results.\n\nshow_plot: bool (optional):\n\nWhether to display the plot. Default is False.\n\nuse_tkagg: bool (optional):\n\nWhether to use ‘TkAgg’ backend for matplotlib. Generally, choose False when using in PyCharm IDE, and choose True when rendering file.qmd to an HTML file.\n\n\n\n\n\n3.13.2 Description\n\nThis function performs a comprehensive analysis and extraction of results from a trained model. It includes: - Extracting and plotting cross-validation results. - Identifying and printing features selected by RFECV and SelectKBest. - Combining and saving selected features in a CSV file. - Extracting and saving feature importances from EnsembleForRFE. - Extracting and saving scores from SelectKBest.\n\n\n\n3.13.3 Usage\n\nextract_and_save_results(clf, X, \"path/to/save/\", show_plot=True)"
  },
  {
    "objectID": "MACFCmain.html#parameters-a",
    "href": "MACFCmain.html#parameters-a",
    "title": "4  MACFCmain.py",
    "section": "4.1 Parameters",
    "text": "4.1 Parameters\n\n\nmax_rank: int\n\nThe total number of gene combinations you want to obtain.\n\nlable_name: string\n\nFor example: gender, age, altitude, temperature, quality, and other categorical variable names.\n\ndata_path: string\n\nFor example: ‘../data/gene_tpm.csv’\nPlease note: Preprocess the input data in advance to remove samples that contain too many missing values or zeros.\nThe input data matrix should have genes as rows and samples as columns.\n\nlabel_path: string\n\nFor example: ‘../data/tumor_class.csv’\nPlease note: The input sample categories must be in a numerical binary format, such as: 1,2,1,1,2,2,1.\nIn this case, the numerical values represent the following classifications: 1: male; 2: female.\n\nthreshold: float\n\nFor example: 0.9\nThe set threshold indicates the proportion of non-zero value samples to all samples in each feature."
  },
  {
    "objectID": "MACFCmain.html#returns-a",
    "href": "MACFCmain.html#returns-a",
    "title": "4  MACFCmain.py",
    "section": "4.2 Returns",
    "text": "4.2 Returns\n\n\nfr: list of strings\n\nRepresenting ranked features.\n\nfre1: dictionary\n\nFeature names as keys and their frequencies as values.\n\nfrequency: list of tuples\n\nFeature names and their frequencies.\nThe frequency outputs a list sorted by occurrence frequency (in descending order). This list includes only those elements from the dictionary fre1 (which represents the counted frequencies of elements in the original data) that have an occurrence frequency greater than once, along with their frequencies.\n\nlen(FName): integer\n\nCount of AUC values greater than 0.5.\n\nFName: array of strings\n\nFeature names after ranking with AUC &gt; 0.5.\n\nFauc: array of floats\n\nAUC values corresponding to the ranked feature names."
  },
  {
    "objectID": "MACFCmain.html#function-principle-explanation-a",
    "href": "MACFCmain.html#function-principle-explanation-a",
    "title": "4  MACFCmain.py",
    "section": "4.3 Function Principle Explanation",
    "text": "4.3 Function Principle Explanation\n\nFeature Frequency and AUC: In this function, features that appear with high frequency indicate their presence in multiple optimal feature sets. Each optimal feature set is determined by calculating its Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), which is a common measure for evaluating classifier performance. During each iteration of the loop, an optimal feature set with the highest average AUC value is selected. Features from this set are then added to a rank list, known as ‘ranklist,’ and when necessary, also to a set named ‘rankset’.\nHigh-Frequency Features and Performance: Because features in each set are chosen based on their contribution to classifier performance, high-frequency features are likely to perform well. In other words, if a feature appears in multiple optimal feature sets, it may have a significant impact on the performance of the classifier.\nNote on Low-Frequency Features: However, it’s important to note that a low frequency of a feature does not necessarily mean it is unimportant. The importance of a feature may depend on how it combines with other features. Additionally, the outcome of feature selection may be influenced by the characteristics of the dataset and random factors. Therefore, the frequency provided by this function should only be used as a reference and is not an absolute indicator of feature performance.\nFurther Evaluation Methods: If you wish to explore feature performance more deeply, you may need to employ other methods for assessing feature importance. This could include model-based importance metrics or statistical tests to evaluate the relationship between features and the target variable."
  },
  {
    "objectID": "MACFCmain.html#usage-workflow-a",
    "href": "MACFCmain.html#usage-workflow-a",
    "title": "4  MACFCmain.py",
    "section": "4.4 Usage Workflow",
    "text": "4.4 Usage Workflow\n\n\nFName is a list of feature names sorted based on their AUC (Area Under the Curve) values. In this sorting method, the primary consideration is the AUC value, followed by the feature name. All features included in FName have an AUC value greater than 0.5.\nfr is the result of another sorting method. In this method, the primary consideration is the “combined” AUC of the features, followed by their individual AUC values. This means that some features, despite having lower individual AUC values, may produce a higher combined AUC when paired with other features. Therefore, their position in the fr list may be higher than in the FName list.\nThe code for fr employs a more complex logic to select and combine features to optimize their combined AUC values. In this process, features are not solely selected and sorted based on their individual AUC values; the effect of their combination with other features is also considered. Consequently, the sorting logic for fr (or rankset) differs from that of FName.\nPlease note: While the code takes into account both individual AUC values and combined AUC values, the sorting of the fr list (i.e., rankset) initially starts based on individual AUC values. This is because at the beginning of each external loop iteration, the first element of fs is the next feature sorted by its individual AUC value. The list is then further optimized by evaluating the combination effects with other features."
  },
  {
    "objectID": "MACFCmain.html#usage-of-macfmain-significant-correlation",
    "href": "MACFCmain.html#usage-of-macfmain-significant-correlation",
    "title": "4  MACFCmain.py",
    "section": "4.5 Usage of MACFCmain (Significant correlation)",
    "text": "4.5 Usage of MACFCmain (Significant correlation)\nThis function uses the MACFC method to select feature genes relevant to classification and ranks them based on their corresponding weights.\n\n\nPlease note:Data characteristics: Features have strong correlation with the classification.\n\n\n\n4.5.1 Import the corresponding module\n\nimport TransProPy.MACFCmain as Tr\nimport TransProPy.UtilsFunction1.GeneNames as TUG\nimport TransProPy.UtilsFunction1.GeneToFeatureMapping as TUGM\n\n\n\n\n4.5.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/four_methods_degs_intersection.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n  Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0        A2M         16.808499         16.506184         17.143433   \n1      A2ML1          1.584963          9.517669          7.434628   \n2      AADAC          4.000000          2.584963          1.584963   \n3    AADACL2          1.000000          1.000000          0.000000   \n4     ABCA12          4.523562          4.321928          3.906891   \n5    ABCA17P          4.584963          5.169925          3.807355   \n6      ABCA9          9.753217          6.906891          3.459432   \n7      ABCB4          9.177420          6.700440          5.000000   \n8      ABCB5         10.134426          4.169925          9.167418   \n9     ABCC11         10.092757          6.491853          5.459432   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0         17.760739         14.766839         16.263691         16.035207   \n1          2.584963          1.584963          2.584963          5.285402   \n2          0.000000          0.000000          0.000000          3.321928   \n3          0.000000          1.000000          0.000000          0.000000   \n4          3.459432          1.584963          3.000000          4.321928   \n5          8.366322          7.228819          7.076816          4.584963   \n6          2.584963          6.357552          6.475733          7.330917   \n7          9.342075         10.392317          7.383704         11.032735   \n8          4.906891         11.340963          3.169925         11.161762   \n9          6.807355          4.247928          5.459432          5.977280   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0         18.355114         16.959379  \n1          2.584963          3.584963  \n2          1.000000          4.584963  \n3          0.000000          1.000000  \n4          4.807355          3.700440  \n5          6.409391          7.139551  \n6          7.954196          9.177420  \n7         10.082149         10.088788  \n8          4.643856         12.393927  \n9          5.614710          8.233620  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      2\n5  TCGA-GN-A26A-06A      2\n6  TCGA-D3-A3BZ-06A      2\n7  TCGA-D3-A51G-06A      2\n8  TCGA-EE-A29R-06A      2\n9  TCGA-D3-A2JE-06A      2\n\n\n\n\n\n4.5.3 MACFCmain\n\nranked_features, fre1, frequency, len_FName, FName, Fauc = Tr.MACFCmain(\n    100, \n    \"class\", \n    0.95, \n    data_path='../test_TransProPy/data/four_methods_degs_intersection.csv', \n    label_path='../test_TransProPy/data/class.csv'\n    )\n\n\n\n\n4.5.4 Result\n\n# Print the first 20 Ranked Features\nprint(\"\\nFirst 20 Ranked Features:\")\nfor i, feature in enumerate(ranked_features[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst 20 Ranked Features:\n1. 355\n2. 68\n3. 867\n4. 78\n5. 97\n6. 90\n7. 432\n8. 313\n9. 497\n10. 511\n11. 66\n12. 172\n13. 544\n14. 1162\n15. 487\n16. 317\n17. 1283\n18. 930\n19. 1290\n20. 1170\n\n\n\n\n# Print the first 20 Feature Frequencies (fre1)\nprint(\"\\nFirst 20 Feature Frequencies:\")\nfor i, (feature, freq) in enumerate(list(fre1.items())[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFirst 20 Feature Frequencies:\n1. Feature: 355, Frequency: 1\n2. Feature: 68, Frequency: 1\n3. Feature: 867, Frequency: 1\n4. Feature: 78, Frequency: 1\n5. Feature: 97, Frequency: 1\n6. Feature: 90, Frequency: 1\n7. Feature: 432, Frequency: 1\n8. Feature: 313, Frequency: 1\n9. Feature: 497, Frequency: 1\n10. Feature: 511, Frequency: 1\n11. Feature: 66, Frequency: 1\n12. Feature: 172, Frequency: 1\n13. Feature: 544, Frequency: 1\n14. Feature: 1162, Frequency: 1\n15. Feature: 487, Frequency: 1\n16. Feature: 317, Frequency: 1\n17. Feature: 1283, Frequency: 1\n18. Feature: 930, Frequency: 1\n19. Feature: 1290, Frequency: 1\n20. Feature: 1170, Frequency: 1\n\n\n\n\n# Print the Features with a frequency greater than 1 \nprint(\"\\nFeatures with a frequency greater than 1 :\")\nfor i, (feature, freq) in enumerate(frequency[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFeatures with a frequency greater than 1 :\n\n\n\n\n# Print the length of FName (len_FName)\nprint(\"\\nCount of Features with AUC &gt; 0.5 (len_FName):\")\nprint(len_FName)\n\n\nCount of Features with AUC &gt; 0.5 (len_FName):\n1\n\n\n\n\n# Print the first 10 Features with AUC &gt; 0.5 (FName)\nprint(\"\\nFirst few Features with AUC &gt; 0.5:\")\nfor i, feature in enumerate(FName[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst few Features with AUC &gt; 0.5:\n1. 355\n\n\n\n\n# Print the first 10 AUC Values for Ranked Features (Fauc)\nprint(\"\\nFirst few AUC Values for Ranked Features:\")\nfor i, auc in enumerate(Fauc[:20], 1):\n    print(f\"{i}. AUC: {auc}\")\n\n\nFirst few AUC Values for Ranked Features:\n1. AUC: 1.0\n\n\n\n\n\n4.5.5 gene_name\n\ngene_names = TUG.gene_name(data_path='../test_TransProPy/data/four_methods_degs_intersection.csv')\n\n\n\n# Print the first 20 gene names\nprint(\"First 20 Gene Names:\")\nfor i, gene_name in enumerate(gene_names[:20], 1):\n    print(f\"{i}. {gene_name}\")\n\nFirst 20 Gene Names:\n1. A2M\n2. A2ML1\n3. AADAC\n4. AADACL2\n5. ABCA12\n6. ABCA17P\n7. ABCA9\n8. ABCB4\n9. ABCB5\n10. ABCC11\n11. ABCC3\n12. ABCD1\n13. ABI3BP\n14. AC002116.8\n15. AC002398.9\n16. AC004057.1\n17. AC004231.2\n18. AC004540.5\n19. AC004623.3\n20. AC004951.5\n\n\n\n\n\n4.5.6 gene_map_feature\n\ngene_to_feature_mapping = TUGM.gene_map_feature(gene_names, ranked_features)\n\n\n\n4.5.6.1 AUC&gt;0.5\n\nimport numpy as np\n# Generating gene_to_feature_mapping\ngene_to_feature_mapping = {}\nfor gene, feature in zip(gene_names, FName):\n    # Find the index of the feature in FName\n    index = np.where(FName == feature)[0][0]\n    # Find the corresponding AUC value using the index\n    auc_value = Fauc[index]\n    # Store the gene name, feature name, and AUC value in the mapping\n    gene_to_feature_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 mappings\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_to_feature_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: A2M, Feature: 355, AUC: 1.0"
  },
  {
    "objectID": "MACFCmain.html#usage-of-macfmain-insignificant-correlation",
    "href": "MACFCmain.html#usage-of-macfmain-insignificant-correlation",
    "title": "4  MACFCmain.py",
    "section": "4.6 Usage of MACFCmain (Insignificant correlation)",
    "text": "4.6 Usage of MACFCmain (Insignificant correlation)\nThis function uses the MACFC method to select feature genes relevant to classification and ranks them based on their corresponding weights.\n\n\nPlease note:Data characteristics: Features have weak correlation with the classification.\nRandomly shuffling the class labels to a certain extent simulates reducing the correlation.\n\n\n\n\n4.6.1 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/four_methods_degs_intersection.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n  Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0        A2M         16.808499         16.506184         17.143433   \n1      A2ML1          1.584963          9.517669          7.434628   \n2      AADAC          4.000000          2.584963          1.584963   \n3    AADACL2          1.000000          1.000000          0.000000   \n4     ABCA12          4.523562          4.321928          3.906891   \n5    ABCA17P          4.584963          5.169925          3.807355   \n6      ABCA9          9.753217          6.906891          3.459432   \n7      ABCB4          9.177420          6.700440          5.000000   \n8      ABCB5         10.134426          4.169925          9.167418   \n9     ABCC11         10.092757          6.491853          5.459432   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0         17.760739         14.766839         16.263691         16.035207   \n1          2.584963          1.584963          2.584963          5.285402   \n2          0.000000          0.000000          0.000000          3.321928   \n3          0.000000          1.000000          0.000000          0.000000   \n4          3.459432          1.584963          3.000000          4.321928   \n5          8.366322          7.228819          7.076816          4.584963   \n6          2.584963          6.357552          6.475733          7.330917   \n7          9.342075         10.392317          7.383704         11.032735   \n8          4.906891         11.340963          3.169925         11.161762   \n9          6.807355          4.247928          5.459432          5.977280   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0         18.355114         16.959379  \n1          2.584963          3.584963  \n2          1.000000          4.584963  \n3          0.000000          1.000000  \n4          4.807355          3.700440  \n5          6.409391          7.139551  \n6          7.954196          9.177420  \n7         10.082149         10.088788  \n8          4.643856         12.393927  \n9          5.614710          8.233620  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/random_classification_class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      1\n5  TCGA-GN-A26A-06A      1\n6  TCGA-D3-A3BZ-06A      1\n7  TCGA-D3-A51G-06A      1\n8  TCGA-EE-A29R-06A      1\n9  TCGA-D3-A2JE-06A      1\n\n\n\n\n\n4.6.2 MACFCmain\n\nranked_features, fre1, frequency, len_FName, FName, Fauc = Tr.MACFCmain(\n    100, \n    \"class\", \n    0.95, \n    data_path='../test_TransProPy/data/four_methods_degs_intersection.csv', \n    label_path='../test_TransProPy/data/random_classification_class.csv'\n    )\n\n\n\n\n4.6.3 Result\n\n# Print the first 20 Ranked Features\nprint(\"\\nFirst 20 Ranked Features:\")\nfor i, feature in enumerate(ranked_features[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst 20 Ranked Features:\n1. 1147\n2. 605\n3. 140\n4. 845\n5. 546\n6. 1052\n7. 188\n8. 431\n9. 182\n10. 120\n11. 362\n12. 998\n13. 1122\n14. 246\n15. 23\n16. 383\n17. 258\n18. 189\n19. 746\n20. 1064\n\n\n\n\n# Print the first 20 Feature Frequencies (fre1)\nprint(\"\\nFirst 20 Feature Frequencies:\")\nfor i, (feature, freq) in enumerate(list(fre1.items())[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFirst 20 Feature Frequencies:\n1. Feature: 1147, Frequency: 1\n2. Feature: 605, Frequency: 1\n3. Feature: 140, Frequency: 1\n4. Feature: 845, Frequency: 1\n5. Feature: 546, Frequency: 1\n6. Feature: 1052, Frequency: 1\n7. Feature: 188, Frequency: 1\n8. Feature: 431, Frequency: 1\n9. Feature: 182, Frequency: 1\n10. Feature: 120, Frequency: 1\n11. Feature: 362, Frequency: 1\n12. Feature: 998, Frequency: 1\n13. Feature: 1122, Frequency: 1\n14. Feature: 246, Frequency: 1\n15. Feature: 23, Frequency: 1\n16. Feature: 383, Frequency: 1\n17. Feature: 258, Frequency: 1\n18. Feature: 189, Frequency: 1\n19. Feature: 746, Frequency: 1\n20. Feature: 1064, Frequency: 1\n\n\n\n\n# Print the Features with a frequency greater than 1 \nprint(\"\\nFeatures with a frequency greater than 1 :\")\nfor i, (feature, freq) in enumerate(frequency[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFeatures with a frequency greater than 1 :\n\n\n\n\n# Print the length of FName (len_FName)\nprint(\"\\nCount of Features with AUC &gt; 0.5 (len_FName):\")\nprint(len_FName)\n\n\nCount of Features with AUC &gt; 0.5 (len_FName):\n757\n\n\n\n\n# Print the first 10 Features with AUC &gt; 0.5 (FName)\nprint(\"\\nFirst few Features with AUC &gt; 0.5:\")\nfor i, feature in enumerate(FName[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst few Features with AUC &gt; 0.5:\n1. 1147\n2. 140\n3. 605\n4. 518\n5. 1080\n6. 826\n7. 541\n8. 695\n9. 1266\n10. 0\n11. 864\n12. 188\n13. 842\n14. 344\n15. 824\n16. 1208\n17. 1086\n18. 602\n19. 295\n20. 1261\n\n\n\n\n# Print the first 10 AUC Values for Ranked Features (Fauc)\nprint(\"\\nFirst few AUC Values for Ranked Features:\")\nfor i, auc in enumerate(Fauc[:20], 1):\n    print(f\"{i}. AUC: {auc}\")\n\n\nFirst few AUC Values for Ranked Features:\n1. AUC: 0.6469530885995243\n2. AUC: 0.6465975134260281\n3. AUC: 0.6447509747664238\n4. AUC: 0.6415704161455651\n5. AUC: 0.6405110473528042\n6. AUC: 0.6403761740111332\n7. AUC: 0.6400941661149121\n8. AUC: 0.6398293239167219\n9. AUC: 0.6387871208219917\n10. AUC: 0.6381814169057604\n11. AUC: 0.6376174011133181\n12. AUC: 0.6373378454596729\n13. AUC: 0.6371956153902744\n14. AUC: 0.6371931631476986\n15. AUC: 0.6370018882267834\n16. AUC: 0.6367713774246548\n17. AUC: 0.636531057652223\n18. AUC: 0.6361656735084235\n19. AUC: 0.6359670418597808\n20. AUC: 0.6358419774884132\n\n\n\n\n\n4.6.4 gene_name\n\ngene_names = TUG.gene_name(data_path='../test_TransProPy/data/four_methods_degs_intersection.csv')\n\n\n\n# Print the first 20 gene names\nprint(\"First 20 Gene Names:\")\nfor i, gene_name in enumerate(gene_names[:20], 1):\n    print(f\"{i}. {gene_name}\")\n\nFirst 20 Gene Names:\n1. A2M\n2. A2ML1\n3. AADAC\n4. AADACL2\n5. ABCA12\n6. ABCA17P\n7. ABCA9\n8. ABCB4\n9. ABCB5\n10. ABCC11\n11. ABCC3\n12. ABCD1\n13. ABI3BP\n14. AC002116.8\n15. AC002398.9\n16. AC004057.1\n17. AC004231.2\n18. AC004540.5\n19. AC004623.3\n20. AC004951.5\n\n\n\n\n\n4.6.5 gene_map_feature\n\ngene_to_feature_mapping = TUGM.gene_map_feature(gene_names, ranked_features)\n\n\n\n4.6.5.1 AUC&gt;0.5\n\nimport numpy as np\n# Generating gene_to_feature_mapping\ngene_to_feature_mapping = {}\nfor gene, feature in zip(gene_names, FName):\n    # Find the index of the feature in FName\n    index = np.where(FName == feature)[0][0]\n    # Find the corresponding AUC value using the index\n    auc_value = Fauc[index]\n    # Store the gene name, feature name, and AUC value in the mapping\n    gene_to_feature_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 mappings\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_to_feature_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: A2M, Feature: 1147, AUC: 0.6469530885995243\n2. Gene: A2ML1, Feature: 140, AUC: 0.6465975134260281\n3. Gene: AADAC, Feature: 605, AUC: 0.6447509747664238\n4. Gene: AADACL2, Feature: 518, AUC: 0.6415704161455651\n5. Gene: ABCA12, Feature: 1080, AUC: 0.6405110473528042\n6. Gene: ABCA17P, Feature: 826, AUC: 0.6403761740111332\n7. Gene: ABCA9, Feature: 541, AUC: 0.6400941661149121\n8. Gene: ABCB4, Feature: 695, AUC: 0.6398293239167219\n9. Gene: ABCB5, Feature: 1266, AUC: 0.6387871208219917\n10. Gene: ABCC11, Feature: 0, AUC: 0.6381814169057604\n11. Gene: ABCC3, Feature: 864, AUC: 0.6376174011133181\n12. Gene: ABCD1, Feature: 188, AUC: 0.6373378454596729\n13. Gene: ABI3BP, Feature: 842, AUC: 0.6371956153902744\n14. Gene: AC002116.8, Feature: 344, AUC: 0.6371931631476986\n15. Gene: AC002398.9, Feature: 824, AUC: 0.6370018882267834\n16. Gene: AC004057.1, Feature: 1208, AUC: 0.6367713774246548\n17. Gene: AC004231.2, Feature: 1086, AUC: 0.636531057652223\n18. Gene: AC004540.5, Feature: 602, AUC: 0.6361656735084235\n19. Gene: AC004623.3, Feature: 295, AUC: 0.6359670418597808\n20. Gene: AC004951.5, Feature: 1261, AUC: 0.6358419774884132"
  },
  {
    "objectID": "MACFCmain.html#references-a",
    "href": "MACFCmain.html#references-a",
    "title": "4  MACFCmain.py",
    "section": "4.7 References",
    "text": "4.7 References\n\n\nSu,Y., Du,K., Wang,J., Wei,J. and Liu,J. (2022) Multi-variable AUC for sifting complementary features and its biomedical application. Briefings in Bioinformatics, 23, bbac029."
  },
  {
    "objectID": "NewMACFCmain.html#parameters-b",
    "href": "NewMACFCmain.html#parameters-b",
    "title": "5  NewMACFCmain.py",
    "section": "5.1 Parameters",
    "text": "5.1 Parameters\n\n\nAUC_threshold: float\n\nAUC threshold for feature selection. Features with AUC values higher than this threshold are recorded but not used in subsequent calculations.\n\nmax_rank: int\n\nThe total number of gene combinations you want to obtain.\n\nlable_name: string\n\nFor example: gender, age, altitude, temperature, quality, and other categorical variable names.\n\nthreshold: float\n\nFor example: 0.9\nThe set threshold indicates the proportion of non-zero value samples to all samples in each feature.\n\ndata_path: string\n\nFor example: ‘../data/gene_tpm.csv’\nPlease note: Preprocess the input data in advance to remove samples that contain too many missing values or zeros.\nThe input data matrix should have genes as rows and samples as columns.\n\nlabel_path: string\n\nFor example: ‘../data/tumor_class.csv’\nPlease note: The input sample categories must be in a numerical binary format, such as: 1,2,1,1,2,2,1.\nIn this case, the numerical values represent the following classifications: 1: male; 2: female."
  },
  {
    "objectID": "NewMACFCmain.html#returns-b",
    "href": "NewMACFCmain.html#returns-b",
    "title": "5  NewMACFCmain.py",
    "section": "5.2 Returns",
    "text": "5.2 Returns\n\n\nhigh_auc_features: list of tuples\n\nThis list contains tuples of feature indices and their corresponding AUC values, where the AUC value is greater than AUC_threshold. Each tuple consists of the feature’s index in string format and its AUC value as a float. This signifies that these features are highly predictive, with a strong ability to distinguish between different classes in the classification task.\n\nfr: list of strings\n\nRepresenting ranked features.\n\nfre1: dictionary\n\nFeature names as keys and their frequencies as values.\n\nfrequency: list of tuples\n\nFeature names and their frequencies.\nThe frequency outputs a list sorted by occurrence frequency (in descending order). This list includes only those elements from the dictionary fre1 (which represents the counted frequencies of elements in the original data) that have an occurrence frequency greater than once, along with their frequencies.\n\nlen(FName): integer\n\nCount of AUC values greater than 0.5.\n\nFName: array of strings\n\nFeature names after ranking with AUC &gt; 0.5.\n\nFauc: array of floats\n\nAUC values corresponding to the ranked feature names."
  },
  {
    "objectID": "NewMACFCmain.html#function-principle-explanation-b",
    "href": "NewMACFCmain.html#function-principle-explanation-b",
    "title": "5  NewMACFCmain.py",
    "section": "5.3 Function Principle Explanation",
    "text": "5.3 Function Principle Explanation\n\nFeature Frequency and AUC: In this function, features that appear with high frequency indicate their presence in multiple optimal feature sets. Each optimal feature set is determined by calculating its Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), which is a common measure for evaluating classifier performance. During each iteration of the loop, an optimal feature set with the highest average AUC value is selected. Features from this set are then added to a rank list, known as ‘ranklist,’ and when necessary, also to a set named ‘rankset’.\nHigh-Frequency Features and Performance: Because features in each set are chosen based on their contribution to classifier performance, high-frequency features are likely to perform well. In other words, if a feature appears in multiple optimal feature sets, it may have a significant impact on the performance of the classifier.\nNote on Low-Frequency Features: However, it’s important to note that a low frequency of a feature does not necessarily mean it is unimportant. The importance of a feature may depend on how it combines with other features. Additionally, the outcome of feature selection may be influenced by the characteristics of the dataset and random factors. Therefore, the frequency provided by this function should only be used as a reference and is not an absolute indicator of feature performance.\nFurther Evaluation Methods: If you wish to explore feature performance more deeply, you may need to employ other methods for assessing feature importance. This could include model-based importance metrics or statistical tests to evaluate the relationship between features and the target variable."
  },
  {
    "objectID": "NewMACFCmain.html#usage-workflow-b",
    "href": "NewMACFCmain.html#usage-workflow-b",
    "title": "5  NewMACFCmain.py",
    "section": "5.4 Usage Workflow",
    "text": "5.4 Usage Workflow\n\n\nFName is a list of feature names sorted based on their AUC (Area Under the Curve) values. In this sorting method, the primary consideration is the AUC value, followed by the feature name. All features included in FName have an AUC value greater than 0.5.\nfr is the result of another sorting method. In this method, the primary consideration is the “combined” AUC of the features, followed by their individual AUC values. This means that some features, despite having lower individual AUC values, may produce a higher combined AUC when paired with other features. Therefore, their position in the fr list may be higher than in the FName list.\nThe code for fr employs a more complex logic to select and combine features to optimize their combined AUC values. In this process, features are not solely selected and sorted based on their individual AUC values; the effect of their combination with other features is also considered. Consequently, the sorting logic for fr (or rankset) differs from that of FName.\nPlease note: While the code takes into account both individual AUC values and combined AUC values, the sorting of the fr list (i.e., rankset) initially starts based on individual AUC values. This is because at the beginning of each external loop iteration, the first element of fs is the next feature sorted by its individual AUC value. The list is then further optimized by evaluating the combination effects with other features."
  },
  {
    "objectID": "NewMACFCmain.html#usage-of-new-macfcmain-four-methods-degs-union-b",
    "href": "NewMACFCmain.html#usage-of-new-macfcmain-four-methods-degs-union-b",
    "title": "5  NewMACFCmain.py",
    "section": "5.5 Usage of New_MACFCmain (four_methods_degs_union)",
    "text": "5.5 Usage of New_MACFCmain (four_methods_degs_union)\nThis function uses the MACFC method to select feature genes relevant to classification and ranks them based on their corresponding weights.\n\n5.5.1 Import the corresponding module\n\nimport TransProPy.NewMACFCmain as TN\nimport TransProPy.UtilsFunction1.GeneNames as TUG\nimport TransProPy.UtilsFunction1.GeneToFeatureMapping as TUGM\n\n\n\n\n5.5.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/four_methods_degs_union.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n            Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0                 A1BG          6.754888          4.000000          5.044394   \n1                  A2M         16.808499         16.506184         17.143433   \n2                A2ML1          1.584963          9.517669          7.434628   \n3                AADAC          4.000000          2.584963          1.584963   \n4              AADACL2          1.000000          1.000000          0.000000   \n5              AADACL3          0.000000          0.000000          0.000000   \n6              AADACL4          0.000000          0.000000          0.000000   \n7          AB019440.50          0.000000          0.000000          0.000000   \n8          AB019441.29          6.392317          4.954196          6.629357   \n9  ABC12-47964100C23.1          0.000000          0.000000          0.000000   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0          5.247928          5.977280          5.044394          5.491853   \n1         17.760739         14.766839         16.263691         16.035207   \n2          2.584963          1.584963          2.584963          5.285402   \n3          0.000000          0.000000          0.000000          3.321928   \n4          0.000000          1.000000          0.000000          0.000000   \n5          0.000000          1.000000          0.000000          0.000000   \n6          0.000000          0.000000          0.000000          0.000000   \n7          0.000000          0.000000          0.000000          0.000000   \n8          6.988685          8.625709          6.614710          6.845490   \n9          0.000000          0.000000          0.000000          0.000000   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0          5.754888          6.357552  \n1         18.355114         16.959379  \n2          2.584963          3.584963  \n3          1.000000          4.584963  \n4          0.000000          1.000000  \n5          4.169925          0.000000  \n6          0.000000          0.000000  \n7          0.000000          0.000000  \n8          7.845490          6.507795  \n9          1.584963          0.000000  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      2\n5  TCGA-GN-A26A-06A      2\n6  TCGA-D3-A3BZ-06A      2\n7  TCGA-D3-A51G-06A      2\n8  TCGA-EE-A29R-06A      2\n9  TCGA-D3-A2JE-06A      2\n\n\n\n\n\n5.5.3 New_MACFCmain\n\nhigh_auc_features, ranked_features, fre1, frequency, len_FName, FName, Fauc = TN.New_MACFCmain(\n    0.9,\n    100, \n    \"class\", \n    0.9, \n    data_path='../test_TransProPy/data/four_methods_degs_union.csv', \n    label_path='../test_TransProPy/data/class.csv'\n    )\n\n\n\n\n5.5.4 Result\n\n5.5.4.1 AUC greater than 0.9 and their AUC values\n\n# Print features with AUC greater than 0.9 and their AUC values\nprint('\\nFeatures with AUC greater than 0.9:')\ntotal_features = len(high_auc_features)\nprint(f\"Total features: {total_features}\")\n\n# Determine the number of features to display\nnum_to_display = min(total_features, 20)\n\nfor i in range(num_to_display):\n    feature, auc_value = high_auc_features[i]\n    print(f\"Feature: {feature}, AUC: {auc_value}\")\n\n\nFeatures with AUC greater than 0.9:\nTotal features: 1421\nFeature: 26, AUC: 1.0\nFeature: 704, AUC: 1.0\nFeature: 717, AUC: 1.0\nFeature: 1172, AUC: 1.0\nFeature: 1899, AUC: 1.0\nFeature: 1948, AUC: 1.0\nFeature: 2338, AUC: 1.0\nFeature: 2596, AUC: 0.9999973764986752\nFeature: 582, AUC: 0.9999947529973503\nFeature: 2453, AUC: 0.9999895059947005\nFeature: 1563, AUC: 0.9999868824933756\nFeature: 786, AUC: 0.9999842589920508\nFeature: 2419, AUC: 0.9999842589920508\nFeature: 204, AUC: 0.9999763884880762\nFeature: 1002, AUC: 0.9999763884880762\nFeature: 291, AUC: 0.9999658944827767\nFeature: 237, AUC: 0.9999370359682032\nFeature: 124, AUC: 0.9999239184615788\nFeature: 1561, AUC: 0.9999081774536296\nFeature: 2171, AUC: 0.9999081774536296\n\n\n\n\n\n5.5.4.2 New_MACFCmain\n\n# Print the first 20 Ranked Features\nprint(\"\\nFirst 20 Ranked Features:\")\nfor i, feature in enumerate(ranked_features[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst 20 Ranked Features:\n1. 2440\n2. 2460\n3. 2096\n4. 482\n5. 2223\n6. 848\n7. 1501\n8. 519\n9. 1417\n10. 1939\n11. 1914\n12. 937\n13. 1340\n14. 100\n15. 1978\n16. 1558\n17. 413\n18. 1809\n19. 2031\n20. 1466\n\n\n\n\n# Print the first 20 Feature Frequencies (fre1)\nprint(\"\\nFirst 20 Feature Frequencies:\")\nfor i, (feature, freq) in enumerate(list(fre1.items())[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFirst 20 Feature Frequencies:\n1. Feature: 2440, Frequency: 1\n2. Feature: 2460, Frequency: 16\n3. Feature: 2096, Frequency: 26\n4. Feature: 482, Frequency: 1\n5. Feature: 2223, Frequency: 13\n6. Feature: 848, Frequency: 1\n7. Feature: 1501, Frequency: 1\n8. Feature: 519, Frequency: 1\n9. Feature: 1417, Frequency: 2\n10. Feature: 1939, Frequency: 1\n11. Feature: 1914, Frequency: 1\n12. Feature: 937, Frequency: 1\n13. Feature: 1340, Frequency: 1\n14. Feature: 100, Frequency: 1\n15. Feature: 1978, Frequency: 1\n16. Feature: 1558, Frequency: 1\n17. Feature: 413, Frequency: 1\n18. Feature: 1809, Frequency: 1\n19. Feature: 2031, Frequency: 1\n20. Feature: 1466, Frequency: 11\n\n\n\n\n# Print the Features with a frequency greater than 1 \nprint(\"\\nFeatures with a frequency greater than 1 :\")\nfor i, (feature, freq) in enumerate(frequency[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFeatures with a frequency greater than 1 :\n1. Feature: 2096, Frequency: 26\n2. Feature: 2460, Frequency: 16\n3. Feature: 2223, Frequency: 13\n4. Feature: 1466, Frequency: 11\n5. Feature: 1773, Frequency: 3\n6. Feature: 900, Frequency: 2\n7. Feature: 2620, Frequency: 2\n8. Feature: 176, Frequency: 2\n9. Feature: 1417, Frequency: 2\n10. Feature: 1136, Frequency: 2\n11. Feature: 1080, Frequency: 2\n\n\n\n\n# Print the length of FName (len_FName)\nprint(\"\\nCount of Features with AUC &gt; 0.5 (len_FName):\")\nprint(len_FName)\n\n\nCount of Features with AUC &gt; 0.5 (len_FName):\n809\n\n\n\n\n# Print the first 10 Features with AUC &gt; 0.5 (FName)\nprint(\"\\nFirst few Features with AUC &gt; 0.5:\")\nfor i, feature in enumerate(FName[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst few Features with AUC &gt; 0.5:\n1. 2440\n2. 482\n3. 848\n4. 1501\n5. 519\n6. 1939\n7. 1914\n8. 937\n9. 1340\n10. 100\n11. 1978\n12. 1558\n13. 413\n14. 1809\n15. 2031\n16. 780\n17. 712\n18. 1362\n19. 1136\n20. 2486\n\n\n\n\n# Print the first 10 AUC Values for Ranked Features (Fauc)\nprint(\"\\nFirst few AUC Values for Ranked Features:\")\nfor i, auc in enumerate(Fauc[:20], 1):\n    print(f\"{i}. AUC: {auc}\")\n\n\nFirst few AUC Values for Ranked Features:\n1. AUC: 0.8999134244562793\n2. AUC: 0.8998950599470052\n3. AUC: 0.899879318939056\n4. AUC: 0.8995041582496\n5. AUC: 0.8994700527323767\n6. AUC: 0.8994595587270772\n7. AUC: 0.8992864076396359\n8. AUC: 0.8992785371356613\n9. AUC: 0.8990555395230475\n10. AUC: 0.8990424220164231\n11. AUC: 0.8989715874806516\n12. AUC: 0.8988351654117586\n13. AUC: 0.8988325419104337\n14. AUC: 0.8987433428653882\n15. AUC: 0.8987276018574389\n16. AUC: 0.8987249783561141\n17. AUC: 0.8986725083296168\n18. AUC: 0.8985649447752971\n19. AUC: 0.8985150982501247\n20. AUC: 0.898449510717003\n\n\n\n\n\n\n5.5.5 gene_name\n\ngene_names = TUG.gene_name(data_path='../test_TransProPy/data/four_methods_degs_union.csv')\n\n\n\n# Print the first 20 gene names\nprint(\"First 20 Gene Names:\")\nfor i, gene_name in enumerate(gene_names[:20], 1):\n    print(f\"{i}. {gene_name}\")\n\nFirst 20 Gene Names:\n1. A1BG\n2. A2M\n3. A2ML1\n4. AADAC\n5. AADACL2\n6. AADACL3\n7. AADACL4\n8. AB019440.50\n9. AB019441.29\n10. ABC12-47964100C23.1\n11. ABC12-49244600F4.4\n12. ABCA10\n13. ABCA12\n14. ABCA17P\n15. ABCA6\n16. ABCA8\n17. ABCA9\n18. ABCB11\n19. ABCB4\n20. ABCB5\n\n\n\n\n\n5.5.6 gene_map_feature\n\n5.5.6.1 high_auc_features_result(AUC&gt;0.9)\n\n# Extract feature indices from high_auc_features\nhigh_ranked_features = [feature for feature, auc_value in high_auc_features]\n\n# Utilize the TUGM.gene_map_feature function\ngene_to_feature_mapping_0_9 = TUGM.gene_map_feature(gene_names, high_ranked_features)\n\n# Creating a dictionary to store gene, feature, and AUC mapping\ngene_feature_auc_mapping = {}\n\n# Iterate over each gene and its corresponding feature\nfor gene, feature in gene_to_feature_mapping_0_9.items():\n    feature = str(feature)  # Adjust this based on your data format\n    # Find the corresponding AUC value for the feature\n    auc_value = next((auc for feat, auc in high_auc_features if str(feat) == feature), None)\n    # Store the gene, feature, and AUC in the mapping\n    gene_feature_auc_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 gene to feature mappings along with AUC values\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_feature_auc_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: ABCD1, Feature: 26, AUC: 1.0\n2. Gene: ANGPTL5, Feature: 704, AUC: 1.0\n3. Gene: ANKRD20A10P, Feature: 717, AUC: 1.0\n4. Gene: CAPN8, Feature: 1172, AUC: 1.0\n5. Gene: CTD-2340E1.2, Feature: 1899, AUC: 1.0\n6. Gene: CTD-2562J17.2, Feature: 1948, AUC: 1.0\n7. Gene: EPGN, Feature: 2338, AUC: 1.0\n8. Gene: FOSB, Feature: 2596, AUC: 0.9999973764986752\n9. Gene: AF064858.8, Feature: 582, AUC: 0.9999947529973503\n10. Gene: FAM229A, Feature: 2453, AUC: 0.9999895059947005\n11. Gene: COL9A1, Feature: 1563, AUC: 0.9999868824933756\n12. Gene: AP001107.1, Feature: 786, AUC: 0.9999842589920508\n13. Gene: FAM155B, Feature: 2419, AUC: 0.9999842589920508\n14. Gene: AC010547.9, Feature: 204, AUC: 0.9999763884880762\n15. Gene: bP-21201H5.1, Feature: 1002, AUC: 0.9999763884880762\n16. Gene: AC023590.1, Feature: 291, AUC: 0.9999658944827767\n17. Gene: AC012512.1, Feature: 237, AUC: 0.9999370359682032\n18. Gene: AC006486.10, Feature: 124, AUC: 0.9999239184615788\n19. Gene: COL7A1, Feature: 1561, AUC: 0.9999081774536296\n20. Gene: DNAH2, Feature: 2171, AUC: 0.9999081774536296\n\n\n\n\n\n5.5.6.2 NewMACFCmain_result(0.9&gt;AUC&gt;0.5)\n\nimport numpy as np\n# Generating gene_to_feature_mapping\ngene_to_feature_mapping = {}\nfor gene, feature in zip(gene_names, FName):\n    # Find the index of the feature in FName\n    index = np.where(FName == feature)[0][0]\n    # Find the corresponding AUC value using the index\n    auc_value = Fauc[index]\n    # Store the gene name, feature name, and AUC value in the mapping\n    gene_to_feature_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 mappings\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_to_feature_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: A1BG, Feature: 2440, AUC: 0.8999134244562793\n2. Gene: A2M, Feature: 482, AUC: 0.8998950599470052\n3. Gene: A2ML1, Feature: 848, AUC: 0.899879318939056\n4. Gene: AADAC, Feature: 1501, AUC: 0.8995041582496\n5. Gene: AADACL2, Feature: 519, AUC: 0.8994700527323767\n6. Gene: AADACL3, Feature: 1939, AUC: 0.8994595587270772\n7. Gene: AADACL4, Feature: 1914, AUC: 0.8992864076396359\n8. Gene: AB019440.50, Feature: 937, AUC: 0.8992785371356613\n9. Gene: AB019441.29, Feature: 1340, AUC: 0.8990555395230475\n10. Gene: ABC12-47964100C23.1, Feature: 100, AUC: 0.8990424220164231\n11. Gene: ABC12-49244600F4.4, Feature: 1978, AUC: 0.8989715874806516\n12. Gene: ABCA10, Feature: 1558, AUC: 0.8988351654117586\n13. Gene: ABCA12, Feature: 413, AUC: 0.8988325419104337\n14. Gene: ABCA17P, Feature: 1809, AUC: 0.8987433428653882\n15. Gene: ABCA6, Feature: 2031, AUC: 0.8987276018574389\n16. Gene: ABCA8, Feature: 780, AUC: 0.8987249783561141\n17. Gene: ABCA9, Feature: 712, AUC: 0.8986725083296168\n18. Gene: ABCB11, Feature: 1362, AUC: 0.8985649447752971\n19. Gene: ABCB4, Feature: 1136, AUC: 0.8985150982501247\n20. Gene: ABCB5, Feature: 2486, AUC: 0.898449510717003\n\n\n\n\n\n\n5.5.7 Save Data\n\n5.5.7.1 high_auc_features_result (AUC&gt;0.9)\n\nimport pandas as pd\n# Convert gene_feature_auc_mapping to a DataFrame\ngene_feature_auc_df = pd.DataFrame.from_dict(gene_feature_auc_mapping, orient='index', columns=['Feature', 'AUC'])\n\n# Reset the index to make gene names a separate column\ngene_feature_auc_df.reset_index(inplace=True)\ngene_feature_auc_df.rename(columns={'index': 'Gene'}, inplace=True)\n\n# Save the DataFrame to a CSV file\ngene_feature_auc_df.to_csv('../test_TransProPy/result/all_degs_count_exp_gene_feature_auc_mapping_0.9.csv', index=False)\n\n\n\n\n5.5.7.2 NewMACFCmain_result (0.9&gt;AUC&gt;0.5)\n\nimport pandas as pd\n# Convert gene_to_feature_mapping to a DataFrame\ngene_to_feature_df = pd.DataFrame.from_dict(gene_to_feature_mapping, orient='index', columns=['Feature', 'AUC'])\n\n# Reset the index to make gene names a separate column\ngene_to_feature_df.reset_index(inplace=True)\ngene_to_feature_df.rename(columns={'index': 'Gene'}, inplace=True)\n\n# Save the DataFrame to a CSV file\ngene_to_feature_df.to_csv('../test_TransProPy/result/all_degs_count_exp_gene_feature_auc_mapping_0.5_0.9.csv', index=False)"
  },
  {
    "objectID": "NewMACFCmain.html#usage-of-new-macfcmain-all-count-exp-b",
    "href": "NewMACFCmain.html#usage-of-new-macfcmain-all-count-exp-b",
    "title": "5  NewMACFCmain.py",
    "section": "5.6 Usage of New_MACFCmain (all_count_exp)",
    "text": "5.6 Usage of New_MACFCmain (all_count_exp)\nThis function uses the MACFC method to select feature genes relevant to classification and ranks them based on their corresponding weights.\n\n5.6.1 Import the corresponding module\n\nimport TransProPy.NewMACFCmain as TN\nimport TransProPy.UtilsFunction1.GeneNames as TUG\nimport TransProPy.UtilsFunction1.GeneToFeatureMapping as TUGM\n\n\n\n\n5.6.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/all_count_exp.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n  Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0  5_8S_rRNA                 0                 0                 0   \n1    5S_rRNA                 0                 0                 0   \n2        7SK                 0                 0                 0   \n3       A1BG               107                15                32   \n4   A1BG-AS1               373               112               363   \n5       A1CF                10                 0                 0   \n6        A2M            114778             93079            144772   \n7    A2M-AS1               538               140                50   \n8      A2ML1                 2               732               172   \n9  A2ML1-AS1                 0                 0                 0   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0                 0                 0                 0                 0   \n1                 0                 0                 0                 0   \n2                 0                 0                 0                 0   \n3                37                62                32                44   \n4               347               222               114               225   \n5                 1                 3                 0                 0   \n6            222082             27877             78678             67154   \n7               211                28               332               136   \n8                 5                 2                 5                38   \n9                 0                 3                 0                 0   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0                 0                 0  \n1                 0                 0  \n2                 0                 0  \n3                53                81  \n4               284               396  \n5                 0                 1  \n6            335304            127432  \n7                54               225  \n8                 5                11  \n9                 0                 1  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      2\n5  TCGA-GN-A26A-06A      2\n6  TCGA-D3-A3BZ-06A      2\n7  TCGA-D3-A51G-06A      2\n8  TCGA-EE-A29R-06A      2\n9  TCGA-D3-A2JE-06A      2\n\n\n\n\n\n5.6.3 New_MACFCmain\n\nhigh_auc_features, ranked_features, fre1, frequency, len_FName, FName, Fauc = TN.New_MACFCmain(\n    0.9,\n    100, \n    \"class\", \n    0.9, \n    data_path='../test_TransProPy/data/all_count_exp.csv', \n    label_path='../test_TransProPy/data/class.csv'\n    )\n\n\n\n\n5.6.4 Result\n\n5.6.4.1 AUC greater than 0.9 and their AUC values\n\n# Print features with AUC greater than 0.9 and their AUC values\nprint('\\nFeatures with AUC greater than 0.9:')\ntotal_features = len(high_auc_features)\nprint(f\"Total features: {total_features}\")\n\n# Determine the number of features to display\nnum_to_display = min(total_features, 20)\n\nfor i in range(num_to_display):\n    feature, auc_value = high_auc_features[i]\n    print(f\"Feature: {feature}, AUC: {auc_value}\")\n\n\nFeatures with AUC greater than 0.9:\nTotal features: 4903\nFeature: 129, AUC: 1.0\nFeature: 4701, AUC: 1.0\nFeature: 4788, AUC: 1.0\nFeature: 7317, AUC: 1.0\nFeature: 12536, AUC: 1.0\nFeature: 12784, AUC: 1.0\nFeature: 15361, AUC: 1.0\nFeature: 17372, AUC: 0.9999973764986752\nFeature: 3731, AUC: 0.9999947529973503\nFeature: 16018, AUC: 0.9999921294960255\nFeature: 16355, AUC: 0.9999895059947005\nFeature: 9932, AUC: 0.9999868824933756\nFeature: 5222, AUC: 0.9999842589920508\nFeature: 1242, AUC: 0.9999763884880762\nFeature: 6418, AUC: 0.9999763884880762\nFeature: 1915, AUC: 0.9999658944827767\nFeature: 1491, AUC: 0.9999370359682032\nFeature: 724, AUC: 0.9999239184615788\nFeature: 9930, AUC: 0.9999081774536296\nFeature: 14212, AUC: 0.9999081774536296\n\n\n\n\n\n5.6.4.2 New_MACFCmain\n\n# Print the first 20 Ranked Features\nprint(\"\\nFirst 20 Ranked Features:\")\nfor i, feature in enumerate(ranked_features[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst 20 Ranked Features:\n1. 15431\n2. 13741\n3. 6034\n4. 12954\n5. 16231\n6. 14367\n7. 10409\n8. 8777\n9. 4031\n10. 5595\n11. 3066\n12. 15959\n13. 3436\n14. 12999\n15. 15583\n16. 6533\n17. 173\n18. 6943\n19. 8390\n20. 5298\n\n\n\n\n# Print the first 20 Feature Frequencies (fre1)\nprint(\"\\nFirst 20 Feature Frequencies:\")\nfor i, (feature, freq) in enumerate(list(fre1.items())[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFirst 20 Feature Frequencies:\n1. Feature: 15431, Frequency: 1\n2. Feature: 13741, Frequency: 62\n3. Feature: 6034, Frequency: 51\n4. Feature: 12954, Frequency: 1\n5. Feature: 16231, Frequency: 2\n6. Feature: 14367, Frequency: 12\n7. Feature: 10409, Frequency: 2\n8. Feature: 8777, Frequency: 1\n9. Feature: 4031, Frequency: 3\n10. Feature: 5595, Frequency: 1\n11. Feature: 3066, Frequency: 1\n12. Feature: 15959, Frequency: 1\n13. Feature: 3436, Frequency: 1\n14. Feature: 12999, Frequency: 1\n15. Feature: 15583, Frequency: 2\n16. Feature: 6533, Frequency: 4\n17. Feature: 173, Frequency: 2\n18. Feature: 6943, Frequency: 2\n19. Feature: 8390, Frequency: 2\n20. Feature: 5298, Frequency: 1\n\n\n\n\n# Print the Features with a frequency greater than 1 \nprint(\"\\nFeatures with a frequency greater than 1 :\")\nfor i, (feature, freq) in enumerate(frequency[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFeatures with a frequency greater than 1 :\n1. Feature: 13741, Frequency: 62\n2. Feature: 6034, Frequency: 51\n3. Feature: 14367, Frequency: 12\n4. Feature: 10844, Frequency: 8\n5. Feature: 16078, Frequency: 7\n6. Feature: 6533, Frequency: 4\n7. Feature: 4031, Frequency: 3\n8. Feature: 8390, Frequency: 2\n9. Feature: 6943, Frequency: 2\n10. Feature: 586, Frequency: 2\n11. Feature: 5249, Frequency: 2\n12. Feature: 173, Frequency: 2\n13. Feature: 16231, Frequency: 2\n14. Feature: 16178, Frequency: 2\n15. Feature: 15583, Frequency: 2\n16. Feature: 10409, Frequency: 2\n\n\n\n\n# Print the length of FName (len_FName)\nprint(\"\\nCount of Features with AUC &gt; 0.5 (len_FName):\")\nprint(len_FName)\n\n\nCount of Features with AUC &gt; 0.5 (len_FName):\n7726\n\n\n\n\n# Print the first 10 Features with AUC &gt; 0.5 (FName)\nprint(\"\\nFirst few Features with AUC &gt; 0.5:\")\nfor i, feature in enumerate(FName[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst few Features with AUC &gt; 0.5:\n1. 15431\n2. 12954\n3. 16231\n4. 8777\n5. 5595\n6. 3066\n7. 15959\n8. 3436\n9. 12999\n10. 17823\n11. 17634\n12. 14506\n13. 2797\n14. 5249\n15. 17163\n16. 10844\n17. 16206\n18. 5614\n19. 5193\n20. 4601\n\n\n\n\n# Print the first 10 AUC Values for Ranked Features (Fauc)\nprint(\"\\nFirst few AUC Values for Ranked Features:\")\nfor i, auc in enumerate(Fauc[:20], 1):\n    print(f\"{i}. AUC: {auc}\")\n\n\nFirst few AUC Values for Ranked Features:\n1. AUC: 0.8999763884880762\n2. AUC: 0.8999685179841016\n3. AUC: 0.8999658944827768\n4. AUC: 0.8999580239788021\n5. AUC: 0.8998924364456804\n6. AUC: 0.8998740719364063\n7. AUC: 0.899860954429782\n8. AUC: 0.8998583309284571\n9. AUC: 0.8998557074271323\n10. AUC: 0.8998058609019598\n11. AUC: 0.8997953668966603\n12. AUC: 0.8997560143767872\n13. AUC: 0.8997035443502899\n14. AUC: 0.8996930503449905\n15. AUC: 0.8996878033423407\n16. AUC: 0.8996510743237925\n17. AUC: 0.8996064748012698\n18. AUC: 0.8996064748012698\n19. AUC: 0.8995933572946454\n20. AUC: 0.8995382637668232\n\n\n\n\n\n\n5.6.5 gene_name\n\ngene_names = TUG.gene_name(data_path='../test_TransProPy/data/all_count_exp.csv')\n\n\n\n# Print the first 20 gene names\nprint(\"First 20 Gene Names:\")\nfor i, gene_name in enumerate(gene_names[:20], 1):\n    print(f\"{i}. {gene_name}\")\n\nFirst 20 Gene Names:\n1. 5_8S_rRNA\n2. 5S_rRNA\n3. 7SK\n4. A1BG\n5. A1BG-AS1\n6. A1CF\n7. A2M\n8. A2M-AS1\n9. A2ML1\n10. A2ML1-AS1\n11. A2ML1-AS2\n12. A2MP1\n13. A3GALT2\n14. A4GALT\n15. A4GNT\n16. AA06\n17. AAAS\n18. AACS\n19. AACSP1\n20. AADAC\n\n\n\n\n\n5.6.6 gene_map_feature\n\n5.6.6.1 high_auc_features_result (AUC&gt;0.9)\n\n# Extract feature indices from high_auc_features\nhigh_ranked_features = [feature for feature, auc_value in high_auc_features]\n\n# Utilize the TUGM.gene_map_feature function\ngene_to_feature_mapping_0_9 = TUGM.gene_map_feature(gene_names, high_ranked_features)\n\n# Creating a dictionary to store gene, feature, and AUC mapping\ngene_feature_auc_mapping = {}\n\n# Iterate over each gene and its corresponding feature\nfor gene, feature in gene_to_feature_mapping_0_9.items():\n    feature = str(feature)  # Adjust this based on your data format\n    # Find the corresponding AUC value for the feature\n    auc_value = next((auc for feat, auc in high_auc_features if str(feat) == feature), None)\n    # Store the gene, feature, and AUC in the mapping\n    gene_feature_auc_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 gene to feature mappings along with AUC values\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_feature_auc_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: ABHD15, Feature: 129, AUC: 1.0\n2. Gene: AF064858.11, Feature: 4701, AUC: 1.0\n3. Gene: AFF4, Feature: 4788, AUC: 1.0\n4. Gene: AY269186.1, Feature: 7317, AUC: 1.0\n5. Gene: CTD-2530H12.5, Feature: 12536, AUC: 1.0\n6. Gene: CTD-2650P22.2, Feature: 12784, AUC: 1.0\n7. Gene: FAM197Y8, Feature: 15361, AUC: 1.0\n8. Gene: GS1-25M2.1, Feature: 17372, AUC: 0.9999973764986752\n9. Gene: AC107016.1, Feature: 3731, AUC: 0.9999947529973503\n10. Gene: FLJ42102, Feature: 16018, AUC: 0.9999921294960255\n11. Gene: FUT9, Feature: 16355, AUC: 0.9999895059947005\n12. Gene: CICP26, Feature: 9932, AUC: 0.9999868824933756\n13. Gene: AL133475.1, Feature: 5222, AUC: 0.9999842589920508\n14. Gene: AC008703.2, Feature: 1242, AUC: 0.9999763884880762\n15. Gene: AP001464.4, Feature: 6418, AUC: 0.9999763884880762\n16. Gene: AC016561.1, Feature: 1915, AUC: 0.9999658944827767\n17. Gene: AC010148.1, Feature: 1491, AUC: 0.9999370359682032\n18. Gene: AC005822.1, Feature: 724, AUC: 0.9999239184615788\n19. Gene: CICP23, Feature: 9930, AUC: 0.9999081774536296\n20. Gene: DUSP12P1, Feature: 14212, AUC: 0.9999081774536296\n\n\n\n\n\n5.6.6.2 NewMACFCmain_result (0.9&gt;AUC&gt;0.5)\n\nimport numpy as np\n# Generating gene_to_feature_mapping\ngene_to_feature_mapping = {}\nfor gene, feature in zip(gene_names, FName):\n    # Find the index of the feature in FName\n    index = np.where(FName == feature)[0][0]\n    # Find the corresponding AUC value using the index\n    auc_value = Fauc[index]\n    # Store the gene name, feature name, and AUC value in the mapping\n    gene_to_feature_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 mappings\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_to_feature_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: 5_8S_rRNA, Feature: 15431, AUC: 0.8999763884880762\n2. Gene: 5S_rRNA, Feature: 12954, AUC: 0.8999685179841016\n3. Gene: 7SK, Feature: 16231, AUC: 0.8999658944827768\n4. Gene: A1BG, Feature: 8777, AUC: 0.8999580239788021\n5. Gene: A1BG-AS1, Feature: 5595, AUC: 0.8998924364456804\n6. Gene: A1CF, Feature: 3066, AUC: 0.8998740719364063\n7. Gene: A2M, Feature: 15959, AUC: 0.899860954429782\n8. Gene: A2M-AS1, Feature: 3436, AUC: 0.8998583309284571\n9. Gene: A2ML1, Feature: 12999, AUC: 0.8998557074271323\n10. Gene: A2ML1-AS1, Feature: 17823, AUC: 0.8998058609019598\n11. Gene: A2ML1-AS2, Feature: 17634, AUC: 0.8997953668966603\n12. Gene: A2MP1, Feature: 14506, AUC: 0.8997560143767872\n13. Gene: A3GALT2, Feature: 2797, AUC: 0.8997035443502899\n14. Gene: A4GALT, Feature: 5249, AUC: 0.8996930503449905\n15. Gene: A4GNT, Feature: 17163, AUC: 0.8996878033423407\n16. Gene: AA06, Feature: 10844, AUC: 0.8996510743237925\n17. Gene: AAAS, Feature: 16206, AUC: 0.8996064748012698\n18. Gene: AACS, Feature: 5614, AUC: 0.8996064748012698\n19. Gene: AACSP1, Feature: 5193, AUC: 0.8995933572946454\n20. Gene: AADAC, Feature: 4601, AUC: 0.8995382637668232\n\n\n\n\n\n\n5.6.7 Save Data\n\n5.6.7.1 high_auc_features_result (AUC&gt;0.9)\n\nimport pandas as pd\n# Convert gene_feature_auc_mapping to a DataFrame\ngene_feature_auc_df = pd.DataFrame.from_dict(gene_feature_auc_mapping, orient='index', columns=['Feature', 'AUC'])\n\n# Reset the index to make gene names a separate column\ngene_feature_auc_df.reset_index(inplace=True)\ngene_feature_auc_df.rename(columns={'index': 'Gene'}, inplace=True)\n\n# Save the DataFrame to a CSV file\ngene_feature_auc_df.to_csv('../test_TransProPy/result/all_count_exp_gene_feature_auc_mapping_0.9.csv', index=False)\n\n\n\n\n5.6.7.2 NewMACFCmain_result (0.9&gt;AUC&gt;0.5)\n\nimport pandas as pd\n# Convert gene_to_feature_mapping to a DataFrame\ngene_to_feature_df = pd.DataFrame.from_dict(gene_to_feature_mapping, orient='index', columns=['Feature', 'AUC'])\n\n# Reset the index to make gene names a separate column\ngene_to_feature_df.reset_index(inplace=True)\ngene_to_feature_df.rename(columns={'index': 'Gene'}, inplace=True)\n\n# Save the DataFrame to a CSV file\ngene_to_feature_df.to_csv('../test_TransProPy/result/all_count_exp_gene_feature_auc_mapping_0.5_0.9.csv', index=False)"
  },
  {
    "objectID": "NewMACFCmain.html#references-b",
    "href": "NewMACFCmain.html#references-b",
    "title": "5  NewMACFCmain.py",
    "section": "5.7 References",
    "text": "5.7 References\n\n\nSu,Y., Du,K., Wang,J., Wei,J. and Liu,J. (2022) Multi-variable AUC for sifting complementary features and its biomedical application. Briefings in Bioinformatics, 23, bbac029."
  },
  {
    "objectID": "AutogluonTimeLimit.html#parameters-c",
    "href": "AutogluonTimeLimit.html#parameters-c",
    "title": "6  AutogluonTimeLimit.py",
    "section": "6.1 Parameters",
    "text": "6.1 Parameters\n\n\ngene_data_path (str):\n\nPath to the gene expression data CSV file.\nFor example: ‘../data/gene_tpm.csv’\n\nclass_data_path (str):\n\nPath to the class data CSV file.\nFor example: ‘../data/tumor_class.csv’\n\nlabel_column (str):\n\nName of the column in the dataset that is the target label for prediction.\n\ntest_size (float):\n\nProportion of the data to be used as the test set.\n\nthreshold (float):\n\nThe threshold used to filter out rows based on the proportion of non-zero values.\n\nrandom_feature (int, optional):\n\nThe number of random feature to select. If None, no random feature selection is performed.\nDefault is None.\n\nnum_bag_folds (int, optional):\n\nPlease note: This parameter annotation source can be referred to the documentation link in References.\nNumber of folds used for bagging of models. When num_bag_folds = k, training time is roughly increased by a factor of k (set = 0 to disable bagging). Disabled by default (0), but we recommend values between 5-10 to maximize predictive performance. Increasing num_bag_folds will result in models with lower bias but that are more prone to overfitting. num_bag_folds = 1 is an invalid value, and will raise a ValueError. Values &gt; 10 may produce diminishing returns, and can even harm overall results due to overfitting. To further improve predictions, avoid increasing num_bag_folds much beyond 10 and instead increase num_bag_sets.\ndefault = None\n\nnum_stack_levels (int, optional):\n\nPlease note: This parameter annotation source can be referred to the documentation link in References.\nNumber of stacking levels to use in stack ensemble. Roughly increases model training time by factor of num_stack_levels+1 (set = 0 to disable stack ensembling). Disabled by default (0), but we recommend values between 1-3 to maximize predictive performance. To prevent overfitting, num_bag_folds &gt;= 2 must also be set or else a ValueError will be raised.\ndefault = None\n\ntime_limit (int, optional):\n\nTime limit for training in seconds.\nDefault is 120.\n\nrandom_state (int, optional):\n\nThe seed used by the random number generator.\nDefault is 42."
  },
  {
    "objectID": "AutogluonTimeLimit.html#returns-c",
    "href": "AutogluonTimeLimit.html#returns-c",
    "title": "6  AutogluonTimeLimit.py",
    "section": "6.2 Returns",
    "text": "6.2 Returns\n\n\nimportance (DataFrame):\n\nDataFrame containing feature importance.\n\nleaderboard (DataFrame):\n\nDataFrame containing model performance on the test data."
  },
  {
    "objectID": "AutogluonTimeLimit.html#usage-of-autogluon_timelimit",
    "href": "AutogluonTimeLimit.html#usage-of-autogluon_timelimit",
    "title": "6  AutogluonTimeLimit.py",
    "section": "6.3 Usage of Autogluon_TimeLimit",
    "text": "6.3 Usage of Autogluon_TimeLimit\nPerforming training and prediction tasks on tabular data using Autogluon.\n\n6.3.1 Objectives\n\n6.3.1.1 Model Training and Selection\n\nAutogluon will attempt various models and hyperparameter combinations within a given time limit to find the best-performing model on the test data. During training, Autogluon may output training logs displaying performance metrics and progress information for different models. The goal is to select the best-performing model for use in subsequent prediction tasks.\n\n\n\n6.3.1.2 Leaderboard\n\nThe leaderboard displays performance scores of different models on the test data, typically including metrics like accuracy, precision, recall, and more. The purpose is to assist users in understanding the performance of different models to choose the most suitable model for predictions.\n\n\n\n6.3.1.3 Importance\n\nFeature importance indicates which features are most critical for the model’s prediction performance. The purpose is to help users understand the importance of specific features in the data, which can be used for feature selection or further data analysis.\n\n\n\n\n6.3.2 Note\n\nPlease note that Autogluon’s output results may vary depending on your data and task. You can review the generated model leaderboard and feature importance to understand model performance and the significance of specific features in the data. These results can aid you in making better predictions and decisions."
  },
  {
    "objectID": "AutogluonTimeLimit.html#insignificant-correlation-c",
    "href": "AutogluonTimeLimit.html#insignificant-correlation-c",
    "title": "6  AutogluonTimeLimit.py",
    "section": "6.4 Insignificant Correlation",
    "text": "6.4 Insignificant Correlation\n\n\nPlease note:Data characteristics: Features have weak correlation with the classification.\nRandomly shuffling the class labels to a certain extent simulates reducing the correlation.\n\n\n\n6.4.1 Import the corresponding module\n\nfrom TransProPy.AutogluonTimeLimit import Autogluon_TimeLimit\n\n\n\n\n6.4.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/four_methods_degs_intersection.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n  Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0        A2M         16.808499         16.506184         17.143433   \n1      A2ML1          1.584963          9.517669          7.434628   \n2      AADAC          4.000000          2.584963          1.584963   \n3    AADACL2          1.000000          1.000000          0.000000   \n4     ABCA12          4.523562          4.321928          3.906891   \n5    ABCA17P          4.584963          5.169925          3.807355   \n6      ABCA9          9.753217          6.906891          3.459432   \n7      ABCB4          9.177420          6.700440          5.000000   \n8      ABCB5         10.134426          4.169925          9.167418   \n9     ABCC11         10.092757          6.491853          5.459432   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0         17.760739         14.766839         16.263691         16.035207   \n1          2.584963          1.584963          2.584963          5.285402   \n2          0.000000          0.000000          0.000000          3.321928   \n3          0.000000          1.000000          0.000000          0.000000   \n4          3.459432          1.584963          3.000000          4.321928   \n5          8.366322          7.228819          7.076816          4.584963   \n6          2.584963          6.357552          6.475733          7.330917   \n7          9.342075         10.392317          7.383704         11.032735   \n8          4.906891         11.340963          3.169925         11.161762   \n9          6.807355          4.247928          5.459432          5.977280   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0         18.355114         16.959379  \n1          2.584963          3.584963  \n2          1.000000          4.584963  \n3          0.000000          1.000000  \n4          4.807355          3.700440  \n5          6.409391          7.139551  \n6          7.954196          9.177420  \n7         10.082149         10.088788  \n8          4.643856         12.393927  \n9          5.614710          8.233620  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/random_classification_class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      1\n5  TCGA-GN-A26A-06A      1\n6  TCGA-D3-A3BZ-06A      1\n7  TCGA-D3-A51G-06A      1\n8  TCGA-EE-A29R-06A      1\n9  TCGA-D3-A2JE-06A      1\n\n\n\n\n\n6.4.3 Autogluon_TimeLimit\n\nimportance, leaderboard = Autogluon_TimeLimit(\n    gene_data_path='../test_TransProPy/data/four_methods_degs_intersection.csv', \n    class_data_path='../test_TransProPy/data/random_classification_class.csv', \n    label_column='class',  \n    test_size=0.3, \n    threshold=0.9, \n    random_feature=None, \n    num_bag_folds=None, \n    num_stack_levels=None, \n    time_limit=1000, \n    random_state=42\n    )\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20240410_105110\\\"\n\n\nBeginning AutoGluon training ... Time limit = 1000s\n\n\nAutoGluon will save models to \"AutogluonModels\\ag-20240410_105110\\\"\n\n\nAutoGluon Version:  0.8.2\n\n\nPython Version:     3.10.11\n\n\nOperating System:   Windows\n\n\nPlatform Machine:   AMD64\n\n\nPlatform Version:   10.0.19044\n\n\nDisk Space Avail:   216.66 GB / 925.93 GB (23.4%)\n\n\nTrain Data Rows:    896\n\n\nTrain Data Columns: 1605\n\n\nLabel Column: class\n\n\nPreprocessing data ...\n\n\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\n\n    2 unique label values:  [1, 2]\n\n\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n\n\nSelected class &lt;--&gt; label mapping:  class 1 = 2, class 0 = 1\n\n\n    Note: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (2) vs negative (1) class.\n    To explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n\n\nUsing Feature Generators to preprocess the data ...\n\n\nFitting AutoMLPipelineFeatureGenerator...\n\n\n    Available Memory:                    19083.68 MB\n\n\n    Train Data (Original)  Memory Usage: 11.5 MB (0.1% of available memory)\n\n\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\n\n    Stage 1 Generators:\n\n\n        Fitting AsTypeFeatureGenerator...\n\n\n    Stage 2 Generators:\n\n\n        Fitting FillNaFeatureGenerator...\n\n\n    Stage 3 Generators:\n\n\n        Fitting IdentityFeatureGenerator...\n\n\n    Stage 4 Generators:\n\n\n        Fitting DropUniqueFeatureGenerator...\n\n\n    Stage 5 Generators:\n\n\n        Fitting DropDuplicatesFeatureGenerator...\n\n\n    Types of features in original data (raw dtype, special dtypes):\n\n\n        ('float', []) : 1605 | ['A2M', 'A2ML1', 'ABCA12', 'ABCA17P', 'ABCA9', ...]\n\n\n    Types of features in processed data (raw dtype, special dtypes):\n\n\n        ('float', []) : 1605 | ['A2M', 'A2ML1', 'ABCA12', 'ABCA17P', 'ABCA9', ...]\n\n\n    1.5s = Fit runtime\n\n\n    1605 features in original data used to generate 1605 features in processed data.\n\n\n    Train Data (Processed) Memory Usage: 11.5 MB (0.1% of available memory)\n\n\nData preprocessing and feature engineering runtime = 1.64s ...\n\n\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\n\n    To change this, specify the eval_metric parameter of Predictor()\n\n\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 716, Val Rows: 180\n\n\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\n\n\nFitting 13 L1 models ...\n\n\nFitting model: KNeighborsUnif ... Training model for up to 998.36s of the 998.35s of remaining time.\n\n\n    0.5722   = Validation score   (accuracy)\n\n\n    16.1s    = Training   runtime\n\n\n    0.46s    = Validation runtime\n\n\nFitting model: KNeighborsDist ... Training model for up to 981.79s of the 981.78s of remaining time.\n\n\n    0.5722   = Validation score   (accuracy)\n\n\n    0.12s    = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: LightGBMXT ... Training model for up to 981.61s of the 981.59s of remaining time.\n\n\n    0.7  = Validation score   (accuracy)\n\n\n    3.3s     = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: LightGBM ... Training model for up to 978.27s of the 978.25s of remaining time.\n\n\n    0.6667   = Validation score   (accuracy)\n\n\n    5.14s    = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: RandomForestGini ... Training model for up to 973.08s of the 973.06s of remaining time.\n\n\n    0.6278   = Validation score   (accuracy)\n\n\n    1.9s     = Training   runtime\n\n\n    0.1s     = Validation runtime\n\n\nFitting model: RandomForestEntr ... Training model for up to 971.04s of the 971.01s of remaining time.\n\n\n    0.6167   = Validation score   (accuracy)\n\n\n    1.1s     = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: CatBoost ... Training model for up to 969.86s of the 969.85s of remaining time.\n\n\n    0.6667   = Validation score   (accuracy)\n\n\n    20.8s    = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: ExtraTreesGini ... Training model for up to 949.03s of the 949.02s of remaining time.\n\n\n    0.6167   = Validation score   (accuracy)\n\n\n    0.76s    = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: ExtraTreesEntr ... Training model for up to 948.22s of the 948.21s of remaining time.\n\n\n    0.6222   = Validation score   (accuracy)\n\n\n    0.82s    = Training   runtime\n\n\n    0.37s    = Validation runtime\n\n\nFitting model: NeuralNetFastAI ... Training model for up to 947.0s of the 946.99s of remaining time.\n\n\n    0.6722   = Validation score   (accuracy)\n\n\n    4.39s    = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: XGBoost ... Training model for up to 942.54s of the 942.53s of remaining time.\n\n\n    0.6778   = Validation score   (accuracy)\n\n\n    14.53s   = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: NeuralNetTorch ... Training model for up to 927.97s of the 927.96s of remaining time.\n\n\n    0.6889   = Validation score   (accuracy)\n\n\n    4.53s    = Training   runtime\n\n\n    0.04s    = Validation runtime\n\n\nFitting model: LightGBMLarge ... Training model for up to 923.39s of the 923.38s of remaining time.\n\n\n    0.6556   = Validation score   (accuracy)\n\n\n    14.93s   = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 908.4s of remaining time.\n\n\n    0.7056   = Validation score   (accuracy)\n\n\n    0.34s    = Training   runtime\n\n\n    0.0s     = Validation runtime\n\n\nAutoGluon training complete, total runtime = 91.99s ... Best model: \"WeightedEnsemble_L2\"\n\n\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240410_105110\\\")\n\n\nComputing feature importance via permutation shuffling for 1605 features using 385 rows with 5 shuffle sets...\n\n\n    632.25s = Expected runtime (126.45s per shuffle set)\n\n\n    96.45s  = Actual runtime (Completed 5 of 5 shuffle sets)\n\n\n                  model  score_test  score_val  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0              LightGBM    0.633766   0.666667        0.022923       0.007973   5.138871                 0.022923                0.007973           5.138871            1       True          4\n1            LightGBMXT    0.631169   0.700000        0.017940       0.013953   3.296461                 0.017940                0.013953           3.296461            1       True          3\n2               XGBoost    0.631169   0.677778        0.028903       0.012957  14.533360                 0.028903                0.012957          14.533360            1       True         11\n3        ExtraTreesGini    0.631169   0.616667        0.038871       0.025913   0.757643                 0.038871                0.025913           0.757643            1       True          8\n4   WeightedEnsemble_L2    0.631169   0.705556        0.062788       0.471563  19.741663                 0.001993                0.000997           0.341957            2       True         14\n5        ExtraTreesEntr    0.623377   0.622222        0.038869       0.365210   0.816270                 0.038869                0.365210           0.816270            1       True          9\n6        KNeighborsDist    0.623377   0.572222        0.040864       0.031893   0.123586                 0.040864                0.031893           0.123586            1       True          2\n7        KNeighborsUnif    0.623377   0.572222        0.042856       0.456613  16.103245                 0.042856                0.456613          16.103245            1       True          1\n8      RandomForestEntr    0.620779   0.616667        0.049834       0.034883   1.101706                 0.049834                0.034883           1.101706            1       True          6\n9        NeuralNetTorch    0.615584   0.688889        0.043418       0.036877   4.525023                 0.043418                0.036877           4.525023            1       True         12\n10     RandomForestGini    0.610390   0.627778        0.068770       0.098670   1.895792                 0.068770                0.098670           1.895792            1       True          5\n11      NeuralNetFastAI    0.589610   0.672222        0.024916       0.031893   4.388833                 0.024916                0.031893           4.388833            1       True         10\n12        LightGBMLarge    0.576623   0.655556        0.014950       0.005981  14.932064                 0.014950                0.005981          14.932064            1       True         13\n13             CatBoost    0.571429   0.666667        0.045847       0.016943  20.801021                 0.045847                0.016943          20.801021            1       True          7\n\n\n\n\n\n6.4.4 Return\n\n# Filtering the DataFrame to get features where importance &gt; 0 and p_value &lt; 0.05\nfiltered_importance = importance[(importance['importance'] &gt; 0) & (importance['p_value'] &lt; 0.05)]\n# Counting the number of such features\nnumber_of_features = filtered_importance.shape[0]\n# Printing the result\nprint(f\"Number of features with importance &gt; 0 and p_value &lt; 0.05: {number_of_features}\")\n\nNumber of features with importance &gt; 0 and p_value &lt; 0.05: 703\n\n\n\n\n# Display the top 20 rows of the Importance DataFrame\nprint(\"Top 20 rows of Importance DataFrame:\")\nprint(importance.head(20))\n\nTop 20 rows of Importance DataFrame:\n              importance    stddev   p_value  n  p99_high   p99_low\nRP11-641D5.1    0.012987  0.002597  0.000182  5  0.018335  0.007639\nCKM             0.012468  0.002173  0.000106  5  0.016942  0.007993\nZBED3-AS1       0.011948  0.003939  0.001234  5  0.020059  0.003837\nPPP1R14C        0.010909  0.005323  0.005082  5  0.021869 -0.000051\nRPS10-NUDT3     0.010909  0.006201  0.008525  5  0.023677 -0.001859\nAOX1            0.010909  0.002173  0.000179  5  0.015384  0.006435\nRP11-411B6.6    0.010909  0.003387  0.000985  5  0.017882  0.003936\nNTRK1           0.010390  0.007113  0.015453  5  0.025036 -0.004257\nGAPDHP1         0.010390  0.001837  0.000112  5  0.014171  0.006608\nSPP1            0.009870  0.003387  0.001431  5  0.016843  0.002897\nCLDN1           0.009351  0.004718  0.005705  5  0.019066 -0.000365\nADORA3          0.009351  0.003939  0.003027  5  0.017461  0.001240\nAC016292.3      0.008831  0.004718  0.006931  5  0.018546 -0.000884\nCORIN           0.008312  0.002845  0.001419  5  0.014170  0.002453\nGAS6            0.008312  0.001162  0.000045  5  0.010703  0.005920\nSTMN2           0.007792  0.001837  0.000344  5  0.011574  0.004011\nMAPK13          0.007792  0.001837  0.000344  5  0.011574  0.004011\nLYPD6B          0.007792  0.003181  0.002704  5  0.014342  0.001242\nTBC1D3B         0.007273  0.002173  0.000853  5  0.011747  0.002798\nMT1M            0.007273  0.003853  0.006733  5  0.015205 -0.000660\n\n\n\n\n# Display the top 20 rows of the Leaderboard DataFrame\nprint(\"\\nTop 20 rows of Leaderboard DataFrame:\")\nprint(leaderboard.head(20))\n\n\nTop 20 rows of Leaderboard DataFrame:\n                  model  score_test  score_val  pred_time_test  pred_time_val  \\\n0              LightGBM    0.633766   0.666667        0.022923       0.007973   \n1            LightGBMXT    0.631169   0.700000        0.017940       0.013953   \n2               XGBoost    0.631169   0.677778        0.028903       0.012957   \n3        ExtraTreesGini    0.631169   0.616667        0.038871       0.025913   \n4   WeightedEnsemble_L2    0.631169   0.705556        0.062788       0.471563   \n5        ExtraTreesEntr    0.623377   0.622222        0.038869       0.365210   \n6        KNeighborsDist    0.623377   0.572222        0.040864       0.031893   \n7        KNeighborsUnif    0.623377   0.572222        0.042856       0.456613   \n8      RandomForestEntr    0.620779   0.616667        0.049834       0.034883   \n9        NeuralNetTorch    0.615584   0.688889        0.043418       0.036877   \n10     RandomForestGini    0.610390   0.627778        0.068770       0.098670   \n11      NeuralNetFastAI    0.589610   0.672222        0.024916       0.031893   \n12        LightGBMLarge    0.576623   0.655556        0.014950       0.005981   \n13             CatBoost    0.571429   0.666667        0.045847       0.016943   \n\n     fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0    5.138871                 0.022923                0.007973   \n1    3.296461                 0.017940                0.013953   \n2   14.533360                 0.028903                0.012957   \n3    0.757643                 0.038871                0.025913   \n4   19.741663                 0.001993                0.000997   \n5    0.816270                 0.038869                0.365210   \n6    0.123586                 0.040864                0.031893   \n7   16.103245                 0.042856                0.456613   \n8    1.101706                 0.049834                0.034883   \n9    4.525023                 0.043418                0.036877   \n10   1.895792                 0.068770                0.098670   \n11   4.388833                 0.024916                0.031893   \n12  14.932064                 0.014950                0.005981   \n13  20.801021                 0.045847                0.016943   \n\n    fit_time_marginal  stack_level  can_infer  fit_order  \n0            5.138871            1       True          4  \n1            3.296461            1       True          3  \n2           14.533360            1       True         11  \n3            0.757643            1       True          8  \n4            0.341957            2       True         14  \n5            0.816270            1       True          9  \n6            0.123586            1       True          2  \n7           16.103245            1       True          1  \n8            1.101706            1       True          6  \n9            4.525023            1       True         12  \n10           1.895792            1       True          5  \n11           4.388833            1       True         10  \n12          14.932064            1       True         13  \n13          20.801021            1       True          7  \n\n\n\n\n\n6.4.5 Save Data\n\n# Save the Importance DataFrame to a CSV file\nimportance.to_csv('../test_TransProPy/data/Insignificant_correlation_Autogluon_TimeLimit_importance.csv', index=False)\n\n# Save the Leaderboard DataFrame to a CSV file\nleaderboard.to_csv('../test_TransProPy/data/Insignificant_correlation_Autogluon_TimeLimit_leaderboard.csv', index=False)"
  },
  {
    "objectID": "AutogluonTimeLimit.html#significant-correlation-c",
    "href": "AutogluonTimeLimit.html#significant-correlation-c",
    "title": "6  AutogluonTimeLimit.py",
    "section": "6.5 Significant Correlation",
    "text": "6.5 Significant Correlation\n\n\nPlease note:Data characteristics: Features have strong correlation with the classification.\n\n\n\n6.5.1 Import the corresponding module\n\nfrom TransProPy.AutogluonTimeLimit import Autogluon_TimeLimit\n\n\n\n\n6.5.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/four_methods_degs_intersection.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n  Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0        A2M         16.808499         16.506184         17.143433   \n1      A2ML1          1.584963          9.517669          7.434628   \n2      AADAC          4.000000          2.584963          1.584963   \n3    AADACL2          1.000000          1.000000          0.000000   \n4     ABCA12          4.523562          4.321928          3.906891   \n5    ABCA17P          4.584963          5.169925          3.807355   \n6      ABCA9          9.753217          6.906891          3.459432   \n7      ABCB4          9.177420          6.700440          5.000000   \n8      ABCB5         10.134426          4.169925          9.167418   \n9     ABCC11         10.092757          6.491853          5.459432   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0         17.760739         14.766839         16.263691         16.035207   \n1          2.584963          1.584963          2.584963          5.285402   \n2          0.000000          0.000000          0.000000          3.321928   \n3          0.000000          1.000000          0.000000          0.000000   \n4          3.459432          1.584963          3.000000          4.321928   \n5          8.366322          7.228819          7.076816          4.584963   \n6          2.584963          6.357552          6.475733          7.330917   \n7          9.342075         10.392317          7.383704         11.032735   \n8          4.906891         11.340963          3.169925         11.161762   \n9          6.807355          4.247928          5.459432          5.977280   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0         18.355114         16.959379  \n1          2.584963          3.584963  \n2          1.000000          4.584963  \n3          0.000000          1.000000  \n4          4.807355          3.700440  \n5          6.409391          7.139551  \n6          7.954196          9.177420  \n7         10.082149         10.088788  \n8          4.643856         12.393927  \n9          5.614710          8.233620  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      2\n5  TCGA-GN-A26A-06A      2\n6  TCGA-D3-A3BZ-06A      2\n7  TCGA-D3-A51G-06A      2\n8  TCGA-EE-A29R-06A      2\n9  TCGA-D3-A2JE-06A      2\n\n\n\n\n\n6.5.3 Autogluon_TimeLimit\n\nimportance, leaderboard = Autogluon_TimeLimit(\n    gene_data_path='../test_TransProPy/data/four_methods_degs_intersection.csv', \n    class_data_path='../test_TransProPy/data/class.csv', \n    label_column='class',  \n    test_size=0.3, \n    threshold=0.9, \n    random_feature=None, \n    num_bag_folds=None, \n    num_stack_levels=None, \n    time_limit=1000, \n    random_state=42\n    )\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20240410_105420\\\"\n\n\nBeginning AutoGluon training ... Time limit = 1000s\n\n\nAutoGluon will save models to \"AutogluonModels\\ag-20240410_105420\\\"\n\n\nAutoGluon Version:  0.8.2\n\n\nPython Version:     3.10.11\n\n\nOperating System:   Windows\n\n\nPlatform Machine:   AMD64\n\n\nPlatform Version:   10.0.19044\n\n\nDisk Space Avail:   216.61 GB / 925.93 GB (23.4%)\n\n\nTrain Data Rows:    896\n\n\nTrain Data Columns: 1605\n\n\nLabel Column: class\n\n\nPreprocessing data ...\n\n\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\n\n    2 unique label values:  [2, 1]\n\n\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n\n\nSelected class &lt;--&gt; label mapping:  class 1 = 2, class 0 = 1\n\n\n    Note: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (2) vs negative (1) class.\n    To explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n\n\nUsing Feature Generators to preprocess the data ...\n\n\nFitting AutoMLPipelineFeatureGenerator...\n\n\n    Available Memory:                    18643.86 MB\n\n\n    Train Data (Original)  Memory Usage: 11.5 MB (0.1% of available memory)\n\n\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\n\n    Stage 1 Generators:\n\n\n        Fitting AsTypeFeatureGenerator...\n\n\n    Stage 2 Generators:\n\n\n        Fitting FillNaFeatureGenerator...\n\n\n    Stage 3 Generators:\n\n\n        Fitting IdentityFeatureGenerator...\n\n\n    Stage 4 Generators:\n\n\n        Fitting DropUniqueFeatureGenerator...\n\n\n    Stage 5 Generators:\n\n\n        Fitting DropDuplicatesFeatureGenerator...\n\n\n    Types of features in original data (raw dtype, special dtypes):\n\n\n        ('float', []) : 1605 | ['A2M', 'A2ML1', 'ABCA12', 'ABCA17P', 'ABCA9', ...]\n\n\n    Types of features in processed data (raw dtype, special dtypes):\n\n\n        ('float', []) : 1605 | ['A2M', 'A2ML1', 'ABCA12', 'ABCA17P', 'ABCA9', ...]\n\n\n    1.5s = Fit runtime\n\n\n    1605 features in original data used to generate 1605 features in processed data.\n\n\n    Train Data (Processed) Memory Usage: 11.5 MB (0.1% of available memory)\n\n\nData preprocessing and feature engineering runtime = 1.6s ...\n\n\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\n\n    To change this, specify the eval_metric parameter of Predictor()\n\n\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 716, Val Rows: 180\n\n\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\n\n\nFitting 13 L1 models ...\n\n\nFitting model: KNeighborsUnif ... Training model for up to 998.4s of the 998.39s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.13s    = Training   runtime\n\n\n    0.05s    = Validation runtime\n\n\nFitting model: KNeighborsDist ... Training model for up to 998.2s of the 998.19s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.13s    = Training   runtime\n\n\n    0.04s    = Validation runtime\n\n\nFitting model: LightGBMXT ... Training model for up to 998.01s of the 997.99s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    1.03s    = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: LightGBM ... Training model for up to 996.95s of the 996.93s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    1.48s    = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: RandomForestGini ... Training model for up to 995.44s of the 995.42s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.78s    = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: RandomForestEntr ... Training model for up to 994.6s of the 994.59s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.74s    = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: CatBoost ... Training model for up to 993.8s of the 993.79s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    19.23s   = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: ExtraTreesGini ... Training model for up to 974.54s of the 974.53s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.71s    = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: ExtraTreesEntr ... Training model for up to 973.78s of the 973.77s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.82s    = Training   runtime\n\n\n    0.2s     = Validation runtime\n\n\nFitting model: NeuralNetFastAI ... Training model for up to 972.72s of the 972.71s of remaining time.\n\n\nNo improvement since epoch 0: early stopping\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    1.24s    = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: XGBoost ... Training model for up to 971.42s of the 971.41s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    3.58s    = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: NeuralNetTorch ... Training model for up to 967.81s of the 967.79s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    2.73s    = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: LightGBMLarge ... Training model for up to 965.02s of the 965.01s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    2.55s    = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 962.41s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.38s    = Training   runtime\n\n\n    0.0s     = Validation runtime\n\n\nAutoGluon training complete, total runtime = 38.0s ... Best model: \"WeightedEnsemble_L2\"\n\n\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240410_105420\\\")\n\n\nComputing feature importance via permutation shuffling for 1605 features using 385 rows with 5 shuffle sets...\n\n\n    168.07s = Expected runtime (33.61s per shuffle set)\n\n\n    45.73s  = Actual runtime (Completed 5 of 5 shuffle sets)\n\n\n                  model  score_test  score_val  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0         LightGBMLarge    1.000000        1.0        0.007975       0.008970   2.553169                 0.007975                0.008970           2.553169            1       True         13\n1   WeightedEnsemble_L2    1.000000        1.0        0.008970       0.009967   2.928912                 0.000996                0.000997           0.375742            2       True         14\n2              LightGBM    1.000000        1.0        0.014950       0.008970   1.477767                 0.014950                0.008970           1.477767            1       True          4\n3       NeuralNetFastAI    1.000000        1.0        0.024917       0.014949   1.237660                 0.024917                0.014949           1.237660            1       True         10\n4        ExtraTreesEntr    1.000000        1.0        0.034883       0.202120   0.822475                 0.034883                0.202120           0.822475            1       True          9\n5        ExtraTreesGini    1.000000        1.0        0.035880       0.024917   0.707324                 0.035880                0.024917           0.707324            1       True          8\n6        KNeighborsDist    1.000000        1.0        0.045846       0.036877   0.127574                 0.045846                0.036877           0.127574            1       True          2\n7      RandomForestEntr    1.000000        1.0        0.048836       0.029900   0.743676                 0.048836                0.029900           0.743676            1       True          6\n8        KNeighborsUnif    1.000000        1.0        0.052823       0.049834   0.133554                 0.052823                0.049834           0.133554            1       True          1\n9      RandomForestGini    1.000000        1.0        0.062790       0.029900   0.784480                 0.062790                0.029900           0.784480            1       True          5\n10       NeuralNetTorch    1.000000        1.0        0.063787       0.032891   2.728810                 0.063787                0.032891           2.728810            1       True         12\n11           LightGBMXT    0.997403        1.0        0.014951       0.008970   1.033237                 0.014951                0.008970           1.033237            1       True          3\n12             CatBoost    0.997403        1.0        0.033887       0.015947  19.227359                 0.033887                0.015947          19.227359            1       True          7\n13              XGBoost    0.994805        1.0        0.029900       0.015947   3.576393                 0.029900                0.015947           3.576393            1       True         11\n\n\n\n\n\n6.5.4 Return\n\n# Filtering the DataFrame to get features where importance &gt; 0 and p_value &lt; 0.05\nfiltered_importance = importance[(importance['importance'] &gt; 0) & (importance['p_value'] &lt; 0.05)]\n# Counting the number of such features\nnumber_of_features = filtered_importance.shape[0]\n# Printing the result\nprint(f\"Number of features with importance &gt; 0 and p_value &lt; 0.05: {number_of_features}\")\n\nNumber of features with importance &gt; 0 and p_value &lt; 0.05: 2\n\n\n\n\n# Display the top 20 rows of the Importance DataFrame\nprint(\"Top 20 rows of Importance DataFrame:\")\nprint(importance.head(20))\n\nTop 20 rows of Importance DataFrame:\n               importance    stddev   p_value  n  p99_high   p99_low\nISY1-RAB43       0.235844  0.017463  0.000004  5    0.2718  0.199888\nRP11-231C14.4    0.235844  0.017463  0.000004  5    0.2718  0.199888\nPSORS1C1         0.000000  0.000000  0.500000  5    0.0000  0.000000\nPSMC1P1          0.000000  0.000000  0.500000  5    0.0000  0.000000\nPRTG             0.000000  0.000000  0.500000  5    0.0000  0.000000\nPRSS8            0.000000  0.000000  0.500000  5    0.0000  0.000000\nPRSS53           0.000000  0.000000  0.500000  5    0.0000  0.000000\nPRSS3            0.000000  0.000000  0.500000  5    0.0000  0.000000\nPRSS22           0.000000  0.000000  0.500000  5    0.0000  0.000000\nPRR19            0.000000  0.000000  0.500000  5    0.0000  0.000000\nPRR15L           0.000000  0.000000  0.500000  5    0.0000  0.000000\nPROM2            0.000000  0.000000  0.500000  5    0.0000  0.000000\nPRODH            0.000000  0.000000  0.500000  5    0.0000  0.000000\nA2M              0.000000  0.000000  0.500000  5    0.0000  0.000000\nPRKCQ            0.000000  0.000000  0.500000  5    0.0000  0.000000\nPRF1             0.000000  0.000000  0.500000  5    0.0000  0.000000\nPRAME            0.000000  0.000000  0.500000  5    0.0000  0.000000\nPPP2R2C          0.000000  0.000000  0.500000  5    0.0000  0.000000\nPPP1R1B          0.000000  0.000000  0.500000  5    0.0000  0.000000\nPPP1R14C         0.000000  0.000000  0.500000  5    0.0000  0.000000\n\n\n\n\n# Display the top 20 rows of the Leaderboard DataFrame\nprint(\"\\nTop 20 rows of Leaderboard DataFrame:\")\nprint(leaderboard.head(20))\n\n\nTop 20 rows of Leaderboard DataFrame:\n                  model  score_test  score_val  pred_time_test  pred_time_val  \\\n0         LightGBMLarge    1.000000        1.0        0.007975       0.008970   \n1   WeightedEnsemble_L2    1.000000        1.0        0.008970       0.009967   \n2              LightGBM    1.000000        1.0        0.014950       0.008970   \n3       NeuralNetFastAI    1.000000        1.0        0.024917       0.014949   \n4        ExtraTreesEntr    1.000000        1.0        0.034883       0.202120   \n5        ExtraTreesGini    1.000000        1.0        0.035880       0.024917   \n6        KNeighborsDist    1.000000        1.0        0.045846       0.036877   \n7      RandomForestEntr    1.000000        1.0        0.048836       0.029900   \n8        KNeighborsUnif    1.000000        1.0        0.052823       0.049834   \n9      RandomForestGini    1.000000        1.0        0.062790       0.029900   \n10       NeuralNetTorch    1.000000        1.0        0.063787       0.032891   \n11           LightGBMXT    0.997403        1.0        0.014951       0.008970   \n12             CatBoost    0.997403        1.0        0.033887       0.015947   \n13              XGBoost    0.994805        1.0        0.029900       0.015947   \n\n     fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0    2.553169                 0.007975                0.008970   \n1    2.928912                 0.000996                0.000997   \n2    1.477767                 0.014950                0.008970   \n3    1.237660                 0.024917                0.014949   \n4    0.822475                 0.034883                0.202120   \n5    0.707324                 0.035880                0.024917   \n6    0.127574                 0.045846                0.036877   \n7    0.743676                 0.048836                0.029900   \n8    0.133554                 0.052823                0.049834   \n9    0.784480                 0.062790                0.029900   \n10   2.728810                 0.063787                0.032891   \n11   1.033237                 0.014951                0.008970   \n12  19.227359                 0.033887                0.015947   \n13   3.576393                 0.029900                0.015947   \n\n    fit_time_marginal  stack_level  can_infer  fit_order  \n0            2.553169            1       True         13  \n1            0.375742            2       True         14  \n2            1.477767            1       True          4  \n3            1.237660            1       True         10  \n4            0.822475            1       True          9  \n5            0.707324            1       True          8  \n6            0.127574            1       True          2  \n7            0.743676            1       True          6  \n8            0.133554            1       True          1  \n9            0.784480            1       True          5  \n10           2.728810            1       True         12  \n11           1.033237            1       True          3  \n12          19.227359            1       True          7  \n13           3.576393            1       True         11  \n\n\n\n\n\n6.5.5 Save Data\n\n# Save the Importance DataFrame to a CSV file\nimportance.to_csv('../test_TransProPy/data/significant_correlation_Autogluon_TimeLimit_importance.csv', index=False)\n\n# Save the Leaderboard DataFrame to a CSV file\nleaderboard.to_csv('../test_TransProPy/data/significant_correlation_Autogluon_TimeLimit_leaderboard.csv', index=False)"
  },
  {
    "objectID": "AutogluonTimeLimit.html#references-c",
    "href": "AutogluonTimeLimit.html#references-c",
    "title": "6  AutogluonTimeLimit.py",
    "section": "6.6 References",
    "text": "6.6 References\n\n6.6.1 Scientific Publications\n\n\nErickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., & Smola, A. (2020). AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data. arXiv preprint arXiv:2003.06505.\nFakoor, R., Mueller, J., Erickson, N., Chaudhari, P., & Smola, A. J. (2020). Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation. arXiv preprint arXiv:2006.14284. \nShi, X., Mueller, J., Erickson, N., Li, M., & Smola, A. (2021). Multimodal AutoML on Structured Tables with Text Fields. In AutoML@ICML 2021. \n\n\n\n\n6.6.2 Articles\n\n\nPrasanna, S. (2020, March 31). Machine learning with AutoGluon, an open source AutoML library. AWS Open Source Blog. \nSun, Y., Wu, C., Zhang, Z., He, T., Mueller, J., & Zhang, H. (n.d.). (2020). Image classification on Kaggle using AutoGluon. Medium.\nErickson, N., Mueller, J., Zhang, H., & Kamakoti, B. (2019). AutoGluon: Deep Learning AutoML. Medium.\n\n\n\n\n6.6.3 Documentation\n\n\nAutoGluon Predictors – AutoGluon Documentation 0.1.0 documentation"
  },
  {
    "objectID": "AutoGluonSelectML.html#parameters-d",
    "href": "AutoGluonSelectML.html#parameters-d",
    "title": "7  AutoGluonSelectML.py",
    "section": "7.1 Parameters",
    "text": "7.1 Parameters\n\n\ngene_data_path (str):\n\nPath to the gene expression data CSV file.\nFor example: ‘../data/gene_tpm.csv’\n\nclass_data_path (str):\n\nPath to the class data CSV file.\nFor example: ‘../data/tumor_class.csv’\n\nlabel_column (str):\n\nName of the column in the dataset that is the target label for prediction.\n\ntest_size (float):\n\nProportion of the data to be used as the test set.\n\nthreshold (float):\n\nThe threshold used to filter out rows based on the proportion of non-zero values.\n\nhyperparameters (dict, optional):\n\nDictionary of hyperparameters for the models.\nFor example: {‘GBM’: {}, ‘RF’: {}}\n\nrandom_feature (int, optional):\n\nThe number of random feature to select. If None, no random feature selection is performed.\nDefault is None.\n\nnum_bag_folds (int, optional)\n\nPlease note: This parameter annotation source can be referred to the documentation link in References.\nNumber of folds used for bagging of models. When num_bag_folds = k, training time is roughly increased by a factor of k (set = 0 to disable bagging). Disabled by default (0), but we recommend values between 5-10 to maximize predictive performance. Increasing num_bag_folds will result in models with lower bias but that are more prone to overfitting. num_bag_folds = 1 is an invalid value, and will raise a ValueError. Values &gt; 10 may produce diminishing returns, and can even harm overall results due to overfitting. To further improve predictions, avoid increasing num_bag_folds much beyond 10 and instead increase num_bag_sets.\ndefault = None\n\nnum_stack_levels (int, optional)\n\nPlease note: This parameter annotation source can be referred to the documentation link in References.\nNumber of stacking levels to use in stack ensemble. Roughly increases model training time by factor of num_stack_levels+1 (set = 0 to disable stack ensembling). Disabled by default (0), but we recommend values between 1-3 to maximize predictive performance. To prevent overfitting, num_bag_folds &gt;= 2 must also be set or else a ValueError will be raised.\ndefault = None\n\ntime_limit (int, optional):\n\nTime limit for training in seconds.\ndefault is 120.\n\nrandom_state (int, optional):\n\nThe seed used by the random number generator.\ndefault is 42."
  },
  {
    "objectID": "AutoGluonSelectML.html#returns-d",
    "href": "AutoGluonSelectML.html#returns-d",
    "title": "7  AutoGluonSelectML.py",
    "section": "7.2 Return",
    "text": "7.2 Return\n\n\nimportance (DataFrame):\n\nDataFrame containing feature importance.\n\nleaderboard (DataFrame):\n\nDataFrame containing model performance on the test data."
  },
  {
    "objectID": "AutoGluonSelectML.html#usage-of-autogluon_selectml",
    "href": "AutoGluonSelectML.html#usage-of-autogluon_selectml",
    "title": "7  AutoGluonSelectML.py",
    "section": "7.3 Usage of Autogluon_SelectML",
    "text": "7.3 Usage of Autogluon_SelectML\nPerforming training and prediction tasks on tabular data using Autogluon.\n\n7.3.1 Objectives\n\n7.3.1.1 Model Training and Selection\n\nAutogluon will attempt various models and hyperparameter combinations within a given time limit to find the best-performing model on the test data. During training, Autogluon may output training logs displaying performance metrics and progress information for different models. The goal is to select the best-performing model for use in subsequent prediction tasks.\n\n\n\n7.3.1.2 Leaderboard\n\nThe leaderboard displays performance scores of different models on the test data, typically including metrics like accuracy, precision, recall, and more. The purpose is to assist users in understanding the performance of different models to choose the most suitable model for predictions.\n\n\n\n7.3.1.3 Importance\n\nFeature importance indicates which features are most critical for the model’s prediction performance. The purpose is to help users understand the importance of specific features in the data, which can be used for feature selection or further data analysis.\n\n\n\n\n7.3.2 Note\n\nPlease note that Autogluon’s output results may vary depending on your data and task. You can review the generated model leaderboard and feature importance to understand model performance and the significance of specific features in the data. These results can aid you in making better predictions and decisions."
  },
  {
    "objectID": "AutoGluonSelectML.html#insignificant-correlation-d",
    "href": "AutoGluonSelectML.html#insignificant-correlation-d",
    "title": "7  AutoGluonSelectML.py",
    "section": "7.4 Insignificant Correlation",
    "text": "7.4 Insignificant Correlation\n\n\nPlease note:Data characteristics: Features have weak correlation with the classification.\nRandomly shuffling the class labels to a certain extent simulates reducing the correlation.\n\n\n\n7.4.1 Import the corresponding module\n\nfrom TransProPy.AutogluonSelectML import AutoGluon_SelectML\n\n\n\n\n7.4.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/four_methods_degs_intersection.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n  Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0        A2M         16.808499         16.506184         17.143433   \n1      A2ML1          1.584963          9.517669          7.434628   \n2      AADAC          4.000000          2.584963          1.584963   \n3    AADACL2          1.000000          1.000000          0.000000   \n4     ABCA12          4.523562          4.321928          3.906891   \n5    ABCA17P          4.584963          5.169925          3.807355   \n6      ABCA9          9.753217          6.906891          3.459432   \n7      ABCB4          9.177420          6.700440          5.000000   \n8      ABCB5         10.134426          4.169925          9.167418   \n9     ABCC11         10.092757          6.491853          5.459432   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0         17.760739         14.766839         16.263691         16.035207   \n1          2.584963          1.584963          2.584963          5.285402   \n2          0.000000          0.000000          0.000000          3.321928   \n3          0.000000          1.000000          0.000000          0.000000   \n4          3.459432          1.584963          3.000000          4.321928   \n5          8.366322          7.228819          7.076816          4.584963   \n6          2.584963          6.357552          6.475733          7.330917   \n7          9.342075         10.392317          7.383704         11.032735   \n8          4.906891         11.340963          3.169925         11.161762   \n9          6.807355          4.247928          5.459432          5.977280   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0         18.355114         16.959379  \n1          2.584963          3.584963  \n2          1.000000          4.584963  \n3          0.000000          1.000000  \n4          4.807355          3.700440  \n5          6.409391          7.139551  \n6          7.954196          9.177420  \n7         10.082149         10.088788  \n8          4.643856         12.393927  \n9          5.614710          8.233620  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/random_classification_class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      1\n5  TCGA-GN-A26A-06A      1\n6  TCGA-D3-A3BZ-06A      1\n7  TCGA-D3-A51G-06A      1\n8  TCGA-EE-A29R-06A      1\n9  TCGA-D3-A2JE-06A      1\n\n\n\n\n\n7.4.3 Autogluon_SelectML\n\n\nThe core purpose of choosing Autogluon_SelectML — to select a larger feature set in AutoGluon that includes both important and secondary features — is reflected in the following custom hyperparameters configuration. This setup is designed to utilize multiple model types so that the models can consider a broader range of features.\nThis configuration encompasses neural networks (using PyTorch and FastAI), gradient boosting machines (LightGBM, XGBoost, and CatBoost), random forests (RF), extremely randomized trees (XT), K-nearest neighbors (KNN), and linear regression (LR).\n\n\n\nimportance, leaderboard = AutoGluon_SelectML(\n    gene_data_path='../test_TransProPy/data/four_methods_degs_intersection.csv', \n    class_data_path='../test_TransProPy/data/random_classification_class.csv', \n    label_column='class', \n    test_size=0.3, \n    threshold=0.9, \n    hyperparameters={\n        'GBM': {}, \n        'RF': {},\n        'CAT': {}, \n        'XGB' : {},\n        # 'NN_TORCH': {}, \n        # 'FASTAI': {},\n        'XT': {}, \n        'KNN': {},\n        'LR': {}\n        },\n    random_feature=None, \n    num_bag_folds=None, \n    num_stack_levels=None, \n    time_limit=1000, \n    random_state=42\n    )\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20240410_105553\\\"\n\n\nBeginning AutoGluon training ... Time limit = 1000s\n\n\nAutoGluon will save models to \"AutogluonModels\\ag-20240410_105553\\\"\n\n\nAutoGluon Version:  0.8.2\n\n\nPython Version:     3.10.11\n\n\nOperating System:   Windows\n\n\nPlatform Machine:   AMD64\n\n\nPlatform Version:   10.0.19044\n\n\nDisk Space Avail:   216.58 GB / 925.93 GB (23.4%)\n\n\nTrain Data Rows:    896\n\n\nTrain Data Columns: 1605\n\n\nLabel Column: class\n\n\nPreprocessing data ...\n\n\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\n\n    2 unique label values:  [1, 2]\n\n\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n\n\nSelected class &lt;--&gt; label mapping:  class 1 = 2, class 0 = 1\n\n\n    Note: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (2) vs negative (1) class.\n    To explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n\n\nUsing Feature Generators to preprocess the data ...\n\n\nFitting AutoMLPipelineFeatureGenerator...\n\n\n    Available Memory:                    19283.74 MB\n\n\n    Train Data (Original)  Memory Usage: 11.5 MB (0.1% of available memory)\n\n\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\n\n    Stage 1 Generators:\n\n\n        Fitting AsTypeFeatureGenerator...\n\n\n    Stage 2 Generators:\n\n\n        Fitting FillNaFeatureGenerator...\n\n\n    Stage 3 Generators:\n\n\n        Fitting IdentityFeatureGenerator...\n\n\n    Stage 4 Generators:\n\n\n        Fitting DropUniqueFeatureGenerator...\n\n\n    Stage 5 Generators:\n\n\n        Fitting DropDuplicatesFeatureGenerator...\n\n\n    Types of features in original data (raw dtype, special dtypes):\n\n\n        ('float', []) : 1605 | ['A2M', 'A2ML1', 'ABCA12', 'ABCA17P', 'ABCA9', ...]\n\n\n    Types of features in processed data (raw dtype, special dtypes):\n\n\n        ('float', []) : 1605 | ['A2M', 'A2ML1', 'ABCA12', 'ABCA17P', 'ABCA9', ...]\n\n\n    1.5s = Fit runtime\n\n\n    1605 features in original data used to generate 1605 features in processed data.\n\n\n    Train Data (Processed) Memory Usage: 11.5 MB (0.1% of available memory)\n\n\nData preprocessing and feature engineering runtime = 1.68s ...\n\n\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\n\n    To change this, specify the eval_metric parameter of Predictor()\n\n\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 716, Val Rows: 180\n\n\nUser-specified model hyperparameters to be fit:\n{\n    'GBM': {},\n    'RF': {},\n    'CAT': {},\n    'XGB': {},\n    'XT': {},\n    'KNN': {},\n    'LR': {},\n}\n\n\nFitting 7 L1 models ...\n\n\nFitting model: KNeighbors ... Training model for up to 998.32s of the 998.31s of remaining time.\n\n\n    0.5722   = Validation score   (accuracy)\n\n\n    1.19s    = Training   runtime\n\n\n    0.15s    = Validation runtime\n\n\nFitting model: LightGBM ... Training model for up to 996.97s of the 996.95s of remaining time.\n\n\n    0.6667   = Validation score   (accuracy)\n\n\n    5.73s    = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: RandomForest ... Training model for up to 991.19s of the 991.17s of remaining time.\n\n\n    0.6278   = Validation score   (accuracy)\n\n\n    1.08s    = Training   runtime\n\n\n    0.04s    = Validation runtime\n\n\nFitting model: CatBoost ... Training model for up to 990.05s of the 990.04s of remaining time.\n\n\n    0.6667   = Validation score   (accuracy)\n\n\n    20.34s   = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: ExtraTrees ... Training model for up to 969.67s of the 969.66s of remaining time.\n\n\n    0.6167   = Validation score   (accuracy)\n\n\n    0.65s    = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: XGBoost ... Training model for up to 968.96s of the 968.95s of remaining time.\n\n\n    0.6778   = Validation score   (accuracy)\n\n\n    14.39s   = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: LinearModel ... Training model for up to 954.53s of the 954.52s of remaining time.\n\n\nE:\\Anaconda\\Anaconda\\envs\\TransPro\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning:\n\nn_quantiles (1000) is greater than the total number of samples (716). n_quantiles is set to n_samples.\n\n\n\n    0.6  = Validation score   (accuracy)\n\n\n    3.23s    = Training   runtime\n\n\n    0.06s    = Validation runtime\n\n\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 951.2s of remaining time.\n\n\n    0.6833   = Validation score   (accuracy)\n\n\n    0.19s    = Training   runtime\n\n\n    0.0s     = Validation runtime\n\n\nAutoGluon training complete, total runtime = 49.04s ... Best model: \"WeightedEnsemble_L2\"\n\n\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240410_105553\\\")\n\n\nComputing feature importance via permutation shuffling for 1605 features using 385 rows with 5 shuffle sets...\n\n\n    656.26s = Expected runtime (131.25s per shuffle set)\n\n\n    205.1s  = Actual runtime (Completed 5 of 5 shuffle sets)\n\n\n                 model  score_test  score_val  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0             LightGBM    0.633766   0.666667        0.017941       0.008970   5.729842                 0.017941                0.008970           5.729842            1       True          2\n1              XGBoost    0.631169   0.677778        0.029900       0.011960  14.393618                 0.029900                0.011960          14.393618            1       True          6\n2           ExtraTrees    0.631169   0.616667        0.041859       0.026910   0.654814                 0.041859                0.026910           0.654814            1       True          5\n3  WeightedEnsemble_L2    0.631169   0.683333        0.088701       0.049834  15.656325                 0.001993                0.000997           0.187509            2       True          8\n4           KNeighbors    0.623377   0.572222        0.040864       0.147507   1.187446                 0.040864                0.147507           1.187446            1       True          1\n5         RandomForest    0.610390   0.627778        0.056808       0.036876   1.075198                 0.056808                0.036876           1.075198            1       True          3\n6             CatBoost    0.571429   0.666667        0.039868       0.016944  20.343217                 0.039868                0.016944          20.343217            1       True          4\n7          LinearModel    0.558442   0.600000        0.044850       0.058803   3.233039                 0.044850                0.058803           3.233039            1       True          7\n\n\n\n\n\n7.4.4 Return\n\n# Filtering the DataFrame to get features where importance &gt; 0 and p_value &lt; 0.05\nfiltered_importance = importance[(importance['importance'] &gt; 0) & (importance['p_value'] &lt; 0.05)]\n# Counting the number of such features\nnumber_of_features = filtered_importance.shape[0]\n# Printing the result\nprint(f\"Number of features with importance &gt; 0 and p_value &lt; 0.05: {number_of_features}\")\n\nNumber of features with importance &gt; 0 and p_value &lt; 0.05: 232\n\n\n\n\n# Display the top 20 rows of the Importance DataFrame\nprint(\"Top 20 rows of Importance DataFrame:\")\nprint(importance.head(20))\n\nTop 20 rows of Importance DataFrame:\n                importance    stddev   p_value  n  p99_high   p99_low\nNTRK1             0.020779  0.012044  0.009090  5  0.045577 -0.004019\nRP11-641D5.1      0.018701  0.004996  0.000557  5  0.028989  0.008414\nHBA2              0.015065  0.006723  0.003718  5  0.028908  0.001222\nSTMN2             0.013506  0.005631  0.002917  5  0.025101  0.001912\nNAIP              0.012987  0.002597  0.000182  5  0.018335  0.007639\nHIST2H2BF         0.012468  0.005631  0.003878  5  0.024062  0.000873\nSPP1              0.011948  0.005386  0.003852  5  0.023038  0.000858\nAC010524.2        0.011429  0.003939  0.001455  5  0.019539  0.003318\nCD24              0.010909  0.002845  0.000508  5  0.016768  0.005051\nADAMDEC1          0.010909  0.006723  0.011097  5  0.024752 -0.002934\nXIST              0.010909  0.005631  0.006165  5  0.022503 -0.000685\nRP11-1212A22.4    0.010390  0.005808  0.008065  5  0.022348 -0.001569\nRPS10-NUDT3       0.009351  0.003939  0.003027  5  0.017461  0.001240\nJAKMIP3           0.008831  0.002961  0.001314  5  0.014929  0.002733\nPPP1R14C          0.008831  0.002961  0.001314  5  0.014929  0.002733\nTBC1D3B           0.008831  0.004718  0.006931  5  0.018546 -0.000884\nSPINK5            0.008831  0.001423  0.000078  5  0.011760  0.005902\nMMP3              0.008831  0.009293  0.050387  5  0.027965 -0.010303\nPPP2R2C           0.008831  0.003939  0.003711  5  0.016942  0.000720\nSAA2              0.008312  0.002845  0.001419  5  0.014170  0.002453\n\n\n\n\n# Display the top 20 rows of the Leaderboard DataFrame\nprint(\"\\nTop 20 rows of Leaderboard DataFrame:\")\nprint(leaderboard.head(20))\n\n\nTop 20 rows of Leaderboard DataFrame:\n                 model  score_test  score_val  pred_time_test  pred_time_val  \\\n0             LightGBM    0.633766   0.666667        0.017941       0.008970   \n1              XGBoost    0.631169   0.677778        0.029900       0.011960   \n2           ExtraTrees    0.631169   0.616667        0.041859       0.026910   \n3  WeightedEnsemble_L2    0.631169   0.683333        0.088701       0.049834   \n4           KNeighbors    0.623377   0.572222        0.040864       0.147507   \n5         RandomForest    0.610390   0.627778        0.056808       0.036876   \n6             CatBoost    0.571429   0.666667        0.039868       0.016944   \n7          LinearModel    0.558442   0.600000        0.044850       0.058803   \n\n    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0   5.729842                 0.017941                0.008970   \n1  14.393618                 0.029900                0.011960   \n2   0.654814                 0.041859                0.026910   \n3  15.656325                 0.001993                0.000997   \n4   1.187446                 0.040864                0.147507   \n5   1.075198                 0.056808                0.036876   \n6  20.343217                 0.039868                0.016944   \n7   3.233039                 0.044850                0.058803   \n\n   fit_time_marginal  stack_level  can_infer  fit_order  \n0           5.729842            1       True          2  \n1          14.393618            1       True          6  \n2           0.654814            1       True          5  \n3           0.187509            2       True          8  \n4           1.187446            1       True          1  \n5           1.075198            1       True          3  \n6          20.343217            1       True          4  \n7           3.233039            1       True          7  \n\n\n\n\n\n7.4.5 Save Data\n\n# Save the Importance DataFrame to a CSV file\nimportance.to_csv('../test_TransProPy/data/Insignificant_correlation_Autogluon_SelectML_importance.csv', index=False)\n\n# Save the Leaderboard DataFrame to a CSV file\nleaderboard.to_csv('../test_TransProPy/data/Insignificant_correlation_Autogluon_SelectML_leaderboard.csv', index=False)"
  },
  {
    "objectID": "AutoGluonSelectML.html#significant-correlation-d",
    "href": "AutoGluonSelectML.html#significant-correlation-d",
    "title": "7  AutoGluonSelectML.py",
    "section": "7.5 Significant Correlation",
    "text": "7.5 Significant Correlation\n\n\nPlease note:Data characteristics: Features have strong correlation with the classification.\n\n\n\n7.5.1 Import the corresponding module\n\nfrom TransProPy.AutogluonSelectML import AutoGluon_SelectML\n\n\n\n\n7.5.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/four_methods_degs_intersection.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n  Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0        A2M         16.808499         16.506184         17.143433   \n1      A2ML1          1.584963          9.517669          7.434628   \n2      AADAC          4.000000          2.584963          1.584963   \n3    AADACL2          1.000000          1.000000          0.000000   \n4     ABCA12          4.523562          4.321928          3.906891   \n5    ABCA17P          4.584963          5.169925          3.807355   \n6      ABCA9          9.753217          6.906891          3.459432   \n7      ABCB4          9.177420          6.700440          5.000000   \n8      ABCB5         10.134426          4.169925          9.167418   \n9     ABCC11         10.092757          6.491853          5.459432   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0         17.760739         14.766839         16.263691         16.035207   \n1          2.584963          1.584963          2.584963          5.285402   \n2          0.000000          0.000000          0.000000          3.321928   \n3          0.000000          1.000000          0.000000          0.000000   \n4          3.459432          1.584963          3.000000          4.321928   \n5          8.366322          7.228819          7.076816          4.584963   \n6          2.584963          6.357552          6.475733          7.330917   \n7          9.342075         10.392317          7.383704         11.032735   \n8          4.906891         11.340963          3.169925         11.161762   \n9          6.807355          4.247928          5.459432          5.977280   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0         18.355114         16.959379  \n1          2.584963          3.584963  \n2          1.000000          4.584963  \n3          0.000000          1.000000  \n4          4.807355          3.700440  \n5          6.409391          7.139551  \n6          7.954196          9.177420  \n7         10.082149         10.088788  \n8          4.643856         12.393927  \n9          5.614710          8.233620  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      2\n5  TCGA-GN-A26A-06A      2\n6  TCGA-D3-A3BZ-06A      2\n7  TCGA-D3-A51G-06A      2\n8  TCGA-EE-A29R-06A      2\n9  TCGA-D3-A2JE-06A      2\n\n\n\n\n\n7.5.3 Autogluon_SelectML\n\n\nThe core purpose of choosing Autogluon_SelectML — to select a larger feature set in AutoGluon that includes both important and secondary features — is reflected in the following custom hyperparameters configuration. This setup is designed to utilize multiple model types so that the models can consider a broader range of features.\nThis configuration encompasses neural networks (using PyTorch and FastAI), gradient boosting machines (LightGBM, XGBoost, and CatBoost), random forests (RF), extremely randomized trees (XT), K-nearest neighbors (KNN), and linear regression (LR).\n\n\n\nimportance, leaderboard = AutoGluon_SelectML(\n    gene_data_path='../test_TransProPy/data/four_methods_degs_intersection.csv', \n    class_data_path='../test_TransProPy/data/class.csv', \n    label_column='class', \n    test_size=0.3, \n    threshold=0.9, \n    hyperparameters={\n        'GBM': {}, \n        'RF': {},\n        'CAT': {}, \n        'XGB' : {},\n        # 'NN_TORCH': {}, \n        # 'FASTAI': {},\n        'XT': {}, \n        'KNN': {},\n        'LR': {}\n        },\n    random_feature=None, \n    num_bag_folds=None, \n    num_stack_levels=None, \n    time_limit=1000, \n    random_state=42\n    )\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20240410_110009\\\"\n\n\nBeginning AutoGluon training ... Time limit = 1000s\n\n\nAutoGluon will save models to \"AutogluonModels\\ag-20240410_110009\\\"\n\n\nAutoGluon Version:  0.8.2\n\n\nPython Version:     3.10.11\n\n\nOperating System:   Windows\n\n\nPlatform Machine:   AMD64\n\n\nPlatform Version:   10.0.19044\n\n\nDisk Space Avail:   216.55 GB / 925.93 GB (23.4%)\n\n\nTrain Data Rows:    896\n\n\nTrain Data Columns: 1605\n\n\nLabel Column: class\n\n\nPreprocessing data ...\n\n\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\n\n    2 unique label values:  [2, 1]\n\n\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n\n\nSelected class &lt;--&gt; label mapping:  class 1 = 2, class 0 = 1\n\n\n    Note: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (2) vs negative (1) class.\n    To explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n\n\nUsing Feature Generators to preprocess the data ...\n\n\nFitting AutoMLPipelineFeatureGenerator...\n\n\n    Available Memory:                    18181.32 MB\n\n\n    Train Data (Original)  Memory Usage: 11.5 MB (0.1% of available memory)\n\n\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\n\n    Stage 1 Generators:\n\n\n        Fitting AsTypeFeatureGenerator...\n\n\n    Stage 2 Generators:\n\n\n        Fitting FillNaFeatureGenerator...\n\n\n    Stage 3 Generators:\n\n\n        Fitting IdentityFeatureGenerator...\n\n\n    Stage 4 Generators:\n\n\n        Fitting DropUniqueFeatureGenerator...\n\n\n    Stage 5 Generators:\n\n\n        Fitting DropDuplicatesFeatureGenerator...\n\n\n    Types of features in original data (raw dtype, special dtypes):\n\n\n        ('float', []) : 1605 | ['A2M', 'A2ML1', 'ABCA12', 'ABCA17P', 'ABCA9', ...]\n\n\n    Types of features in processed data (raw dtype, special dtypes):\n\n\n        ('float', []) : 1605 | ['A2M', 'A2ML1', 'ABCA12', 'ABCA17P', 'ABCA9', ...]\n\n\n    1.6s = Fit runtime\n\n\n    1605 features in original data used to generate 1605 features in processed data.\n\n\n    Train Data (Processed) Memory Usage: 11.5 MB (0.1% of available memory)\n\n\nData preprocessing and feature engineering runtime = 1.75s ...\n\n\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\n\n    To change this, specify the eval_metric parameter of Predictor()\n\n\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 716, Val Rows: 180\n\n\nUser-specified model hyperparameters to be fit:\n{\n    'GBM': {},\n    'RF': {},\n    'CAT': {},\n    'XGB': {},\n    'XT': {},\n    'KNN': {},\n    'LR': {},\n}\n\n\nFitting 7 L1 models ...\n\n\nFitting model: KNeighbors ... Training model for up to 998.25s of the 998.24s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.13s    = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: LightGBM ... Training model for up to 998.07s of the 998.06s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.73s    = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: RandomForest ... Training model for up to 997.31s of the 997.29s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.79s    = Training   runtime\n\n\n    0.45s    = Validation runtime\n\n\nFitting model: CatBoost ... Training model for up to 996.03s of the 996.0s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    19.55s   = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: ExtraTrees ... Training model for up to 976.43s of the 976.42s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.67s    = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: XGBoost ... Training model for up to 975.71s of the 975.7s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    3.42s    = Training   runtime\n\n\n    0.01s    = Validation runtime\n\n\nFitting model: LinearModel ... Training model for up to 972.26s of the 972.24s of remaining time.\n\n\nE:\\Anaconda\\Anaconda\\envs\\TransPro\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning:\n\nn_quantiles (1000) is greater than the total number of samples (716). n_quantiles is set to n_samples.\n\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    1.85s    = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 970.33s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.2s     = Training   runtime\n\n\n    0.0s     = Validation runtime\n\n\nAutoGluon training complete, total runtime = 29.9s ... Best model: \"WeightedEnsemble_L2\"\n\n\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240410_110009\\\")\n\n\nComputing feature importance via permutation shuffling for 1605 features using 385 rows with 5 shuffle sets...\n\n\n    392.16s = Expected runtime (78.43s per shuffle set)\n\n\n    41.22s  = Actual runtime (Completed 5 of 5 shuffle sets)\n\n\n                 model  score_test  score_val  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0             LightGBM    1.000000        1.0        0.010963       0.009966   0.734149                 0.010963                0.009966           0.734149            1       True          2\n1           ExtraTrees    1.000000        1.0        0.038870       0.026909   0.667217                 0.038870                0.026909           0.667217            1       True          5\n2          LinearModel    1.000000        1.0        0.039866       0.030895   1.848171                 0.039866                0.030895           1.848171            1       True          7\n3  WeightedEnsemble_L2    1.000000        1.0        0.039867       0.027906   0.864556                 0.000997                0.000997           0.197340            2       True          8\n4           KNeighbors    1.000000        1.0        0.044850       0.032889   0.129567                 0.044850                0.032889           0.129567            1       True          1\n5         RandomForest    1.000000        1.0        0.053820       0.448648   0.792030                 0.053820                0.448648           0.792030            1       True          3\n6             CatBoost    0.997403        1.0        0.037874       0.015947  19.546661                 0.037874                0.015947          19.546661            1       True          4\n7              XGBoost    0.994805        1.0        0.023920       0.014951   3.416737                 0.023920                0.014951           3.416737            1       True          6\n\n\n\n\n\n7.5.4 Return\n\n# Filtering the DataFrame to get features where importance &gt; 0 and p_value &lt; 0.05\nfiltered_importance = importance[(importance['importance'] &gt; 0) & (importance['p_value'] &lt; 0.05)]\n# Counting the number of such features\nnumber_of_features = filtered_importance.shape[0]\n# Printing the result\nprint(f\"Number of features with importance &gt; 0 and p_value &lt; 0.05: {number_of_features}\")\n\nNumber of features with importance &gt; 0 and p_value &lt; 0.05: 0\n\n\n\n\n# Display the top 20 rows of the Importance DataFrame\nprint(\"Top 20 rows of Importance DataFrame:\")\nprint(importance.head(20))\n\nTop 20 rows of Importance DataFrame:\n          importance  stddev  p_value  n  p99_high  p99_low\nA2M              0.0     0.0      0.5  5       0.0      0.0\nPROM2            0.0     0.0      0.5  5       0.0      0.0\nPSORS1C2         0.0     0.0      0.5  5       0.0      0.0\nPSORS1C1         0.0     0.0      0.5  5       0.0      0.0\nPSMC1P1          0.0     0.0      0.5  5       0.0      0.0\nPRTG             0.0     0.0      0.5  5       0.0      0.0\nPRSS8            0.0     0.0      0.5  5       0.0      0.0\nPRSS53           0.0     0.0      0.5  5       0.0      0.0\nPRSS3            0.0     0.0      0.5  5       0.0      0.0\nPRSS22           0.0     0.0      0.5  5       0.0      0.0\nPRR19            0.0     0.0      0.5  5       0.0      0.0\nPRR15L           0.0     0.0      0.5  5       0.0      0.0\nPRODH            0.0     0.0      0.5  5       0.0      0.0\nPI16             0.0     0.0      0.5  5       0.0      0.0\nPRKCQ            0.0     0.0      0.5  5       0.0      0.0\nPRF1             0.0     0.0      0.5  5       0.0      0.0\nPRAME            0.0     0.0      0.5  5       0.0      0.0\nPPP2R2C          0.0     0.0      0.5  5       0.0      0.0\nPPP1R1B          0.0     0.0      0.5  5       0.0      0.0\nPPP1R14C         0.0     0.0      0.5  5       0.0      0.0\n\n\n\n\n# Display the top 20 rows of the Leaderboard DataFrame\nprint(\"\\nTop 20 rows of Leaderboard DataFrame:\")\nprint(leaderboard.head(20))\n\n\nTop 20 rows of Leaderboard DataFrame:\n                 model  score_test  score_val  pred_time_test  pred_time_val  \\\n0             LightGBM    1.000000        1.0        0.010963       0.009966   \n1           ExtraTrees    1.000000        1.0        0.038870       0.026909   \n2          LinearModel    1.000000        1.0        0.039866       0.030895   \n3  WeightedEnsemble_L2    1.000000        1.0        0.039867       0.027906   \n4           KNeighbors    1.000000        1.0        0.044850       0.032889   \n5         RandomForest    1.000000        1.0        0.053820       0.448648   \n6             CatBoost    0.997403        1.0        0.037874       0.015947   \n7              XGBoost    0.994805        1.0        0.023920       0.014951   \n\n    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0   0.734149                 0.010963                0.009966   \n1   0.667217                 0.038870                0.026909   \n2   1.848171                 0.039866                0.030895   \n3   0.864556                 0.000997                0.000997   \n4   0.129567                 0.044850                0.032889   \n5   0.792030                 0.053820                0.448648   \n6  19.546661                 0.037874                0.015947   \n7   3.416737                 0.023920                0.014951   \n\n   fit_time_marginal  stack_level  can_infer  fit_order  \n0           0.734149            1       True          2  \n1           0.667217            1       True          5  \n2           1.848171            1       True          7  \n3           0.197340            2       True          8  \n4           0.129567            1       True          1  \n5           0.792030            1       True          3  \n6          19.546661            1       True          4  \n7           3.416737            1       True          6  \n\n\n\n\n\n7.5.5 Save Data\n\n# Save the Importance DataFrame to a CSV file\nimportance.to_csv('../test_TransProPy/data/significant_correlation_Autogluon_SelectML_importance.csv', index=False)\n\n# Save the Leaderboard DataFrame to a CSV file\nleaderboard.to_csv('../test_TransProPy/data/significant_correlation_Autogluon_SelectML_leaderboard.csv', index=False)"
  },
  {
    "objectID": "AutoGluonSelectML.html#references-d",
    "href": "AutoGluonSelectML.html#references-d",
    "title": "7  AutoGluonSelectML.py",
    "section": "7.6 References",
    "text": "7.6 References\n\n7.6.1 Scientific Publications\n\n\nErickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., & Smola, A. (2020). AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data. arXiv preprint arXiv:2003.06505.\nFakoor, R., Mueller, J., Erickson, N., Chaudhari, P., & Smola, A. J. (2020). Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation. arXiv preprint arXiv:2006.14284. \nShi, X., Mueller, J., Erickson, N., Li, M., & Smola, A. (2021). Multimodal AutoML on Structured Tables with Text Fields. In AutoML@ICML 2021. \n\n\n\n\n7.6.2 Articles\n\n\nPrasanna, S. (2020, March 31). Machine learning with AutoGluon, an open source AutoML library. AWS Open Source Blog. \nSun, Y., Wu, C., Zhang, Z., He, T., Mueller, J., & Zhang, H. (n.d.). (2020). Image classification on Kaggle using AutoGluon. Medium.\nErickson, N., Mueller, J., Zhang, H., & Kamakoti, B. (2019). AutoGluon: Deep Learning AutoML. Medium.\n\n\n\n\n7.6.3 Documentation\n\n\nAutoGluon Predictors –AutoGluon Documentation 0.1.0 documentation"
  },
  {
    "objectID": "AutoFeatureSelection.html#parameters-f",
    "href": "AutoFeatureSelection.html#parameters-f",
    "title": "8  AutoFeatureSelection.py",
    "section": "8.1 Parameters",
    "text": "8.1 Parameters\n\n\ndata_file: str:\n\nPath to the feature data file.\n\nlabel_file: str:\n\nPath to the label data file.\n\nlabel_col: str:\n\nName of the label column.\n\nthreshold: float:\n\nThreshold for data preprocessing.\n\nshow_plot: bool:\n\nWhether to display plot.\n\nshow_progress: bool:\n\nWhether to show a progress bar.\n\nn_iter: int:\n\nNumber of iterations for RandomizedSearchCV.\n\nn_cv: int:\n\nNumber of folds for cross-validation.\n\nn_jobs: int:\n\nNumber of parallel jobs for RandomizedSearchCV.\n\nsave_path: str:\n\nPath to save results.\n\nsleep_interval: int:\n\nInterval time in seconds for progress bar update.\n\nuse_tkagg: bool:\n\nWhether to use ‘TkAgg’ backend for matplotlib. Generally, choose True when using in PyCharm IDE, and choose False when rendering file.qmd to an HTML file."
  },
  {
    "objectID": "AutoFeatureSelection.html#please-note",
    "href": "AutoFeatureSelection.html#please-note",
    "title": "8  AutoFeatureSelection.py",
    "section": "8.2 Please note",
    "text": "8.2 Please note\n\n\n\n\n\n\nNote\n\n\n\n\nIf n_jobs &gt; 1 leads to the use of multiprocessing in the function, communication and state sharing between processes become complex. In a multiprocessing environment, each process has its own memory space, and updating the status of the progress bar needs to be done across processes, which can lead to inconsistent or delayed updates. Therefore, if n_jobs &gt; 1 or = -1, please set show_progress to False. If you have sufficient computing resources, such as on a cloud server, pay attention to whether the number of computing nodes, cores per node, and the number of CPUs allocated per task match the value of n_jobs."
  },
  {
    "objectID": "AutoFeatureSelection.html#description",
    "href": "AutoFeatureSelection.html#description",
    "title": "8  AutoFeatureSelection.py",
    "section": "8.3 Description",
    "text": "8.3 Description\n\nThe auto_feature_selection function automates the entire process of feature selection, model training, and result extraction. It includes:\n\nLoading and preprocessing data using load_and_preprocess_data.\nSetting up feature selection with setup_feature_selection.\nDefining and training a model with train_model using RandomizedSearchCV.\nOptionally displaying a progress bar using tqdm and threading for real-time progress updates.\nExtracting and saving results with extract_and_save_results.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nSingle-threaded computation: show_progress=True, n_jobs=1 Multithreaded computation: show_progress=False, n_jobs=n"
  },
  {
    "objectID": "AutoFeatureSelection.html#usage-multithreaded-computation-showprogress-false-njobs-1",
    "href": "AutoFeatureSelection.html#usage-multithreaded-computation-showprogress-false-njobs-1",
    "title": "8  AutoFeatureSelection.py",
    "section": "8.4 Usage (Multithreaded computation: show_progress=False, n_jobs=-1)",
    "text": "8.4 Usage (Multithreaded computation: show_progress=False, n_jobs=-1)\n\n8.4.1 four_methods_degs_intersection\nfrom TransProPy.AutoFeatureSelection import auto_feature_selection\nauto_feature_selection(\n    data_file='../test_TransProPy/data/four_methods_degs_intersection.csv',\n    label_file='../test_TransProPy/data/class.csv',\n    label_col='class',\n    threshold=0.95,\n    show_plot=True,\n    show_progress=False,\n    n_iter=8,\n    n_cv=3,\n    n_jobs=-1,\n    save_path='../test_TransProPy/result/four_methods_degs_intersection_',\n    sleep_interval=100,\n    use_tkagg=False\n)\n\n\nFig1\n\n\n\nFig2\n\n#=============================#\n# Features selected by RFECV: #\n#=============================#\nIndex(['A2M', 'A2ML1', 'ABCA17P', 'ABCA9', 'ABCB4', 'ABCD1', 'AC016757.3',\n       'AC138035.2', 'AC159540.1', 'ACAD11',\n       ...\n       'ZBTB16', 'ZBTB32', 'ZBTB9', 'ZNF114', 'ZNF280B', 'ZNF337', 'ZNF410',\n       'ZNF564', 'ZNF697', 'ZNRF3'],\n      dtype='object', length=561)\n\n#===================================#\n# Features selected by SelectKBest: #\n#===================================#\nIndex(['ABCD1', 'AC125232.1', 'AC138035.2', 'AC159540.1', 'ACAD11', 'ACOT7',\n       'AGAP5', 'AKAP2', 'ALX1', 'ANKHD1',\n       ...\n       'TRIB2', 'TRIM34', 'TTC4', 'U2AF1', 'UBBP4', 'UBE2V1',\n       'XXbac-B461K10.4', 'ZBTB9', 'ZNF410', 'ZNRF3'],\n      dtype='object', length=184)\n\n#========================================#\n# Total number of selected features: 581 #\n#========================================#\n\n#==========================================#\n# Feature Importances from EnsembleForRFE: #\n#==========================================#\n                    Feature  Importance\n0             SLX1A-SULT1A3    0.455213\n1                    EIF3CL    0.095509\n2                     EEF1G    0.061155\n3             RP11-231C14.4    0.060150\n4                 RPL13AP25    0.001863\n5                     MEX3A    0.001836\n6                     SRXN1    0.001824\n7                  EEF1A1P9    0.001793\n8                PPP1R14BP3    0.001741\n9             RP11-513I15.6    0.001741\n10                HSD17B1P1    0.001626\n11                 HNRNPKP1    0.001606\n12                    TUBA8    0.001606\n13                RPL13AP20    0.001594\n14                 HNRNPCP2    0.001592\n15                    PYURF    0.001572\n16             RP11-676M6.1    0.001569\n17                EEF1A1P12    0.001568\n18           C15orf38-AP3S2    0.001516\n19                     DLX1    0.001484\n20            RP11-543P15.1    0.001480\n21             RP3-475N16.1    0.001441\n22                    WDR88    0.001439\n23                    TIMP3    0.001431\n24                   BOLA2B    0.001430\n25             RP4-756H11.3    0.001423\n26            RP11-977G19.5    0.001423\n27             CTC-451P13.1    0.001399\n28                    AKAP2    0.001398\n29            CTD-2303H24.2    0.001397\n..                      ...         ...\n531                 IL12RB1    0.000578\n532                 RASGRF1    0.000577\n533           RP5-1039K5.19    0.000577\n534  DTX2P1-UPK3BP1-PMS2P11    0.000574\n535                   FCRLA    0.000569\n536                    C1QC    0.000569\n537                  ARL2BP    0.000565\n538            RP11-434H6.6    0.000565\n539                   KRT14    0.000564\n540                   MCHR1    0.000564\n541                  DBNDD1    0.000563\n542                  ADAM23    0.000555\n543                     ADM    0.000554\n544                    CD74    0.000553\n545                    LAG3    0.000553\n546                 SLC28A3    0.000552\n547                     KMO    0.000551\n548                   KLK10    0.000551\n549                   ITGAL    0.000550\n550                   NEAT1    0.000549\n551                   RSAD2    0.000537\n552                  PKD1P5    0.000537\n553                   SEPT3    0.000527\n554                     LYZ    0.000525\n555            RP11-20D14.6    0.000523\n556                   GSDMC    0.000518\n557                  FAM46C    0.000511\n558                 SLCO5A1    0.000504\n559                   GSDMB    0.000435\n560                     FLG    0.000432\n\n[561 rows x 2 columns]\n\n#==========================#\n# Scores from SelectKBest: #\n#==========================#\n            Feature     Score\n0             EEF1G  0.657676\n1     SLX1A-SULT1A3  0.657676\n2            EIF3CL  0.656309\n3     RP11-231C14.4  0.655948\n4             U2AF1  0.654053\n5              TEN1  0.653639\n6           FAM156A  0.653524\n7     CTD-2231E14.8  0.652795\n8            NPIPB5  0.652436\n9           GTF2IP1  0.651938\n10   TMEM256-PLSCR3  0.651155\n11           EIF4A1  0.650506\n12          ARL6IP4  0.650506\n13            ATRIP  0.649792\n14            SPSB3  0.648970\n15       PPP1R14BP3  0.648784\n16   C15orf38-AP3S2  0.648361\n17            ZBTB9  0.647219\n18            AKAP2  0.646561\n19           NPIPA1  0.646222\n20            SNX15  0.646055\n21        RPL13AP25  0.645636\n22            AGAP5  0.645010\n23            CBWD3  0.644778\n24            EIF3C  0.644602\n25           PIK3R2  0.644362\n26           NPIPB3  0.643772\n27            MATR3  0.643762\n28           ZNF410  0.643468\n29            TOP3B  0.643425\n..              ...       ...\n154           ABCD1  0.581264\n155          DBNDD1  0.579869\n156      AC138035.2  0.579813\n157       HSD17B1P1  0.579566\n158           CHST6  0.579342\n159         RABGEF1  0.579211\n160            PLP1  0.578937\n161          TRIM34  0.578699\n162    RP11-42I10.1  0.578196\n163            ALX1  0.577680\n164          PIK3CD  0.577326\n165    RP4-756H11.3  0.576957\n166           PLOD3  0.576935\n167          FAM78A  0.576135\n168            CHKB  0.575878\n169             SDS  0.575652\n170            CCL5  0.575628\n171         CYP27A1  0.574835\n172          RNF157  0.574751\n173    RP11-419C5.2  0.574712\n174        C19orf38  0.573959\n175           NHEJ1  0.573535\n176     RP3-337H4.8  0.573386\n177          IGSF11  0.573195\n178       RPL13AP20  0.573170\n179   RP11-549B18.1  0.571859\n180           HLA-G  0.571613\n181     CH507-9B2.3  0.571400\n182           ACOT7  0.570546\n183           S100B  0.570479\n\n[184 rows x 2 columns]\n\n\n8.4.2 four_methods_degs_union\nfrom TransProPy.AutoFeatureSelection import auto_feature_selection\nauto_feature_selection(\n    data_file='../test_TransProPy/data/four_methods_degs_union.csv',\n    label_file='../test_TransProPy/data/class.csv',\n    label_col='class',\n    threshold=0.95,\n    show_plot=True,\n    show_progress=False,\n    n_iter=8,\n    n_cv=3,\n    n_jobs=-1,\n    save_path='../test_TransProPy/result/four_methods_degs_union_',\n    sleep_interval=100,\n    use_tkagg=False\n)\n\n\nFig3\n\n\n\nFig4\n\n#=============================#\n# Features selected by RFECV: #\n#=============================#\nIndex(['A1BG', 'A2M', 'A2ML1', 'AB019441.29', 'ABCA17P', 'ABCC11', 'ABCC3',\n       'ABCD1', 'AC006486.10', 'AC006538.1',\n       ...\n       'ZNF23', 'ZNF280B', 'ZNF337', 'ZNF410', 'ZNF564', 'ZNF697', 'ZNRF3',\n       'ZRANB2-AS2', 'ZSCAN12P1', 'ZSCAN32'],\n      dtype='object', length=1007)\n\n#===================================#\n# Features selected by SelectKBest: #\n#===================================#\nIndex(['ABCD1', 'AC125232.1', 'AC138035.2', 'AC159540.1', 'ACAD11', 'AGAP5',\n       'AKAP2', 'ALX1', 'ANKHD1', 'AP1S2',\n       ...\n       'TRIB2', 'TRIM34', 'TTC4', 'U2AF1', 'UBBP4', 'UBE2V1',\n       'XXbac-B461K10.4', 'ZBTB9', 'ZNF410', 'ZNRF3'],\n      dtype='object', length=180)\n\n#=========================================#\n# Total number of selected features: 1020 #\n#=========================================#\n\n#==========================================#\n# Feature Importances from EnsembleForRFE: #\n#==========================================#\n             Feature  Importance\n0             EIF3CL    0.404330\n1      RP11-231C14.4    0.147479\n2              EEF1G    0.062324\n3      SLX1A-SULT1A3    0.055901\n4              SRXN1    0.001105\n5             BOLA2B    0.001065\n6            CROCCP2    0.001064\n7              PYURF    0.001043\n8      RP11-513I15.6    0.001018\n9             GCSHP5    0.000976\n10      CTC-325H20.4    0.000974\n11    C15orf38-AP3S2    0.000962\n12             NOMO2    0.000959\n13         RPL13AP25    0.000947\n14      RP5-827C21.1    0.000941\n15          EEF1A1P9    0.000930\n16    RP11-386G11.10    0.000930\n17    MSANTD3-TMEFF1    0.000929\n18             TIMP3    0.000919\n19          C22orf23    0.000916\n20            NDUFV2    0.000913\n21             RAPH1    0.000908\n22             EIF3C    0.000905\n23           GTF2IP1    0.000889\n24             RTEL1    0.000884\n25           FABP5P7    0.000883\n26          RPS18P12    0.000874\n27            RNASEK    0.000873\n28            MRPL53    0.000870\n29     RP11-1148L6.5    0.000867\n...              ...         ...\n977             LAG3    0.000326\n978          TMPRSS2    0.000326\n979          FTH1P20    0.000326\n980           ACSBG1    0.000326\n981    RP11-144G6.12    0.000326\n982           ATRNL1    0.000324\n983            CERS1    0.000320\n984             LMO3    0.000319\n985           FBXO16    0.000318\n986          RPLP0P2    0.000317\n987            NUAK2    0.000317\n988            GSDMC    0.000316\n989           ADAM23    0.000316\n990    RP11-206L10.2    0.000315\n991             JPH2    0.000315\n992           SLC7A5    0.000312\n993       AC079922.3    0.000312\n994            CLIC3    0.000309\n995              HPN    0.000307\n996            TDRD9    0.000304\n997           ALOXE3    0.000304\n998          TMEM45A    0.000303\n999            DGAT2    0.000302\n1000           CPT1B    0.000302\n1001            SIK1    0.000298\n1002            GJA3    0.000295\n1003           DUSP1    0.000294\n1004         SLCO5A1    0.000290\n1005        ADAMTSL1    0.000288\n1006           HMGA2    0.000278\n\n[1007 rows x 2 columns]\n\n#==========================#\n# Scores from SelectKBest: #\n#==========================#\n            Feature     Score\n0             EEF1G  0.657676\n1     SLX1A-SULT1A3  0.657676\n2            EIF3CL  0.656309\n3     RP11-231C14.4  0.655948\n4             U2AF1  0.654053\n5           FAM156A  0.653784\n6              TEN1  0.653639\n7     CTD-2231E14.8  0.652795\n8            NPIPB5  0.652436\n9           GTF2IP1  0.651938\n10           EIF4A1  0.650506\n11          ARL6IP4  0.650506\n12   TMEM256-PLSCR3  0.649853\n13            ATRIP  0.649792\n14            SPSB3  0.648970\n15   C15orf38-AP3S2  0.648361\n16       PPP1R14BP3  0.648003\n17            AKAP2  0.646821\n18            SNX15  0.646576\n19            ZBTB9  0.646439\n20           NPIPA1  0.646222\n21        RPL13AP25  0.645636\n22            CBWD3  0.645298\n23            EIF3C  0.644602\n24            TOP3B  0.644247\n25            AGAP5  0.643969\n26           NPIPB3  0.643772\n27            MATR3  0.643762\n28           ARL17A  0.643547\n29          GUSBP11  0.643351\n..              ...       ...\n150         SLC31A2  0.583532\n151          LRRC8E  0.583163\n152            SNCA  0.582802\n153      AC138035.2  0.582591\n154            APC2  0.582428\n155        HIST1H4I  0.582267\n156           PSEN2  0.581922\n157       HSD17B1P1  0.581778\n158           PTPRJ  0.581601\n159           ABCD1  0.581524\n160          DBNDD1  0.581320\n161          SH2D2A  0.581028\n162    RP11-42I10.1  0.580332\n163            ALX1  0.579402\n164          STXBP1  0.579196\n165           CHST6  0.578714\n166         RABGEF1  0.578688\n167            PLP1  0.578346\n168    RP11-419C5.2  0.578111\n169             SDS  0.578105\n170          TRIM34  0.577805\n171          PIK3CD  0.577049\n172        C19orf38  0.577025\n173           PLOD3  0.576935\n174    RP4-756H11.3  0.576699\n175            CHKB  0.575946\n176          RNF157  0.575775\n177           NHEJ1  0.575764\n178            CCL5  0.575437\n179        C22orf23  0.575293\n\n[180 rows x 2 columns]\n\n\n\n\n\n\nNote\n\n\n\nPlease specify both the save path and the prefix for the generated file name when entering the save_path parameter.\n\n\n\n\n\n\n\n\nTip\n\n\n\nGiven that users have datasets of varying sizes, please configure appropriate computational resources before use. It is recommended for ordinary laptops or desktop computers not to exceed 10,000 feature genes to maintain computational efficiency. For datasets with tens of thousands of feature genes, utilizing a server is advised to reduce computational time and costs."
  },
  {
    "objectID": "Contact.html",
    "href": "Contact.html",
    "title": "9  Contact",
    "section": "",
    "text": "Any problems encountered or inspiration gained from using this Python package can be addressed by contacting me in the following ways. I am very willing to help you wholeheartedly.\n\nemailgithub\n\n\nyudongyue@mail.nankai.edu.cn\n\n\nhttps://github.com/SSSYDYSSS/TransProPy"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "10  References",
    "section": "",
    "text": "Su,Y., Du,K., Wang,J., Wei,J. and Liu,J. (2022) Multi-variable AUC for sifting complementary features and its biomedical application. Briefings in Bioinformatics, 23, bbac029.\nErickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., & Smola, A. (2020). AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data. arXiv preprint arXiv:2003.06505.\nFakoor, R., Mueller, J., Erickson, N., Chaudhari, P., & Smola, A. J. (2020). Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation. arXiv preprint arXiv:2006.14284. \nShi, X., Mueller, J., Erickson, N., Li, M., & Smola, A. (2021). Multimodal AutoML on Structured Tables with Text Fields. In AutoML@ICML 2021. \nPrasanna, S. (2020, March 31). Machine learning with AutoGluon, an open source AutoML library. AWS Open Source Blog. \nSun, Y., Wu, C., Zhang, Z., He, T., Mueller, J., & Zhang, H. (n.d.). (2020). Image classification on Kaggle using AutoGluon. Medium.\nErickson, N., Mueller, J., Zhang, H., & Kamakoti, B. (2019). AutoGluon: Deep Learning AutoML. Medium.\nAutoGluon Predictors –AutoGluon Documentation 0.1.0 documentation\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis manual is meticulously crafted to elaborate on the principles of the referenced functions, with an emphasis on the methods and objectives users should be cognizant of when utilizing these functions. It is designed to foster a thorough understanding and facilitate the adaptation of this Python package. References to the algorithms that are employed or adapted are presented as mentioned above.\nShould there be any omissions in the cited references that are significantly related to the content discussed herein, we would be immensely grateful to be informed by our esteemed users. Additionally, we sincerely apologize for any imperfections that may exist within this manual."
  }
]