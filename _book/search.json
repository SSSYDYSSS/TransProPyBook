[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TransProPyBook",
    "section": "",
    "text": "Preface\nTo learn more about TransProPy visit https://github.com/SSSYDYSSS/TransProPy."
  },
  {
    "objectID": "index.html#utilsfunction",
    "href": "index.html#utilsfunction",
    "title": "TransProPyBook",
    "section": "UtilsFunction*",
    "text": "UtilsFunction*\n\nThese functions usually contain various utilities and helper functions, which can sometimes be considered as low-level functionalities.\n\n\nUtilsFunction1\nUtilsFunction2"
  },
  {
    "objectID": "index.html#main-function",
    "href": "index.html#main-function",
    "title": "TransProPyBook",
    "section": "*main Function",
    "text": "*main Function\n\nThe main function constructed with the help of auxiliary functions.\n\n\nMACFCmain\nNewMACFCmain\nAutogluonTimeLimit\nAutoGluonSelectML"
  },
  {
    "objectID": "UtilsFunction1.html#auc.py",
    "href": "UtilsFunction1.html#auc.py",
    "title": "1  UtilsFunction1",
    "section": "1.1 Auc.py",
    "text": "1.1 Auc.py\nAssists the MACFCmain function in calculating AUC, obtaining Feature Frequency, and performing sorting.\n\n1.1.1 Introduction\n\n\nIn this function, features that appear with high frequency indicate their presence in multiple optimal feature sets.\nEach optimal feature set is determined by calculating its Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), which is a common measure for evaluating classifier performance.\nDuring each iteration of the loop, an optimal feature set with the highest average AUC value is selected.\nFeatures from this set are then added to a rank list, known as ‘ranklist,’ and when necessary, also to a set named ‘rankset’.\n\n\n\n\n1.1.2 Usage\nauc(tlofe, ne, n0, n1)"
  },
  {
    "objectID": "UtilsFunction1.html#autonorm.py",
    "href": "UtilsFunction1.html#autonorm.py",
    "title": "1  UtilsFunction1",
    "section": "1.2 AutoNorm.py",
    "text": "1.2 AutoNorm.py\nNormalization Function The auto_norm function is designed to normalize a two-dimensional array (matrix). The purpose of normalization is generally to bring all features into the same numerical range, facilitating subsequent analysis or model training.\n\n1.2.1 Parameters\n\n\ndata: ndarray\n\nOrder Requirements for Input Data：\n1.This function does indeed have specific requirements for the row and column order of the input matrix data. Rows should represent individual samples, and columns should represent different features. In other words, each row vector represents a sample containing multiple features.\n2.Each column of the matrix will be independently normalized, so different features should be placed in separate columns.\n\n\n\n\n\n1.2.2 Returns\n\n\nnorm_data: ndarray\n\nIt is the normalized data.\n\n\n\n\n\n1.2.3 Usage\nauto_norm(data)"
  },
  {
    "objectID": "UtilsFunction1.html#featureranking.py",
    "href": "UtilsFunction1.html#featureranking.py",
    "title": "1  UtilsFunction1",
    "section": "1.3 FeatureRanking.py",
    "text": "1.3 FeatureRanking.py\n\n1.3.1 Introduction\n\n\nHigh-Frequency Features and Performance: Because features in each set are chosen based on their contribution to classifier performance, high-frequency features are likely to perform well. In other words, if a feature appears in multiple optimal feature sets, it may have a significant impact on the performance of the classifier.\nNote on Low-Frequency Features: However, it’s important to note that a low frequency of a feature does not necessarily mean it is unimportant. The importance of a feature may depend on how it combines with other features. Additionally, the outcome of feature selection may be influenced by the characteristics of the dataset and random factors. Therefore, the frequency provided by this function should only be used as a reference and is not an absolute indicator of feature performance.\n\n\n\n\n1.3.2 Usage\nfeature_ranking(f, c, max_rank, pos, neg, n0, n1)"
  },
  {
    "objectID": "UtilsFunction1.html#loaddata.py",
    "href": "UtilsFunction1.html#loaddata.py",
    "title": "1  UtilsFunction1",
    "section": "1.4 LoadData.py",
    "text": "1.4 LoadData.py\nData Reading and Transformation.\n\n1.4.1 Introduction\n\n\nData normalization for constant value.\nExtract matrix data and categorical data.\n\n\n\n\n1.4.2 Parameters\n\n\nlable_name: string\n\nFor example: gender, age, altitude, temperature, quality, and other categorical variable names.\n\ndata_path: string\n\nFor example: ‘../data/gene_tpm.csv’\nPlease note: Preprocess the input data in advance to remove samples that contain too many missing values or zeros.\nThe input data matrix should have genes as rows and samples as columns.\n\nlabel_path: string\n\nFor example: ‘../data/tumor_class.csv’\nPlease note: The input CSV data should have rows representing sample names and columns representing class names.\nThe input sample categories must be in a numerical binary format, such as: 1,2,1,1,2,2,1.\nIn this case, the numerical values represent the following classifications: 1: male; 2: female.\n\nthreshold: float\n\nFor example: 0.9\nThe set threshold indicates the proportion of non-zero value samples to all samples in each feature.\n\n\n\n\n\n1.4.3 Returns\n\n\ntranspose(f): ndarray\n\nA transposed feature-sample matrix.\n\nc: ndarray\n\nA NumPy array containing classification labels.\n\n\n\n\n\n1.4.4 Usage\nload_data(\n    lable_name, \n    threshold, \n    data_path='../data/gene_tpm.csv', \n    label_path='../data/tumor_class.csv'\n    )"
  },
  {
    "objectID": "UtilsFunction1.html#printresults.py",
    "href": "UtilsFunction1.html#printresults.py",
    "title": "1  UtilsFunction1",
    "section": "1.5 PrintResults.py",
    "text": "1.5 PrintResults.py\n\n1.5.1 Returns\n\n\nfr: list of strings\n\nRepresenting ranked features.\n\nfre1: dictionary\n\nFeature names as keys and their frequencies as values.\n\nfrequency: list of tuples\n\nFeature names and their frequencies.\n\nlen(FName): integer\n\nCount of AUC values greater than 0.5.\n\nFName: array of strings\n\nFeature names after ranking with AUC &gt; 0.5.\n\nFauc: array of floats\n\nAUC values corresponding to the ranked feature names.\n\n\n\n\n\n1.5.2 Usage\n print_results(fr, fre1, frequency, len_FName, FName, Fauc)"
  },
  {
    "objectID": "UtilsFunction1.html#filtersamples.py",
    "href": "UtilsFunction1.html#filtersamples.py",
    "title": "1  UtilsFunction1",
    "section": "1.6 FilterSamples.py",
    "text": "1.6 FilterSamples.py\nRemove samples with high zero expression.\n\n1.6.1 Parameters\n\n\ndata_path: string\n\nFor example: ‘../data/gene_tpm.csv’\nPlease note: The input data matrix should have genes as rows and samples as columns.\n\nthreshold: float\n\nFor example: 0.9\nThe set threshold indicates the proportion of non-zero value samples to all samples in each feature.\n\n\n\n\n\n1.6.2 Return\n\n\nX: pandas.core.frame.DataFrame\n\n\n\n\n1.6.3 Usage\nfilter_samples(threshold, data_path='../data/gene_tpm.csv')"
  },
  {
    "objectID": "UtilsFunction1.html#genenames.py",
    "href": "UtilsFunction1.html#genenames.py",
    "title": "1  UtilsFunction1",
    "section": "1.7 GeneNames.py",
    "text": "1.7 GeneNames.py\nExtract gene_names data.\n\n1.7.1 Parameters\n\n\ndata_path: string\n\nFor example: ‘../data/gene_tpm.csv’\nPlease note: Preprocess the input data in advance to remove samples that contain too many missing values or zeros.\nThe input data matrix should have genes as rows and samples as columns.\n\n\n\n\n\n1.7.2 Return\n\n\ngene_names: list\n\n\n\n\n1.7.3 Usage\ngene_name(data_path='../data/gene_tpm.csv')"
  },
  {
    "objectID": "UtilsFunction1.html#genetofeaturemapping.py",
    "href": "UtilsFunction1.html#genetofeaturemapping.py",
    "title": "1  UtilsFunction1",
    "section": "1.8 GeneToFeatureMapping.py",
    "text": "1.8 GeneToFeatureMapping.py\ngene map feature.\n\n1.8.1 Parameters\n\n\ngene_names: list\n\nFor example: [‘GeneA’, ‘GeneB’, ‘GeneC’, ‘GeneD’, ‘GeneE’]\ncontaining strings\n\nranked_features: list\n\nFor example: [2, 0, 1]\ncontaining integers\n\n\n\n\n\n1.8.2 Return\n\n\ngene_to_feature_mapping: dictionary\n\ngene_to_feature_mapping is a Python dictionary type. It is used to map gene names to their corresponding feature (or ranked feature) names.\n\n\n\n\n\n1.8.3 Usage\ngene_map_feature(gene_names, ranked_features)"
  },
  {
    "objectID": "UtilsFunction1.html#references",
    "href": "UtilsFunction1.html#references",
    "title": "1  UtilsFunction1",
    "section": "1.9 References",
    "text": "1.9 References\n\n\nSu,Y., Du,K., Wang,J., Wei,J. and Liu,J. (2022) Multi-variable AUC for sifting complementary features and its biomedical application. Briefings in Bioinformatics, 23, bbac029."
  },
  {
    "objectID": "UtilsFunction2.html#splitdata.py",
    "href": "UtilsFunction2.html#splitdata.py",
    "title": "2  UtilsFunction2",
    "section": "2.1 splitdata.py",
    "text": "2.1 splitdata.py\nReads the gene expression and class data, processes it, and splits it into training and testing sets.\n\n2.1.1 Parameters\n\n\ngene_data_path (str):\n\nPath to the CSV file containing the gene expression data.\nFor example: ‘../data/gene_tpm.csv’\n\nclass_data_path (str):\n\nPath to the CSV file containing the class data.\nFor example: ‘../data/tumor_class.csv’\n\nclass_name (str):\n\nThe name of the class column in the class data.\n\ntest_size (float, optional):\n\nThe proportion of the data to be used as the testing set.\nDefault is 0.2.\n\nrandom_state (int, optional):\n\nThe seed used by the random number generator.\nDefault is 42.\n\nthreshold (float, optional):\n\nThe threshold used to filter out rows based on the proportion of non-zero values.\nDefault is 0.9.\n\nrandom_feature (int, optional):\n\nThe number of random feature to select. If None, no random feature selection is performed.\nDefault is None.\n\n\n\n\n\n2.1.2 Returns\n\n\ntrain_data (pd.DataFrame):\n\nThe training data.\n\ntest_data (pd.DataFrame):\n\nThe testing data.\n\n\n\n\n\n2.1.3 Usage\nsplit_data(\n    gene_data_path='../data/gene_tpm.csv', \n    class_data_path='../data/tumor_class.csv', \n    class_name, \n    test_size=0.2, \n    random_state=42, \n    threshold=0.9, \n    random_feature=None\n    )"
  },
  {
    "objectID": "UtilsFunction2.html#logtransform.py",
    "href": "UtilsFunction2.html#logtransform.py",
    "title": "2  UtilsFunction2",
    "section": "2.2 LogTransform.py",
    "text": "2.2 LogTransform.py\nEvaluate and potentially apply log2 transformation to data. - This function checks data against a set of criteria to determine if a log2 transformation is needed, applying the transformation if necessary.\n\n2.2.1 Parameters\n\n\ndata (np.ndarray):\n\nA numerical numpy array.\n\n\n\n\n\n2.2.2 Returns\n\n\nresult np.ndarray\n\nThe original data or the data transformed with log2.\n\n\n\n\n\n2.2.3 Usage\nlog_transform(\n    data\n    )"
  },
  {
    "objectID": "MACFCmain.html#parameters",
    "href": "MACFCmain.html#parameters",
    "title": "3  MACFCmain.py",
    "section": "3.1 Parameters",
    "text": "3.1 Parameters\n\n\nmax_rank: int\n\nThe total number of gene combinations you want to obtain.\n\nlable_name: string\n\nFor example: gender, age, altitude, temperature, quality, and other categorical variable names.\n\ndata_path: string\n\nFor example: ‘../data/gene_tpm.csv’\nPlease note: Preprocess the input data in advance to remove samples that contain too many missing values or zeros.\nThe input data matrix should have genes as rows and samples as columns.\n\nlabel_path: string\n\nFor example: ‘../data/tumor_class.csv’\nPlease note: The input sample categories must be in a numerical binary format, such as: 1,2,1,1,2,2,1.\nIn this case, the numerical values represent the following classifications: 1: male; 2: female.\n\nthreshold: float\n\nFor example: 0.9\nThe set threshold indicates the proportion of non-zero value samples to all samples in each feature."
  },
  {
    "objectID": "MACFCmain.html#returns",
    "href": "MACFCmain.html#returns",
    "title": "3  MACFCmain.py",
    "section": "3.2 Returns",
    "text": "3.2 Returns\n\n\nfr: list of strings\n\nRepresenting ranked features.\n\nfre1: dictionary\n\nFeature names as keys and their frequencies as values.\n\nfrequency: list of tuples\n\nFeature names and their frequencies.\nThe frequency outputs a list sorted by occurrence frequency (in descending order). This list includes only those elements from the dictionary fre1 (which represents the counted frequencies of elements in the original data) that have an occurrence frequency greater than once, along with their frequencies.\n\nlen(FName): integer\n\nCount of AUC values greater than 0.5.\n\nFName: array of strings\n\nFeature names after ranking with AUC &gt; 0.5.\n\nFauc: array of floats\n\nAUC values corresponding to the ranked feature names."
  },
  {
    "objectID": "MACFCmain.html#function-principle-explanation",
    "href": "MACFCmain.html#function-principle-explanation",
    "title": "3  MACFCmain.py",
    "section": "3.3 Function Principle Explanation",
    "text": "3.3 Function Principle Explanation\n\nFeature Frequency and AUC: In this function, features that appear with high frequency indicate their presence in multiple optimal feature sets. Each optimal feature set is determined by calculating its Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), which is a common measure for evaluating classifier performance. During each iteration of the loop, an optimal feature set with the highest average AUC value is selected. Features from this set are then added to a rank list, known as ‘ranklist,’ and when necessary, also to a set named ‘rankset’.\nHigh-Frequency Features and Performance: Because features in each set are chosen based on their contribution to classifier performance, high-frequency features are likely to perform well. In other words, if a feature appears in multiple optimal feature sets, it may have a significant impact on the performance of the classifier.\nNote on Low-Frequency Features: However, it’s important to note that a low frequency of a feature does not necessarily mean it is unimportant. The importance of a feature may depend on how it combines with other features. Additionally, the outcome of feature selection may be influenced by the characteristics of the dataset and random factors. Therefore, the frequency provided by this function should only be used as a reference and is not an absolute indicator of feature performance.\nFurther Evaluation Methods: If you wish to explore feature performance more deeply, you may need to employ other methods for assessing feature importance. This could include model-based importance metrics or statistical tests to evaluate the relationship between features and the target variable."
  },
  {
    "objectID": "MACFCmain.html#usage-workflow",
    "href": "MACFCmain.html#usage-workflow",
    "title": "3  MACFCmain.py",
    "section": "3.4 Usage Workflow",
    "text": "3.4 Usage Workflow\n\n\nFName is a list of feature names sorted based on their AUC (Area Under the Curve) values. In this sorting method, the primary consideration is the AUC value, followed by the feature name. All features included in FName have an AUC value greater than 0.5.\nfr is the result of another sorting method. In this method, the primary consideration is the “combined” AUC of the features, followed by their individual AUC values. This means that some features, despite having lower individual AUC values, may produce a higher combined AUC when paired with other features. Therefore, their position in the fr list may be higher than in the FName list.\nThe code for fr employs a more complex logic to select and combine features to optimize their combined AUC values. In this process, features are not solely selected and sorted based on their individual AUC values; the effect of their combination with other features is also considered. Consequently, the sorting logic for fr (or rankset) differs from that of FName.\nPlease note: While the code takes into account both individual AUC values and combined AUC values, the sorting of the fr list (i.e., rankset) initially starts based on individual AUC values. This is because at the beginning of each external loop iteration, the first element of fs is the next feature sorted by its individual AUC value. The list is then further optimized by evaluating the combination effects with other features."
  },
  {
    "objectID": "MACFCmain.html#usage-of-macfcmain-significant-correlation",
    "href": "MACFCmain.html#usage-of-macfcmain-significant-correlation",
    "title": "3  MACFCmain.py",
    "section": "3.5 Usage of MACFCmain (Significant correlation)",
    "text": "3.5 Usage of MACFCmain (Significant correlation)\nThis function uses the MACFC method to select feature genes relevant to classification and ranks them based on their corresponding weights.\n\n\nPlease note:Data characteristics: Features have strong correlation with the classification.\n\n\n\n3.5.1 Import the corresponding module\n\nimport TransProPy.MACFCmain as Tr\nimport TransProPy.UtilsFunction1.GeneNames as TUG\nimport TransProPy.UtilsFunction1.GeneToFeatureMapping as TUGM\n\n\n\n\n3.5.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/all_degs_count_exp.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n            Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0                 A1BG          6.754888          4.000000          5.044394   \n1                  A2M         16.808499         16.506184         17.143433   \n2                A2ML1          1.584963          9.517669          7.434628   \n3                AADAC          4.000000          2.584963          1.584963   \n4              AADACL2          1.000000          1.000000          0.000000   \n5              AADACL3          0.000000          0.000000          0.000000   \n6              AADACL4          0.000000          0.000000          0.000000   \n7          AB019440.50          0.000000          0.000000          0.000000   \n8          AB019441.29          6.392317          4.954196          6.629357   \n9  ABC12-47964100C23.1          0.000000          0.000000          0.000000   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0          5.247928          5.977280          5.044394          5.491853   \n1         17.760739         14.766839         16.263691         16.035207   \n2          2.584963          1.584963          2.584963          5.285402   \n3          0.000000          0.000000          0.000000          3.321928   \n4          0.000000          1.000000          0.000000          0.000000   \n5          0.000000          1.000000          0.000000          0.000000   \n6          0.000000          0.000000          0.000000          0.000000   \n7          0.000000          0.000000          0.000000          0.000000   \n8          6.988685          8.625709          6.614710          6.845490   \n9          0.000000          0.000000          0.000000          0.000000   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0          5.754888          6.357552  \n1         18.355114         16.959379  \n2          2.584963          3.584963  \n3          1.000000          4.584963  \n4          0.000000          1.000000  \n5          4.169925          0.000000  \n6          0.000000          0.000000  \n7          0.000000          0.000000  \n8          7.845490          6.507795  \n9          1.584963          0.000000  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      2\n5  TCGA-GN-A26A-06A      2\n6  TCGA-D3-A3BZ-06A      2\n7  TCGA-D3-A51G-06A      2\n8  TCGA-EE-A29R-06A      2\n9  TCGA-D3-A2JE-06A      2\n\n\n\n\n\n3.5.3 MACFCmain\n\nranked_features, fre1, frequency, len_FName, FName, Fauc = Tr.MACFCmain(\n    100, \n    \"class\", \n    0.95, \n    data_path='../test_TransProPy/data/all_degs_count_exp.csv', \n    label_path='../test_TransProPy/data/class.csv'\n    )\n\n\n\n\n3.5.4 Result\n\n# Print the first 20 Ranked Features\nprint(\"\\nFirst 20 Ranked Features:\")\nfor i, feature in enumerate(ranked_features[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst 20 Ranked Features:\n1. 1887\n2. 393\n3. 820\n4. 549\n5. 1750\n6. 1328\n7. 569\n8. 2030\n9. 1975\n10. 1400\n11. 261\n12. 376\n13. 171\n14. 885\n15. 1378\n16. 47\n17. 132\n18. 1334\n19. 2091\n20. 135\n\n\n\n\n# Print the first 20 Feature Frequencies (fre1)\nprint(\"\\nFirst 20 Feature Frequencies:\")\nfor i, (feature, freq) in enumerate(list(fre1.items())[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFirst 20 Feature Frequencies:\n1. Feature: 1887, Frequency: 1\n2. Feature: 393, Frequency: 1\n3. Feature: 820, Frequency: 1\n4. Feature: 549, Frequency: 1\n5. Feature: 1750, Frequency: 1\n6. Feature: 1328, Frequency: 1\n7. Feature: 569, Frequency: 1\n8. Feature: 2030, Frequency: 1\n9. Feature: 1975, Frequency: 1\n10. Feature: 1400, Frequency: 1\n11. Feature: 261, Frequency: 1\n12. Feature: 376, Frequency: 1\n13. Feature: 171, Frequency: 1\n14. Feature: 885, Frequency: 1\n15. Feature: 1378, Frequency: 1\n16. Feature: 47, Frequency: 1\n17. Feature: 132, Frequency: 1\n18. Feature: 1334, Frequency: 1\n19. Feature: 2091, Frequency: 1\n20. Feature: 135, Frequency: 1\n\n\n\n\n# Print the Features with a frequency greater than 1 \nprint(\"\\nFeatures with a frequency greater than 1 :\")\nfor i, (feature, freq) in enumerate(frequency[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFeatures with a frequency greater than 1 :\n\n\n\n\n# Print the length of FName (len_FName)\nprint(\"\\nCount of Features with AUC &gt; 0.5 (len_FName):\")\nprint(len_FName)\n\n\nCount of Features with AUC &gt; 0.5 (len_FName):\n1\n\n\n\n\n# Print the first 10 Features with AUC &gt; 0.5 (FName)\nprint(\"\\nFirst few Features with AUC &gt; 0.5:\")\nfor i, feature in enumerate(FName[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst few Features with AUC &gt; 0.5:\n1. 1887\n\n\n\n\n# Print the first 10 AUC Values for Ranked Features (Fauc)\nprint(\"\\nFirst few AUC Values for Ranked Features:\")\nfor i, auc in enumerate(Fauc[:20], 1):\n    print(f\"{i}. AUC: {auc}\")\n\n\nFirst few AUC Values for Ranked Features:\n1. AUC: 1.0\n\n\n\n\n\n3.5.5 gene_name\n\ngene_names = TUG.gene_name(data_path='../test_TransProPy/data/all_degs_count_exp.csv')\n\n\n\n# Print the first 20 gene names\nprint(\"First 20 Gene Names:\")\nfor i, gene_name in enumerate(gene_names[:20], 1):\n    print(f\"{i}. {gene_name}\")\n\nFirst 20 Gene Names:\n1. A1BG\n2. A2M\n3. A2ML1\n4. AADAC\n5. AADACL2\n6. AADACL3\n7. AADACL4\n8. AB019440.50\n9. AB019441.29\n10. ABC12-47964100C23.1\n11. ABC12-49244600F4.4\n12. ABCA10\n13. ABCA12\n14. ABCA17P\n15. ABCA6\n16. ABCA8\n17. ABCA9\n18. ABCB11\n19. ABCB4\n20. ABCB5\n\n\n\n\n\n3.5.6 gene_map_feature\n\ngene_to_feature_mapping = TUGM.gene_map_feature(gene_names, ranked_features)\n\n\n\n3.5.6.1 AUC&gt;0.5\n\nimport numpy as np\n# Generating gene_to_feature_mapping\ngene_to_feature_mapping = {}\nfor gene, feature in zip(gene_names, FName):\n    # Find the index of the feature in FName\n    index = np.where(FName == feature)[0][0]\n    # Find the corresponding AUC value using the index\n    auc_value = Fauc[index]\n    # Store the gene name, feature name, and AUC value in the mapping\n    gene_to_feature_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 mappings\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_to_feature_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: A1BG, Feature: 1887, AUC: 1.0"
  },
  {
    "objectID": "MACFCmain.html#usage-of-macfcmain-insignificant-correlation",
    "href": "MACFCmain.html#usage-of-macfcmain-insignificant-correlation",
    "title": "3  MACFCmain.py",
    "section": "3.6 Usage of MACFCmain (Insignificant correlation)",
    "text": "3.6 Usage of MACFCmain (Insignificant correlation)\nThis function uses the MACFC method to select feature genes relevant to classification and ranks them based on their corresponding weights.\n\n\nPlease note:Data characteristics: Features have weak correlation with the classification.\nRandomly shuffling the class labels to a certain extent simulates reducing the correlation.\n\n\n\n\n3.6.1 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/all_degs_count_exp.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n            Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0                 A1BG          6.754888          4.000000          5.044394   \n1                  A2M         16.808499         16.506184         17.143433   \n2                A2ML1          1.584963          9.517669          7.434628   \n3                AADAC          4.000000          2.584963          1.584963   \n4              AADACL2          1.000000          1.000000          0.000000   \n5              AADACL3          0.000000          0.000000          0.000000   \n6              AADACL4          0.000000          0.000000          0.000000   \n7          AB019440.50          0.000000          0.000000          0.000000   \n8          AB019441.29          6.392317          4.954196          6.629357   \n9  ABC12-47964100C23.1          0.000000          0.000000          0.000000   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0          5.247928          5.977280          5.044394          5.491853   \n1         17.760739         14.766839         16.263691         16.035207   \n2          2.584963          1.584963          2.584963          5.285402   \n3          0.000000          0.000000          0.000000          3.321928   \n4          0.000000          1.000000          0.000000          0.000000   \n5          0.000000          1.000000          0.000000          0.000000   \n6          0.000000          0.000000          0.000000          0.000000   \n7          0.000000          0.000000          0.000000          0.000000   \n8          6.988685          8.625709          6.614710          6.845490   \n9          0.000000          0.000000          0.000000          0.000000   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0          5.754888          6.357552  \n1         18.355114         16.959379  \n2          2.584963          3.584963  \n3          1.000000          4.584963  \n4          0.000000          1.000000  \n5          4.169925          0.000000  \n6          0.000000          0.000000  \n7          0.000000          0.000000  \n8          7.845490          6.507795  \n9          1.584963          0.000000  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/random_classification_class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      1\n5  TCGA-GN-A26A-06A      1\n6  TCGA-D3-A3BZ-06A      1\n7  TCGA-D3-A51G-06A      1\n8  TCGA-EE-A29R-06A      1\n9  TCGA-D3-A2JE-06A      1\n\n\n\n\n\n3.6.2 MACFCmain\n\nranked_features, fre1, frequency, len_FName, FName, Fauc = Tr.MACFCmain(\n    100, \n    \"class\", \n    0.95, \n    data_path='../test_TransProPy/data/all_degs_count_exp.csv', \n    label_path='../test_TransProPy/data/random_classification_class.csv'\n    )\n\n\n\n\n3.6.3 Result\n\n# Print the first 20 Ranked Features\nprint(\"\\nFirst 20 Ranked Features:\")\nfor i, feature in enumerate(ranked_features[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst 20 Ranked Features:\n1. 1922\n2. 977\n3. 1379\n4. 850\n5. 868\n6. 513\n7. 717\n8. 319\n9. 2031\n10. 971\n11. 1447\n12. 1066\n13. 49\n14. 1215\n15. 401\n16. 1145\n17. 1199\n18. 1775\n19. 644\n20. 454\n\n\n\n\n# Print the first 20 Feature Frequencies (fre1)\nprint(\"\\nFirst 20 Feature Frequencies:\")\nfor i, (feature, freq) in enumerate(list(fre1.items())[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFirst 20 Feature Frequencies:\n1. Feature: 1922, Frequency: 1\n2. Feature: 977, Frequency: 1\n3. Feature: 1379, Frequency: 1\n4. Feature: 850, Frequency: 1\n5. Feature: 868, Frequency: 1\n6. Feature: 513, Frequency: 1\n7. Feature: 717, Frequency: 1\n8. Feature: 319, Frequency: 1\n9. Feature: 2031, Frequency: 1\n10. Feature: 971, Frequency: 1\n11. Feature: 1447, Frequency: 1\n12. Feature: 1066, Frequency: 1\n13. Feature: 49, Frequency: 1\n14. Feature: 1215, Frequency: 1\n15. Feature: 401, Frequency: 1\n16. Feature: 1145, Frequency: 1\n17. Feature: 1199, Frequency: 1\n18. Feature: 1775, Frequency: 1\n19. Feature: 644, Frequency: 1\n20. Feature: 454, Frequency: 1\n\n\n\n\n# Print the Features with a frequency greater than 1 \nprint(\"\\nFeatures with a frequency greater than 1 :\")\nfor i, (feature, freq) in enumerate(frequency[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFeatures with a frequency greater than 1 :\n\n\n\n\n# Print the length of FName (len_FName)\nprint(\"\\nCount of Features with AUC &gt; 0.5 (len_FName):\")\nprint(len_FName)\n\n\nCount of Features with AUC &gt; 0.5 (len_FName):\n1273\n\n\n\n\n# Print the first 10 Features with AUC &gt; 0.5 (FName)\nprint(\"\\nFirst few Features with AUC &gt; 0.5:\")\nfor i, feature in enumerate(FName[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst few Features with AUC &gt; 0.5:\n1. 1922\n2. 977\n3. 1797\n4. 1512\n5. 461\n6. 850\n7. 502\n8. 885\n9. 1349\n10. 1120\n11. 2162\n12. 1573\n13. 2103\n14. 1\n15. 1234\n16. 1809\n17. 326\n18. 1376\n19. 583\n20. 2024\n\n\n\n\n# Print the first 10 AUC Values for Ranked Features (Fauc)\nprint(\"\\nFirst few AUC Values for Ranked Features:\")\nfor i, auc in enumerate(Fauc[:20], 1):\n    print(f\"{i}. AUC: {auc}\")\n\n\nFirst few AUC Values for Ranked Features:\n1. AUC: 0.6462149635841977\n2. AUC: 0.6447607837367272\n3. AUC: 0.6423551337698326\n4. AUC: 0.6423232546163467\n5. AUC: 0.6420535079330048\n6. AUC: 0.6418009269476936\n7. AUC: 0.6417690477942078\n8. AUC: 0.641631722209961\n9. AUC: 0.6410456362343363\n10. AUC: 0.6398661075553593\n11. AUC: 0.6396233355403517\n12. AUC: 0.6392040020598838\n13. AUC: 0.6386375340248658\n14. AUC: 0.6381740601780328\n15. AUC: 0.6379778807719659\n16. AUC: 0.6375045979548297\n17. AUC: 0.6372323990289119\n18. AUC: 0.6372323990289119\n19. AUC: 0.6371661884793645\n20. AUC: 0.6368032565781407\n\n\n\n\n\n3.6.4 gene_name\n\ngene_names = TUG.gene_name(data_path='../test_TransProPy/data/all_degs_count_exp.csv')\n\n\n\n# Print the first 20 gene names\nprint(\"First 20 Gene Names:\")\nfor i, gene_name in enumerate(gene_names[:20], 1):\n    print(f\"{i}. {gene_name}\")\n\nFirst 20 Gene Names:\n1. A1BG\n2. A2M\n3. A2ML1\n4. AADAC\n5. AADACL2\n6. AADACL3\n7. AADACL4\n8. AB019440.50\n9. AB019441.29\n10. ABC12-47964100C23.1\n11. ABC12-49244600F4.4\n12. ABCA10\n13. ABCA12\n14. ABCA17P\n15. ABCA6\n16. ABCA8\n17. ABCA9\n18. ABCB11\n19. ABCB4\n20. ABCB5\n\n\n\n\n\n3.6.5 gene_map_feature\n\ngene_to_feature_mapping = TUGM.gene_map_feature(gene_names, ranked_features)\n\n\n\n3.6.5.1 AUC&gt;0.5\n\nimport numpy as np\n# Generating gene_to_feature_mapping\ngene_to_feature_mapping = {}\nfor gene, feature in zip(gene_names, FName):\n    # Find the index of the feature in FName\n    index = np.where(FName == feature)[0][0]\n    # Find the corresponding AUC value using the index\n    auc_value = Fauc[index]\n    # Store the gene name, feature name, and AUC value in the mapping\n    gene_to_feature_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 mappings\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_to_feature_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: A1BG, Feature: 1922, AUC: 0.6462149635841977\n2. Gene: A2M, Feature: 977, AUC: 0.6447607837367272\n3. Gene: A2ML1, Feature: 1797, AUC: 0.6423551337698326\n4. Gene: AADAC, Feature: 1512, AUC: 0.6423232546163467\n5. Gene: AADACL2, Feature: 461, AUC: 0.6420535079330048\n6. Gene: AADACL3, Feature: 850, AUC: 0.6418009269476936\n7. Gene: AADACL4, Feature: 502, AUC: 0.6417690477942078\n8. Gene: AB019440.50, Feature: 885, AUC: 0.641631722209961\n9. Gene: AB019441.29, Feature: 1349, AUC: 0.6410456362343363\n10. Gene: ABC12-47964100C23.1, Feature: 1120, AUC: 0.6398661075553593\n11. Gene: ABC12-49244600F4.4, Feature: 2162, AUC: 0.6396233355403517\n12. Gene: ABCA10, Feature: 1573, AUC: 0.6392040020598838\n13. Gene: ABCA12, Feature: 2103, AUC: 0.6386375340248658\n14. Gene: ABCA17P, Feature: 1, AUC: 0.6381740601780328\n15. Gene: ABCA6, Feature: 1234, AUC: 0.6379778807719659\n16. Gene: ABCA8, Feature: 1809, AUC: 0.6375045979548297\n17. Gene: ABCA9, Feature: 326, AUC: 0.6372323990289119\n18. Gene: ABCB11, Feature: 1376, AUC: 0.6372323990289119\n19. Gene: ABCB4, Feature: 583, AUC: 0.6371661884793645\n20. Gene: ABCB5, Feature: 2024, AUC: 0.6368032565781407"
  },
  {
    "objectID": "MACFCmain.html#references",
    "href": "MACFCmain.html#references",
    "title": "3  MACFCmain.py",
    "section": "3.7 References",
    "text": "3.7 References\n\n\nSu,Y., Du,K., Wang,J., Wei,J. and Liu,J. (2022) Multi-variable AUC for sifting complementary features and its biomedical application. Briefings in Bioinformatics, 23, bbac029."
  },
  {
    "objectID": "NewMACFCmain.html#parameters",
    "href": "NewMACFCmain.html#parameters",
    "title": "4  NewMACFCmain.py",
    "section": "4.1 Parameters",
    "text": "4.1 Parameters\n\n\nAUC_threshold: float\n\nAUC threshold for feature selection. Features with AUC values higher than this threshold are recorded but not used in subsequent calculations.\n\nmax_rank: int\n\nThe total number of gene combinations you want to obtain.\n\nlable_name: string\n\nFor example: gender, age, altitude, temperature, quality, and other categorical variable names.\n\nthreshold: float\n\nFor example: 0.9\nThe set threshold indicates the proportion of non-zero value samples to all samples in each feature.\n\ndata_path: string\n\nFor example: ‘../data/gene_tpm.csv’\nPlease note: Preprocess the input data in advance to remove samples that contain too many missing values or zeros.\nThe input data matrix should have genes as rows and samples as columns.\n\nlabel_path: string\n\nFor example: ‘../data/tumor_class.csv’\nPlease note: The input sample categories must be in a numerical binary format, such as: 1,2,1,1,2,2,1.\nIn this case, the numerical values represent the following classifications: 1: male; 2: female."
  },
  {
    "objectID": "NewMACFCmain.html#returns",
    "href": "NewMACFCmain.html#returns",
    "title": "4  NewMACFCmain.py",
    "section": "4.2 Returns",
    "text": "4.2 Returns\n\n\nhigh_auc_features: list of tuples\n\nThis list contains tuples of feature indices and their corresponding AUC values, where the AUC value is greater than AUC_threshold. Each tuple consists of the feature’s index in string format and its AUC value as a float. This signifies that these features are highly predictive, with a strong ability to distinguish between different classes in the classification task.\n\nfr: list of strings\n\nRepresenting ranked features.\n\nfre1: dictionary\n\nFeature names as keys and their frequencies as values.\n\nfrequency: list of tuples\n\nFeature names and their frequencies.\nThe frequency outputs a list sorted by occurrence frequency (in descending order). This list includes only those elements from the dictionary fre1 (which represents the counted frequencies of elements in the original data) that have an occurrence frequency greater than once, along with their frequencies.\n\nlen(FName): integer\n\nCount of AUC values greater than 0.5.\n\nFName: array of strings\n\nFeature names after ranking with AUC &gt; 0.5.\n\nFauc: array of floats\n\nAUC values corresponding to the ranked feature names."
  },
  {
    "objectID": "NewMACFCmain.html#function-principle-explanation",
    "href": "NewMACFCmain.html#function-principle-explanation",
    "title": "4  NewMACFCmain.py",
    "section": "4.3 Function Principle Explanation",
    "text": "4.3 Function Principle Explanation\n\nFeature Frequency and AUC: In this function, features that appear with high frequency indicate their presence in multiple optimal feature sets. Each optimal feature set is determined by calculating its Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), which is a common measure for evaluating classifier performance. During each iteration of the loop, an optimal feature set with the highest average AUC value is selected. Features from this set are then added to a rank list, known as ‘ranklist,’ and when necessary, also to a set named ‘rankset’.\nHigh-Frequency Features and Performance: Because features in each set are chosen based on their contribution to classifier performance, high-frequency features are likely to perform well. In other words, if a feature appears in multiple optimal feature sets, it may have a significant impact on the performance of the classifier.\nNote on Low-Frequency Features: However, it’s important to note that a low frequency of a feature does not necessarily mean it is unimportant. The importance of a feature may depend on how it combines with other features. Additionally, the outcome of feature selection may be influenced by the characteristics of the dataset and random factors. Therefore, the frequency provided by this function should only be used as a reference and is not an absolute indicator of feature performance.\nFurther Evaluation Methods: If you wish to explore feature performance more deeply, you may need to employ other methods for assessing feature importance. This could include model-based importance metrics or statistical tests to evaluate the relationship between features and the target variable."
  },
  {
    "objectID": "NewMACFCmain.html#usage-workflow",
    "href": "NewMACFCmain.html#usage-workflow",
    "title": "4  NewMACFCmain.py",
    "section": "4.4 Usage Workflow",
    "text": "4.4 Usage Workflow\n\n\nFName is a list of feature names sorted based on their AUC (Area Under the Curve) values. In this sorting method, the primary consideration is the AUC value, followed by the feature name. All features included in FName have an AUC value greater than 0.5.\nfr is the result of another sorting method. In this method, the primary consideration is the “combined” AUC of the features, followed by their individual AUC values. This means that some features, despite having lower individual AUC values, may produce a higher combined AUC when paired with other features. Therefore, their position in the fr list may be higher than in the FName list.\nThe code for fr employs a more complex logic to select and combine features to optimize their combined AUC values. In this process, features are not solely selected and sorted based on their individual AUC values; the effect of their combination with other features is also considered. Consequently, the sorting logic for fr (or rankset) differs from that of FName.\nPlease note: While the code takes into account both individual AUC values and combined AUC values, the sorting of the fr list (i.e., rankset) initially starts based on individual AUC values. This is because at the beginning of each external loop iteration, the first element of fs is the next feature sorted by its individual AUC value. The list is then further optimized by evaluating the combination effects with other features."
  },
  {
    "objectID": "NewMACFCmain.html#usage-of-new_macfcmain-all_degs_count_exp",
    "href": "NewMACFCmain.html#usage-of-new_macfcmain-all_degs_count_exp",
    "title": "4  NewMACFCmain.py",
    "section": "4.5 Usage of New_MACFCmain (all_degs_count_exp)",
    "text": "4.5 Usage of New_MACFCmain (all_degs_count_exp)\nThis function uses the MACFC method to select feature genes relevant to classification and ranks them based on their corresponding weights.\n\n4.5.1 Import the corresponding module\n\nimport TransProPy.NewMACFCmain as TN\nimport TransProPy.UtilsFunction1.GeneNames as TUG\nimport TransProPy.UtilsFunction1.GeneToFeatureMapping as TUGM\n\n\n\n\n4.5.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/all_degs_count_exp.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n            Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0                 A1BG          6.754888          4.000000          5.044394   \n1                  A2M         16.808499         16.506184         17.143433   \n2                A2ML1          1.584963          9.517669          7.434628   \n3                AADAC          4.000000          2.584963          1.584963   \n4              AADACL2          1.000000          1.000000          0.000000   \n5              AADACL3          0.000000          0.000000          0.000000   \n6              AADACL4          0.000000          0.000000          0.000000   \n7          AB019440.50          0.000000          0.000000          0.000000   \n8          AB019441.29          6.392317          4.954196          6.629357   \n9  ABC12-47964100C23.1          0.000000          0.000000          0.000000   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0          5.247928          5.977280          5.044394          5.491853   \n1         17.760739         14.766839         16.263691         16.035207   \n2          2.584963          1.584963          2.584963          5.285402   \n3          0.000000          0.000000          0.000000          3.321928   \n4          0.000000          1.000000          0.000000          0.000000   \n5          0.000000          1.000000          0.000000          0.000000   \n6          0.000000          0.000000          0.000000          0.000000   \n7          0.000000          0.000000          0.000000          0.000000   \n8          6.988685          8.625709          6.614710          6.845490   \n9          0.000000          0.000000          0.000000          0.000000   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0          5.754888          6.357552  \n1         18.355114         16.959379  \n2          2.584963          3.584963  \n3          1.000000          4.584963  \n4          0.000000          1.000000  \n5          4.169925          0.000000  \n6          0.000000          0.000000  \n7          0.000000          0.000000  \n8          7.845490          6.507795  \n9          1.584963          0.000000  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      2\n5  TCGA-GN-A26A-06A      2\n6  TCGA-D3-A3BZ-06A      2\n7  TCGA-D3-A51G-06A      2\n8  TCGA-EE-A29R-06A      2\n9  TCGA-D3-A2JE-06A      2\n\n\n\n\n\n4.5.3 New_MACFCmain\n\nhigh_auc_features, ranked_features, fre1, frequency, len_FName, FName, Fauc = TN.New_MACFCmain(\n    0.9,\n    100, \n    \"class\", \n    0.9, \n    data_path='../test_TransProPy/data/all_degs_count_exp.csv', \n    label_path='../test_TransProPy/data/class.csv'\n    )\n\n\n\n\n4.5.4 Result\n\n4.5.4.1 AUC greater than 0.9 and their AUC values\n\n# Print features with AUC greater than 0.9 and their AUC values\nprint('\\nFeatures with AUC greater than 0.9:')\ntotal_features = len(high_auc_features)\nprint(f\"Total features: {total_features}\")\n\n# Determine the number of features to display\nnum_to_display = min(total_features, 20)\n\nfor i in range(num_to_display):\n    feature, auc_value = high_auc_features[i]\n    print(f\"Feature: {feature}, AUC: {auc_value}\")\n\n\nFeatures with AUC greater than 0.9:\nTotal features: 1421\nFeature: 26, AUC: 1.0\nFeature: 704, AUC: 1.0\nFeature: 717, AUC: 1.0\nFeature: 1172, AUC: 1.0\nFeature: 1899, AUC: 1.0\nFeature: 1948, AUC: 1.0\nFeature: 2338, AUC: 1.0\nFeature: 2596, AUC: 0.9999973764986752\nFeature: 582, AUC: 0.9999947529973503\nFeature: 2453, AUC: 0.9999895059947005\nFeature: 1563, AUC: 0.9999868824933756\nFeature: 786, AUC: 0.9999842589920508\nFeature: 2419, AUC: 0.9999842589920508\nFeature: 204, AUC: 0.9999763884880762\nFeature: 1002, AUC: 0.9999763884880762\nFeature: 291, AUC: 0.9999658944827767\nFeature: 237, AUC: 0.9999370359682032\nFeature: 124, AUC: 0.9999239184615788\nFeature: 1561, AUC: 0.9999081774536296\nFeature: 2171, AUC: 0.9999081774536296\n\n\n\n\n\n4.5.4.2 New_MACFCmain\n\n# Print the first 20 Ranked Features\nprint(\"\\nFirst 20 Ranked Features:\")\nfor i, feature in enumerate(ranked_features[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst 20 Ranked Features:\n1. 2440\n2. 2460\n3. 2096\n4. 482\n5. 2223\n6. 848\n7. 1501\n8. 519\n9. 1417\n10. 1939\n11. 1914\n12. 937\n13. 1340\n14. 100\n15. 1978\n16. 1558\n17. 413\n18. 1809\n19. 2031\n20. 1466\n\n\n\n\n# Print the first 20 Feature Frequencies (fre1)\nprint(\"\\nFirst 20 Feature Frequencies:\")\nfor i, (feature, freq) in enumerate(list(fre1.items())[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFirst 20 Feature Frequencies:\n1. Feature: 2440, Frequency: 1\n2. Feature: 2460, Frequency: 16\n3. Feature: 2096, Frequency: 26\n4. Feature: 482, Frequency: 1\n5. Feature: 2223, Frequency: 13\n6. Feature: 848, Frequency: 1\n7. Feature: 1501, Frequency: 1\n8. Feature: 519, Frequency: 1\n9. Feature: 1417, Frequency: 2\n10. Feature: 1939, Frequency: 1\n11. Feature: 1914, Frequency: 1\n12. Feature: 937, Frequency: 1\n13. Feature: 1340, Frequency: 1\n14. Feature: 100, Frequency: 1\n15. Feature: 1978, Frequency: 1\n16. Feature: 1558, Frequency: 1\n17. Feature: 413, Frequency: 1\n18. Feature: 1809, Frequency: 1\n19. Feature: 2031, Frequency: 1\n20. Feature: 1466, Frequency: 11\n\n\n\n\n# Print the Features with a frequency greater than 1 \nprint(\"\\nFeatures with a frequency greater than 1 :\")\nfor i, (feature, freq) in enumerate(frequency[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFeatures with a frequency greater than 1 :\n1. Feature: 2096, Frequency: 26\n2. Feature: 2460, Frequency: 16\n3. Feature: 2223, Frequency: 13\n4. Feature: 1466, Frequency: 11\n5. Feature: 1773, Frequency: 3\n6. Feature: 900, Frequency: 2\n7. Feature: 2620, Frequency: 2\n8. Feature: 176, Frequency: 2\n9. Feature: 1417, Frequency: 2\n10. Feature: 1136, Frequency: 2\n11. Feature: 1080, Frequency: 2\n\n\n\n\n# Print the length of FName (len_FName)\nprint(\"\\nCount of Features with AUC &gt; 0.5 (len_FName):\")\nprint(len_FName)\n\n\nCount of Features with AUC &gt; 0.5 (len_FName):\n809\n\n\n\n\n# Print the first 10 Features with AUC &gt; 0.5 (FName)\nprint(\"\\nFirst few Features with AUC &gt; 0.5:\")\nfor i, feature in enumerate(FName[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst few Features with AUC &gt; 0.5:\n1. 2440\n2. 482\n3. 848\n4. 1501\n5. 519\n6. 1939\n7. 1914\n8. 937\n9. 1340\n10. 100\n11. 1978\n12. 1558\n13. 413\n14. 1809\n15. 2031\n16. 780\n17. 712\n18. 1362\n19. 1136\n20. 2486\n\n\n\n\n# Print the first 10 AUC Values for Ranked Features (Fauc)\nprint(\"\\nFirst few AUC Values for Ranked Features:\")\nfor i, auc in enumerate(Fauc[:20], 1):\n    print(f\"{i}. AUC: {auc}\")\n\n\nFirst few AUC Values for Ranked Features:\n1. AUC: 0.8999134244562793\n2. AUC: 0.8998950599470052\n3. AUC: 0.899879318939056\n4. AUC: 0.8995041582496\n5. AUC: 0.8994700527323767\n6. AUC: 0.8994595587270772\n7. AUC: 0.8992864076396359\n8. AUC: 0.8992785371356613\n9. AUC: 0.8990555395230475\n10. AUC: 0.8990424220164231\n11. AUC: 0.8989715874806516\n12. AUC: 0.8988351654117586\n13. AUC: 0.8988325419104337\n14. AUC: 0.8987433428653882\n15. AUC: 0.8987276018574389\n16. AUC: 0.8987249783561141\n17. AUC: 0.8986725083296168\n18. AUC: 0.8985649447752971\n19. AUC: 0.8985150982501247\n20. AUC: 0.898449510717003\n\n\n\n\n\n\n4.5.5 gene_name\n\ngene_names = TUG.gene_name(data_path='../test_TransProPy/data/all_degs_count_exp.csv')\n\n\n\n# Print the first 20 gene names\nprint(\"First 20 Gene Names:\")\nfor i, gene_name in enumerate(gene_names[:20], 1):\n    print(f\"{i}. {gene_name}\")\n\nFirst 20 Gene Names:\n1. A1BG\n2. A2M\n3. A2ML1\n4. AADAC\n5. AADACL2\n6. AADACL3\n7. AADACL4\n8. AB019440.50\n9. AB019441.29\n10. ABC12-47964100C23.1\n11. ABC12-49244600F4.4\n12. ABCA10\n13. ABCA12\n14. ABCA17P\n15. ABCA6\n16. ABCA8\n17. ABCA9\n18. ABCB11\n19. ABCB4\n20. ABCB5\n\n\n\n\n\n4.5.6 gene_map_feature\n\n4.5.6.1 high_auc_features_result(AUC&gt;0.9)\n\n# Extract feature indices from high_auc_features\nhigh_ranked_features = [feature for feature, auc_value in high_auc_features]\n\n# Utilize the TUGM.gene_map_feature function\ngene_to_feature_mapping_0_9 = TUGM.gene_map_feature(gene_names, high_ranked_features)\n\n# Creating a dictionary to store gene, feature, and AUC mapping\ngene_feature_auc_mapping = {}\n\n# Iterate over each gene and its corresponding feature\nfor gene, feature in gene_to_feature_mapping_0_9.items():\n    feature = str(feature)  # Adjust this based on your data format\n    # Find the corresponding AUC value for the feature\n    auc_value = next((auc for feat, auc in high_auc_features if str(feat) == feature), None)\n    # Store the gene, feature, and AUC in the mapping\n    gene_feature_auc_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 gene to feature mappings along with AUC values\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_feature_auc_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: ABCD1, Feature: 26, AUC: 1.0\n2. Gene: ANGPTL5, Feature: 704, AUC: 1.0\n3. Gene: ANKRD20A10P, Feature: 717, AUC: 1.0\n4. Gene: CAPN8, Feature: 1172, AUC: 1.0\n5. Gene: CTD-2340E1.2, Feature: 1899, AUC: 1.0\n6. Gene: CTD-2562J17.2, Feature: 1948, AUC: 1.0\n7. Gene: EPGN, Feature: 2338, AUC: 1.0\n8. Gene: FOSB, Feature: 2596, AUC: 0.9999973764986752\n9. Gene: AF064858.8, Feature: 582, AUC: 0.9999947529973503\n10. Gene: FAM229A, Feature: 2453, AUC: 0.9999895059947005\n11. Gene: COL9A1, Feature: 1563, AUC: 0.9999868824933756\n12. Gene: AP001107.1, Feature: 786, AUC: 0.9999842589920508\n13. Gene: FAM155B, Feature: 2419, AUC: 0.9999842589920508\n14. Gene: AC010547.9, Feature: 204, AUC: 0.9999763884880762\n15. Gene: bP-21201H5.1, Feature: 1002, AUC: 0.9999763884880762\n16. Gene: AC023590.1, Feature: 291, AUC: 0.9999658944827767\n17. Gene: AC012512.1, Feature: 237, AUC: 0.9999370359682032\n18. Gene: AC006486.10, Feature: 124, AUC: 0.9999239184615788\n19. Gene: COL7A1, Feature: 1561, AUC: 0.9999081774536296\n20. Gene: DNAH2, Feature: 2171, AUC: 0.9999081774536296\n\n\n\n\n\n4.5.6.2 NewMACFCmain_result(0.9&gt;AUC&gt;0.5)\n\nimport numpy as np\n# Generating gene_to_feature_mapping\ngene_to_feature_mapping = {}\nfor gene, feature in zip(gene_names, FName):\n    # Find the index of the feature in FName\n    index = np.where(FName == feature)[0][0]\n    # Find the corresponding AUC value using the index\n    auc_value = Fauc[index]\n    # Store the gene name, feature name, and AUC value in the mapping\n    gene_to_feature_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 mappings\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_to_feature_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: A1BG, Feature: 2440, AUC: 0.8999134244562793\n2. Gene: A2M, Feature: 482, AUC: 0.8998950599470052\n3. Gene: A2ML1, Feature: 848, AUC: 0.899879318939056\n4. Gene: AADAC, Feature: 1501, AUC: 0.8995041582496\n5. Gene: AADACL2, Feature: 519, AUC: 0.8994700527323767\n6. Gene: AADACL3, Feature: 1939, AUC: 0.8994595587270772\n7. Gene: AADACL4, Feature: 1914, AUC: 0.8992864076396359\n8. Gene: AB019440.50, Feature: 937, AUC: 0.8992785371356613\n9. Gene: AB019441.29, Feature: 1340, AUC: 0.8990555395230475\n10. Gene: ABC12-47964100C23.1, Feature: 100, AUC: 0.8990424220164231\n11. Gene: ABC12-49244600F4.4, Feature: 1978, AUC: 0.8989715874806516\n12. Gene: ABCA10, Feature: 1558, AUC: 0.8988351654117586\n13. Gene: ABCA12, Feature: 413, AUC: 0.8988325419104337\n14. Gene: ABCA17P, Feature: 1809, AUC: 0.8987433428653882\n15. Gene: ABCA6, Feature: 2031, AUC: 0.8987276018574389\n16. Gene: ABCA8, Feature: 780, AUC: 0.8987249783561141\n17. Gene: ABCA9, Feature: 712, AUC: 0.8986725083296168\n18. Gene: ABCB11, Feature: 1362, AUC: 0.8985649447752971\n19. Gene: ABCB4, Feature: 1136, AUC: 0.8985150982501247\n20. Gene: ABCB5, Feature: 2486, AUC: 0.898449510717003\n\n\n\n\n\n\n4.5.7 Save Data\n\n4.5.7.1 high_auc_features_result (AUC&gt;0.9)\n\nimport pandas as pd\n# Convert gene_feature_auc_mapping to a DataFrame\ngene_feature_auc_df = pd.DataFrame.from_dict(gene_feature_auc_mapping, orient='index', columns=['Feature', 'AUC'])\n\n# Reset the index to make gene names a separate column\ngene_feature_auc_df.reset_index(inplace=True)\ngene_feature_auc_df.rename(columns={'index': 'Gene'}, inplace=True)\n\n# Save the DataFrame to a CSV file\ngene_feature_auc_df.to_csv('../test_TransProPy/data/all_degs_count_exp_gene_feature_auc_mapping_0.9.csv', index=False)\n\n\n\n\n4.5.7.2 NewMACFCmain_result (0.9&gt;AUC&gt;0.5)\n\nimport pandas as pd\n# Convert gene_to_feature_mapping to a DataFrame\ngene_to_feature_df = pd.DataFrame.from_dict(gene_to_feature_mapping, orient='index', columns=['Feature', 'AUC'])\n\n# Reset the index to make gene names a separate column\ngene_to_feature_df.reset_index(inplace=True)\ngene_to_feature_df.rename(columns={'index': 'Gene'}, inplace=True)\n\n# Save the DataFrame to a CSV file\ngene_to_feature_df.to_csv('../test_TransProPy/data/all_degs_count_exp_gene_feature_auc_mapping_0.5.csv', index=False)"
  },
  {
    "objectID": "NewMACFCmain.html#usage-of-new_macfcmain-all_count_exp",
    "href": "NewMACFCmain.html#usage-of-new_macfcmain-all_count_exp",
    "title": "4  NewMACFCmain.py",
    "section": "4.6 Usage of New_MACFCmain (all_count_exp)",
    "text": "4.6 Usage of New_MACFCmain (all_count_exp)\nThis function uses the MACFC method to select feature genes relevant to classification and ranks them based on their corresponding weights.\n\n4.6.1 Import the corresponding module\n\nimport TransProPy.NewMACFCmain as TN\nimport TransProPy.UtilsFunction1.GeneNames as TUG\nimport TransProPy.UtilsFunction1.GeneToFeatureMapping as TUGM\n\n\n\n\n4.6.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/all_count_exp.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n  Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0  5_8S_rRNA                 0                 0                 0   \n1    5S_rRNA                 0                 0                 0   \n2        7SK                 0                 0                 0   \n3       A1BG               107                15                32   \n4   A1BG-AS1               373               112               363   \n5       A1CF                10                 0                 0   \n6        A2M            114778             93079            144772   \n7    A2M-AS1               538               140                50   \n8      A2ML1                 2               732               172   \n9  A2ML1-AS1                 0                 0                 0   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0                 0                 0                 0                 0   \n1                 0                 0                 0                 0   \n2                 0                 0                 0                 0   \n3                37                62                32                44   \n4               347               222               114               225   \n5                 1                 3                 0                 0   \n6            222082             27877             78678             67154   \n7               211                28               332               136   \n8                 5                 2                 5                38   \n9                 0                 3                 0                 0   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0                 0                 0  \n1                 0                 0  \n2                 0                 0  \n3                53                81  \n4               284               396  \n5                 0                 1  \n6            335304            127432  \n7                54               225  \n8                 5                11  \n9                 0                 1  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      2\n5  TCGA-GN-A26A-06A      2\n6  TCGA-D3-A3BZ-06A      2\n7  TCGA-D3-A51G-06A      2\n8  TCGA-EE-A29R-06A      2\n9  TCGA-D3-A2JE-06A      2\n\n\n\n\n\n4.6.3 New_MACFCmain\n\nhigh_auc_features, ranked_features, fre1, frequency, len_FName, FName, Fauc = TN.New_MACFCmain(\n    0.9,\n    100, \n    \"class\", \n    0.9, \n    data_path='../test_TransProPy/data/all_count_exp.csv', \n    label_path='../test_TransProPy/data/class.csv'\n    )\n\n\n\n\n4.6.4 Result\n\n4.6.4.1 AUC greater than 0.9 and their AUC values\n\n# Print features with AUC greater than 0.9 and their AUC values\nprint('\\nFeatures with AUC greater than 0.9:')\ntotal_features = len(high_auc_features)\nprint(f\"Total features: {total_features}\")\n\n# Determine the number of features to display\nnum_to_display = min(total_features, 20)\n\nfor i in range(num_to_display):\n    feature, auc_value = high_auc_features[i]\n    print(f\"Feature: {feature}, AUC: {auc_value}\")\n\n\nFeatures with AUC greater than 0.9:\nTotal features: 4903\nFeature: 129, AUC: 1.0\nFeature: 4701, AUC: 1.0\nFeature: 4788, AUC: 1.0\nFeature: 7317, AUC: 1.0\nFeature: 12536, AUC: 1.0\nFeature: 12784, AUC: 1.0\nFeature: 15361, AUC: 1.0\nFeature: 17372, AUC: 0.9999973764986752\nFeature: 3731, AUC: 0.9999947529973503\nFeature: 16018, AUC: 0.9999921294960255\nFeature: 16355, AUC: 0.9999895059947005\nFeature: 9932, AUC: 0.9999868824933756\nFeature: 5222, AUC: 0.9999842589920508\nFeature: 1242, AUC: 0.9999763884880762\nFeature: 6418, AUC: 0.9999763884880762\nFeature: 1915, AUC: 0.9999658944827767\nFeature: 1491, AUC: 0.9999370359682032\nFeature: 724, AUC: 0.9999239184615788\nFeature: 9930, AUC: 0.9999081774536296\nFeature: 14212, AUC: 0.9999081774536296\n\n\n\n\n\n4.6.4.2 New_MACFCmain\n\n# Print the first 20 Ranked Features\nprint(\"\\nFirst 20 Ranked Features:\")\nfor i, feature in enumerate(ranked_features[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst 20 Ranked Features:\n1. 15431\n2. 13741\n3. 6034\n4. 12954\n5. 16231\n6. 14367\n7. 10409\n8. 8777\n9. 4031\n10. 5595\n11. 3066\n12. 15959\n13. 3436\n14. 12999\n15. 15583\n16. 6533\n17. 173\n18. 6943\n19. 8390\n20. 5298\n\n\n\n\n# Print the first 20 Feature Frequencies (fre1)\nprint(\"\\nFirst 20 Feature Frequencies:\")\nfor i, (feature, freq) in enumerate(list(fre1.items())[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFirst 20 Feature Frequencies:\n1. Feature: 15431, Frequency: 1\n2. Feature: 13741, Frequency: 62\n3. Feature: 6034, Frequency: 51\n4. Feature: 12954, Frequency: 1\n5. Feature: 16231, Frequency: 2\n6. Feature: 14367, Frequency: 12\n7. Feature: 10409, Frequency: 2\n8. Feature: 8777, Frequency: 1\n9. Feature: 4031, Frequency: 3\n10. Feature: 5595, Frequency: 1\n11. Feature: 3066, Frequency: 1\n12. Feature: 15959, Frequency: 1\n13. Feature: 3436, Frequency: 1\n14. Feature: 12999, Frequency: 1\n15. Feature: 15583, Frequency: 2\n16. Feature: 6533, Frequency: 4\n17. Feature: 173, Frequency: 2\n18. Feature: 6943, Frequency: 2\n19. Feature: 8390, Frequency: 2\n20. Feature: 5298, Frequency: 1\n\n\n\n\n# Print the Features with a frequency greater than 1 \nprint(\"\\nFeatures with a frequency greater than 1 :\")\nfor i, (feature, freq) in enumerate(frequency[:20], 1):\n    print(f\"{i}. Feature: {feature}, Frequency: {freq}\")\n\n\nFeatures with a frequency greater than 1 :\n1. Feature: 13741, Frequency: 62\n2. Feature: 6034, Frequency: 51\n3. Feature: 14367, Frequency: 12\n4. Feature: 10844, Frequency: 8\n5. Feature: 16078, Frequency: 7\n6. Feature: 6533, Frequency: 4\n7. Feature: 4031, Frequency: 3\n8. Feature: 8390, Frequency: 2\n9. Feature: 6943, Frequency: 2\n10. Feature: 586, Frequency: 2\n11. Feature: 5249, Frequency: 2\n12. Feature: 173, Frequency: 2\n13. Feature: 16231, Frequency: 2\n14. Feature: 16178, Frequency: 2\n15. Feature: 15583, Frequency: 2\n16. Feature: 10409, Frequency: 2\n\n\n\n\n# Print the length of FName (len_FName)\nprint(\"\\nCount of Features with AUC &gt; 0.5 (len_FName):\")\nprint(len_FName)\n\n\nCount of Features with AUC &gt; 0.5 (len_FName):\n7726\n\n\n\n\n# Print the first 10 Features with AUC &gt; 0.5 (FName)\nprint(\"\\nFirst few Features with AUC &gt; 0.5:\")\nfor i, feature in enumerate(FName[:20], 1):\n    print(f\"{i}. {feature}\")\n\n\nFirst few Features with AUC &gt; 0.5:\n1. 15431\n2. 12954\n3. 16231\n4. 8777\n5. 5595\n6. 3066\n7. 15959\n8. 3436\n9. 12999\n10. 17823\n11. 17634\n12. 14506\n13. 2797\n14. 5249\n15. 17163\n16. 10844\n17. 16206\n18. 5614\n19. 5193\n20. 4601\n\n\n\n\n# Print the first 10 AUC Values for Ranked Features (Fauc)\nprint(\"\\nFirst few AUC Values for Ranked Features:\")\nfor i, auc in enumerate(Fauc[:20], 1):\n    print(f\"{i}. AUC: {auc}\")\n\n\nFirst few AUC Values for Ranked Features:\n1. AUC: 0.8999763884880762\n2. AUC: 0.8999685179841016\n3. AUC: 0.8999658944827768\n4. AUC: 0.8999580239788021\n5. AUC: 0.8998924364456804\n6. AUC: 0.8998740719364063\n7. AUC: 0.899860954429782\n8. AUC: 0.8998583309284571\n9. AUC: 0.8998557074271323\n10. AUC: 0.8998058609019598\n11. AUC: 0.8997953668966603\n12. AUC: 0.8997560143767872\n13. AUC: 0.8997035443502899\n14. AUC: 0.8996930503449905\n15. AUC: 0.8996878033423407\n16. AUC: 0.8996510743237925\n17. AUC: 0.8996064748012698\n18. AUC: 0.8996064748012698\n19. AUC: 0.8995933572946454\n20. AUC: 0.8995382637668232\n\n\n\n\n\n\n4.6.5 gene_name\n\ngene_names = TUG.gene_name(data_path='../test_TransProPy/data/all_count_exp.csv')\n\n\n\n# Print the first 20 gene names\nprint(\"First 20 Gene Names:\")\nfor i, gene_name in enumerate(gene_names[:20], 1):\n    print(f\"{i}. {gene_name}\")\n\nFirst 20 Gene Names:\n1. 5_8S_rRNA\n2. 5S_rRNA\n3. 7SK\n4. A1BG\n5. A1BG-AS1\n6. A1CF\n7. A2M\n8. A2M-AS1\n9. A2ML1\n10. A2ML1-AS1\n11. A2ML1-AS2\n12. A2MP1\n13. A3GALT2\n14. A4GALT\n15. A4GNT\n16. AA06\n17. AAAS\n18. AACS\n19. AACSP1\n20. AADAC\n\n\n\n\n\n4.6.6 gene_map_feature\n\n4.6.6.1 high_auc_features_result (AUC&gt;0.9)\n\n# Extract feature indices from high_auc_features\nhigh_ranked_features = [feature for feature, auc_value in high_auc_features]\n\n# Utilize the TUGM.gene_map_feature function\ngene_to_feature_mapping_0_9 = TUGM.gene_map_feature(gene_names, high_ranked_features)\n\n# Creating a dictionary to store gene, feature, and AUC mapping\ngene_feature_auc_mapping = {}\n\n# Iterate over each gene and its corresponding feature\nfor gene, feature in gene_to_feature_mapping_0_9.items():\n    feature = str(feature)  # Adjust this based on your data format\n    # Find the corresponding AUC value for the feature\n    auc_value = next((auc for feat, auc in high_auc_features if str(feat) == feature), None)\n    # Store the gene, feature, and AUC in the mapping\n    gene_feature_auc_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 gene to feature mappings along with AUC values\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_feature_auc_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: ABHD15, Feature: 129, AUC: 1.0\n2. Gene: AF064858.11, Feature: 4701, AUC: 1.0\n3. Gene: AFF4, Feature: 4788, AUC: 1.0\n4. Gene: AY269186.1, Feature: 7317, AUC: 1.0\n5. Gene: CTD-2530H12.5, Feature: 12536, AUC: 1.0\n6. Gene: CTD-2650P22.2, Feature: 12784, AUC: 1.0\n7. Gene: FAM197Y8, Feature: 15361, AUC: 1.0\n8. Gene: GS1-25M2.1, Feature: 17372, AUC: 0.9999973764986752\n9. Gene: AC107016.1, Feature: 3731, AUC: 0.9999947529973503\n10. Gene: FLJ42102, Feature: 16018, AUC: 0.9999921294960255\n11. Gene: FUT9, Feature: 16355, AUC: 0.9999895059947005\n12. Gene: CICP26, Feature: 9932, AUC: 0.9999868824933756\n13. Gene: AL133475.1, Feature: 5222, AUC: 0.9999842589920508\n14. Gene: AC008703.2, Feature: 1242, AUC: 0.9999763884880762\n15. Gene: AP001464.4, Feature: 6418, AUC: 0.9999763884880762\n16. Gene: AC016561.1, Feature: 1915, AUC: 0.9999658944827767\n17. Gene: AC010148.1, Feature: 1491, AUC: 0.9999370359682032\n18. Gene: AC005822.1, Feature: 724, AUC: 0.9999239184615788\n19. Gene: CICP23, Feature: 9930, AUC: 0.9999081774536296\n20. Gene: DUSP12P1, Feature: 14212, AUC: 0.9999081774536296\n\n\n\n\n\n4.6.6.2 NewMACFCmain_result (0.9&gt;AUC&gt;0.5)\n\nimport numpy as np\n# Generating gene_to_feature_mapping\ngene_to_feature_mapping = {}\nfor gene, feature in zip(gene_names, FName):\n    # Find the index of the feature in FName\n    index = np.where(FName == feature)[0][0]\n    # Find the corresponding AUC value using the index\n    auc_value = Fauc[index]\n    # Store the gene name, feature name, and AUC value in the mapping\n    gene_to_feature_mapping[gene] = (feature, auc_value)\n\n# Print the first 20 mappings\nprint(\"\\nFirst 20 Gene to Feature Mappings with AUC Values:\")\nfor i, (gene, (feature, auc)) in enumerate(list(gene_to_feature_mapping.items())[:20], 1):\n    print(f\"{i}. Gene: {gene}, Feature: {feature}, AUC: {auc}\")\n\n\nFirst 20 Gene to Feature Mappings with AUC Values:\n1. Gene: 5_8S_rRNA, Feature: 15431, AUC: 0.8999763884880762\n2. Gene: 5S_rRNA, Feature: 12954, AUC: 0.8999685179841016\n3. Gene: 7SK, Feature: 16231, AUC: 0.8999658944827768\n4. Gene: A1BG, Feature: 8777, AUC: 0.8999580239788021\n5. Gene: A1BG-AS1, Feature: 5595, AUC: 0.8998924364456804\n6. Gene: A1CF, Feature: 3066, AUC: 0.8998740719364063\n7. Gene: A2M, Feature: 15959, AUC: 0.899860954429782\n8. Gene: A2M-AS1, Feature: 3436, AUC: 0.8998583309284571\n9. Gene: A2ML1, Feature: 12999, AUC: 0.8998557074271323\n10. Gene: A2ML1-AS1, Feature: 17823, AUC: 0.8998058609019598\n11. Gene: A2ML1-AS2, Feature: 17634, AUC: 0.8997953668966603\n12. Gene: A2MP1, Feature: 14506, AUC: 0.8997560143767872\n13. Gene: A3GALT2, Feature: 2797, AUC: 0.8997035443502899\n14. Gene: A4GALT, Feature: 5249, AUC: 0.8996930503449905\n15. Gene: A4GNT, Feature: 17163, AUC: 0.8996878033423407\n16. Gene: AA06, Feature: 10844, AUC: 0.8996510743237925\n17. Gene: AAAS, Feature: 16206, AUC: 0.8996064748012698\n18. Gene: AACS, Feature: 5614, AUC: 0.8996064748012698\n19. Gene: AACSP1, Feature: 5193, AUC: 0.8995933572946454\n20. Gene: AADAC, Feature: 4601, AUC: 0.8995382637668232\n\n\n\n\n\n\n4.6.7 Save Data\n\n4.6.7.1 high_auc_features_result (AUC&gt;0.9)\n\nimport pandas as pd\n# Convert gene_feature_auc_mapping to a DataFrame\ngene_feature_auc_df = pd.DataFrame.from_dict(gene_feature_auc_mapping, orient='index', columns=['Feature', 'AUC'])\n\n# Reset the index to make gene names a separate column\ngene_feature_auc_df.reset_index(inplace=True)\ngene_feature_auc_df.rename(columns={'index': 'Gene'}, inplace=True)\n\n# Save the DataFrame to a CSV file\ngene_feature_auc_df.to_csv('../test_TransProPy/data/all_count_exp_gene_feature_auc_mapping_0.9.csv', index=False)\n\n\n\n\n4.6.7.2 NewMACFCmain_result (0.9&gt;AUC&gt;0.5)\n\nimport pandas as pd\n# Convert gene_to_feature_mapping to a DataFrame\ngene_to_feature_df = pd.DataFrame.from_dict(gene_to_feature_mapping, orient='index', columns=['Feature', 'AUC'])\n\n# Reset the index to make gene names a separate column\ngene_to_feature_df.reset_index(inplace=True)\ngene_to_feature_df.rename(columns={'index': 'Gene'}, inplace=True)\n\n# Save the DataFrame to a CSV file\ngene_to_feature_df.to_csv('../test_TransProPy/data/all_count_exp_gene_feature_auc_mapping_0.5.csv', index=False)"
  },
  {
    "objectID": "NewMACFCmain.html#references",
    "href": "NewMACFCmain.html#references",
    "title": "4  NewMACFCmain.py",
    "section": "4.7 References",
    "text": "4.7 References\n\n\nSu,Y., Du,K., Wang,J., Wei,J. and Liu,J. (2022) Multi-variable AUC for sifting complementary features and its biomedical application. Briefings in Bioinformatics, 23, bbac029."
  },
  {
    "objectID": "AutogluonTimeLimit.html#parameters",
    "href": "AutogluonTimeLimit.html#parameters",
    "title": "5  AutogluonTimeLimit.py",
    "section": "5.1 Parameters",
    "text": "5.1 Parameters\n\n\ngene_data_path (str):\n\nPath to the gene expression data CSV file.\nFor example: ‘../data/gene_tpm.csv’\n\nclass_data_path (str):\n\nPath to the class data CSV file.\nFor example: ‘../data/tumor_class.csv’\n\nlabel_column (str):\n\nName of the column in the dataset that is the target label for prediction.\n\ntest_size (float):\n\nProportion of the data to be used as the test set.\n\nthreshold (float):\n\nThe threshold used to filter out rows based on the proportion of non-zero values.\n\nrandom_feature (int, optional):\n\nThe number of random feature to select. If None, no random feature selection is performed.\nDefault is None.\n\nnum_bag_folds (int, optional):\n\nPlease note: This parameter annotation source can be referred to the documentation link in References.\nNumber of folds used for bagging of models. When num_bag_folds = k, training time is roughly increased by a factor of k (set = 0 to disable bagging). Disabled by default (0), but we recommend values between 5-10 to maximize predictive performance. Increasing num_bag_folds will result in models with lower bias but that are more prone to overfitting. num_bag_folds = 1 is an invalid value, and will raise a ValueError. Values &gt; 10 may produce diminishing returns, and can even harm overall results due to overfitting. To further improve predictions, avoid increasing num_bag_folds much beyond 10 and instead increase num_bag_sets.\ndefault = None\n\nnum_stack_levels (int, optional):\n\nPlease note: This parameter annotation source can be referred to the documentation link in References.\nNumber of stacking levels to use in stack ensemble. Roughly increases model training time by factor of num_stack_levels+1 (set = 0 to disable stack ensembling). Disabled by default (0), but we recommend values between 1-3 to maximize predictive performance. To prevent overfitting, num_bag_folds &gt;= 2 must also be set or else a ValueError will be raised.\ndefault = None\n\ntime_limit (int, optional):\n\nTime limit for training in seconds.\nDefault is 120.\n\nrandom_state (int, optional):\n\nThe seed used by the random number generator.\nDefault is 42."
  },
  {
    "objectID": "AutogluonTimeLimit.html#returns",
    "href": "AutogluonTimeLimit.html#returns",
    "title": "5  AutogluonTimeLimit.py",
    "section": "5.2 Returns",
    "text": "5.2 Returns\n\n\nimportance (DataFrame):\n\nDataFrame containing feature importance.\n\nleaderboard (DataFrame):\n\nDataFrame containing model performance on the test data."
  },
  {
    "objectID": "AutogluonTimeLimit.html#usage-of-autogluon_timelimit",
    "href": "AutogluonTimeLimit.html#usage-of-autogluon_timelimit",
    "title": "5  AutogluonTimeLimit.py",
    "section": "5.3 Usage of Autogluon_TimeLimit",
    "text": "5.3 Usage of Autogluon_TimeLimit\nPerforming training and prediction tasks on tabular data using Autogluon.\n\n5.3.1 Objectives\n\n5.3.1.1 Model Training and Selection\n\nAutogluon will attempt various models and hyperparameter combinations within a given time limit to find the best-performing model on the test data. During training, Autogluon may output training logs displaying performance metrics and progress information for different models. The goal is to select the best-performing model for use in subsequent prediction tasks.\n\n\n\n5.3.1.2 Leaderboard\n\nThe leaderboard displays performance scores of different models on the test data, typically including metrics like accuracy, precision, recall, and more. The purpose is to assist users in understanding the performance of different models to choose the most suitable model for predictions.\n\n\n\n5.3.1.3 Importance\n\nFeature importance indicates which features are most critical for the model’s prediction performance. The purpose is to help users understand the importance of specific features in the data, which can be used for feature selection or further data analysis.\n\n\n\n\n5.3.2 Note\n\nPlease note that Autogluon’s output results may vary depending on your data and task. You can review the generated model leaderboard and feature importance to understand model performance and the significance of specific features in the data. These results can aid you in making better predictions and decisions."
  },
  {
    "objectID": "AutogluonTimeLimit.html#insignificant-correlation",
    "href": "AutogluonTimeLimit.html#insignificant-correlation",
    "title": "5  AutogluonTimeLimit.py",
    "section": "5.4 Insignificant Correlation",
    "text": "5.4 Insignificant Correlation\n\n\nPlease note:Data characteristics: Features have weak correlation with the classification.\nRandomly shuffling the class labels to a certain extent simulates reducing the correlation.\n\n\n\n5.4.1 Import the corresponding module\n\nfrom TransProPy.AutogluonTimeLimit import Autogluon_TimeLimit\n\n\n\n\n5.4.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/all_degs_count_exp.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n            Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0                 A1BG          6.754888          4.000000          5.044394   \n1                  A2M         16.808499         16.506184         17.143433   \n2                A2ML1          1.584963          9.517669          7.434628   \n3                AADAC          4.000000          2.584963          1.584963   \n4              AADACL2          1.000000          1.000000          0.000000   \n5              AADACL3          0.000000          0.000000          0.000000   \n6              AADACL4          0.000000          0.000000          0.000000   \n7          AB019440.50          0.000000          0.000000          0.000000   \n8          AB019441.29          6.392317          4.954196          6.629357   \n9  ABC12-47964100C23.1          0.000000          0.000000          0.000000   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0          5.247928          5.977280          5.044394          5.491853   \n1         17.760739         14.766839         16.263691         16.035207   \n2          2.584963          1.584963          2.584963          5.285402   \n3          0.000000          0.000000          0.000000          3.321928   \n4          0.000000          1.000000          0.000000          0.000000   \n5          0.000000          1.000000          0.000000          0.000000   \n6          0.000000          0.000000          0.000000          0.000000   \n7          0.000000          0.000000          0.000000          0.000000   \n8          6.988685          8.625709          6.614710          6.845490   \n9          0.000000          0.000000          0.000000          0.000000   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0          5.754888          6.357552  \n1         18.355114         16.959379  \n2          2.584963          3.584963  \n3          1.000000          4.584963  \n4          0.000000          1.000000  \n5          4.169925          0.000000  \n6          0.000000          0.000000  \n7          0.000000          0.000000  \n8          7.845490          6.507795  \n9          1.584963          0.000000  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/random_classification_class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      1\n5  TCGA-GN-A26A-06A      1\n6  TCGA-D3-A3BZ-06A      1\n7  TCGA-D3-A51G-06A      1\n8  TCGA-EE-A29R-06A      1\n9  TCGA-D3-A2JE-06A      1\n\n\n\n\n\n5.4.3 Autogluon_TimeLimit\n\nimportance, leaderboard = Autogluon_TimeLimit(\n    gene_data_path='../test_TransProPy/data/all_degs_count_exp.csv', \n    class_data_path='../test_TransProPy/data/random_classification_class.csv', \n    label_column='class',  \n    test_size=0.3, \n    threshold=0.9, \n    random_feature=None, \n    num_bag_folds=None, \n    num_stack_levels=None, \n    time_limit=1000, \n    random_state=42\n    )\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231214_054821\\\"\n\n\nBeginning AutoGluon training ... Time limit = 1000s\n\n\nAutoGluon will save models to \"AutogluonModels\\ag-20231214_054821\\\"\n\n\nAutoGluon Version:  0.8.2\n\n\nPython Version:     3.10.11\n\n\nOperating System:   Windows\n\n\nPlatform Machine:   AMD64\n\n\nPlatform Version:   10.0.19044\n\n\nDisk Space Avail:   246.10 GB / 925.93 GB (26.6%)\n\n\nTrain Data Rows:    896\n\n\nTrain Data Columns: 2697\n\n\nLabel Column: class\n\n\nPreprocessing data ...\n\n\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\n\n    2 unique label values:  [1, 2]\n\n\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n\n\nSelected class &lt;--&gt; label mapping:  class 1 = 2, class 0 = 1\n\n\n    Note: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (2) vs negative (1) class.\n    To explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n\n\nUsing Feature Generators to preprocess the data ...\n\n\nFitting AutoMLPipelineFeatureGenerator...\n\n\n    Available Memory:                    19727.48 MB\n\n\n    Train Data (Original)  Memory Usage: 19.33 MB (0.1% of available memory)\n\n\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\n\n    Stage 1 Generators:\n\n\n        Fitting AsTypeFeatureGenerator...\n\n\n    Stage 2 Generators:\n\n\n        Fitting FillNaFeatureGenerator...\n\n\n    Stage 3 Generators:\n\n\n        Fitting IdentityFeatureGenerator...\n\n\n    Stage 4 Generators:\n\n\n        Fitting DropUniqueFeatureGenerator...\n\n\n    Stage 5 Generators:\n\n\n        Fitting DropDuplicatesFeatureGenerator...\n\n\n    Types of features in original data (raw dtype, special dtypes):\n\n\n        ('float', []) : 2697 | ['A1BG', 'A2M', 'A2ML1', 'AB019441.29', 'ABCA10', ...]\n\n\n    Types of features in processed data (raw dtype, special dtypes):\n\n\n        ('float', []) : 2697 | ['A1BG', 'A2M', 'A2ML1', 'AB019441.29', 'ABCA10', ...]\n\n\n    2.6s = Fit runtime\n\n\n    2697 features in original data used to generate 2697 features in processed data.\n\n\n    Train Data (Processed) Memory Usage: 19.33 MB (0.1% of available memory)\n\n\nData preprocessing and feature engineering runtime = 2.8s ...\n\n\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\n\n    To change this, specify the eval_metric parameter of Predictor()\n\n\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 716, Val Rows: 180\n\n\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\n\n\nFitting 13 L1 models ...\n\n\nFitting model: KNeighborsUnif ... Training model for up to 997.2s of the 997.17s of remaining time.\n\n\n    0.6  = Validation score   (accuracy)\n\n\n    2.87s    = Training   runtime\n\n\n    0.21s    = Validation runtime\n\n\nFitting model: KNeighborsDist ... Training model for up to 994.08s of the 994.03s of remaining time.\n\n\n    0.6  = Validation score   (accuracy)\n\n\n    0.49s    = Training   runtime\n\n\n    0.06s    = Validation runtime\n\n\nFitting model: LightGBMXT ... Training model for up to 993.48s of the 993.42s of remaining time.\n\n\n    0.6667   = Validation score   (accuracy)\n\n\n    6.29s    = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: LightGBM ... Training model for up to 987.1s of the 987.05s of remaining time.\n\n\n    0.6722   = Validation score   (accuracy)\n\n\n    15.99s   = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: RandomForestGini ... Training model for up to 971.02s of the 970.97s of remaining time.\n\n\n    0.65     = Validation score   (accuracy)\n\n\n    4.97s    = Training   runtime\n\n\n    0.07s    = Validation runtime\n\n\nFitting model: RandomForestEntr ... Training model for up to 965.91s of the 965.85s of remaining time.\n\n\n    0.6389   = Validation score   (accuracy)\n\n\n    5.34s    = Training   runtime\n\n\n    0.06s    = Validation runtime\n\n\nFitting model: CatBoost ... Training model for up to 960.42s of the 960.37s of remaining time.\n\n\n    0.6722   = Validation score   (accuracy)\n\n\n    122.26s  = Training   runtime\n\n\n    0.06s    = Validation runtime\n\n\nFitting model: ExtraTreesGini ... Training model for up to 838.04s of the 837.99s of remaining time.\n\n\n    0.6111   = Validation score   (accuracy)\n\n\n    1.88s    = Training   runtime\n\n\n    0.06s    = Validation runtime\n\n\nFitting model: ExtraTreesEntr ... Training model for up to 836.01s of the 835.96s of remaining time.\n\n\n    0.65     = Validation score   (accuracy)\n\n\n    1.92s    = Training   runtime\n\n\n    0.05s    = Validation runtime\n\n\nFitting model: NeuralNetFastAI ... Training model for up to 833.97s of the 833.92s of remaining time.\n\n\n    0.6778   = Validation score   (accuracy)\n\n\n    4.1s     = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: XGBoost ... Training model for up to 829.71s of the 829.66s of remaining time.\n\n\n    0.6778   = Validation score   (accuracy)\n\n\n    31.65s   = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: NeuralNetTorch ... Training model for up to 797.97s of the 797.92s of remaining time.\n\n\n    0.7056   = Validation score   (accuracy)\n\n\n    60.27s   = Training   runtime\n\n\n    0.16s    = Validation runtime\n\n\nFitting model: LightGBMLarge ... Training model for up to 737.48s of the 737.43s of remaining time.\n\n\n    0.6889   = Validation score   (accuracy)\n\n\n    52.34s   = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 684.89s of remaining time.\n\n\n    0.7444   = Validation score   (accuracy)\n\n\n    0.69s    = Training   runtime\n\n\n    0.0s     = Validation runtime\n\n\nAutoGluon training complete, total runtime = 315.85s ... Best model: \"WeightedEnsemble_L2\"\n\n\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231214_054821\\\")\n\n\nComputing feature importance via permutation shuffling for 2697 features using 385 rows with 5 shuffle sets...\n\n\n    8655.36s    = Expected runtime (1731.07s per shuffle set)\n\n\n    1386.22s    = Actual runtime (Completed 5 of 5 shuffle sets)\n\n\n                  model  score_test  score_val  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2    0.631169   0.744444        0.654799       0.408600  239.843281                 0.003986                0.000997           0.687398            2       True         14\n1       NeuralNetFastAI    0.628571   0.677778        0.052823       0.027496    4.104610                 0.052823                0.027496           4.104610            1       True         10\n2        NeuralNetTorch    0.628571   0.705556        0.151266       0.159752   60.267400                 0.151266                0.159752          60.267400            1       True         12\n3        ExtraTreesGini    0.620779   0.611111        0.075752       0.055813    1.884982                 0.075752                0.055813           1.884982            1       True          8\n4        ExtraTreesEntr    0.618182   0.650000        0.074750       0.052823    1.916070                 0.074750                0.052823           1.916070            1       True          9\n5              LightGBM    0.615584   0.672222        0.025913       0.018937   15.993874                 0.025913                0.018937          15.993874            1       True          4\n6        KNeighborsUnif    0.607792   0.600000        0.077740       0.206310    2.868490                 0.077740                0.206310           2.868490            1       True          1\n7        KNeighborsDist    0.607792   0.600000        0.083721       0.061445    0.486563                 0.083721                0.061445           0.486563            1       True          2\n8      RandomForestEntr    0.605195   0.638889        0.073635       0.062790    5.339290                 0.073635                0.062790           5.339290            1       True          6\n9              CatBoost    0.602597   0.672222        0.226454       0.057837  122.261497                 0.226454                0.057837         122.261497            1       True          7\n10           LightGBMXT    0.597403   0.666667        0.024916       0.018937    6.290214                 0.024916                0.018937           6.290214            1       True          3\n11              XGBoost    0.594805   0.677778        0.048837       0.024916   31.652874                 0.048837                0.024916          31.652874            1       True         11\n12     RandomForestGini    0.594805   0.650000        0.072757       0.067774    4.969003                 0.072757                0.067774           4.969003            1       True          5\n13        LightGBMLarge    0.561039   0.688889        0.038871       0.019933   52.339370                 0.038871                0.019933          52.339370            1       True         13\n\n\n\n\n\n5.4.4 Return\n\n# Filtering the DataFrame to get features where importance &gt; 0 and p_value &lt; 0.05\nfiltered_importance = importance[(importance['importance'] &gt; 0) & (importance['p_value'] &lt; 0.05)]\n# Counting the number of such features\nnumber_of_features = filtered_importance.shape[0]\n# Printing the result\nprint(f\"Number of features with importance &gt; 0 and p_value &lt; 0.05: {number_of_features}\")\n\nNumber of features with importance &gt; 0 and p_value &lt; 0.05: 211\n\n\n\n\n# Display the top 20 rows of the Importance DataFrame\nprint(\"Top 20 rows of Importance DataFrame:\")\nprint(importance.head(20))\n\nTop 20 rows of Importance DataFrame:\n               importance    stddev   p_value  n  p99_high   p99_low\nAIM2             0.007273  0.001162  0.000076  5  0.009664  0.004881\nRP11-15J10.1     0.007273  0.005923  0.025803  5  0.019468 -0.004923\nRP11-20D14.6     0.006234  0.001423  0.000304  5  0.009163  0.003304\nCTAGE5           0.005195  0.003673  0.017055  5  0.012758 -0.002369\nSMIM2-AS1        0.005195  0.001837  0.001599  5  0.008976  0.001413\nADORA3           0.005195  0.004859  0.037565  5  0.015200 -0.004811\nADAMDEC1         0.004675  0.003387  0.018341  5  0.011648 -0.002298\nRP11-244H3.4     0.004675  0.003387  0.018341  5  0.011648 -0.002298\nGALNT4           0.004156  0.002961  0.017460  5  0.010254 -0.001942\nTMEM163          0.004156  0.002323  0.008065  5  0.008939 -0.000628\nRP11-66B24.7     0.004156  0.001423  0.001419  5  0.007085  0.001227\nSULT1C2          0.004156  0.002961  0.017460  5  0.010254 -0.001942\nPDE11A           0.004156  0.002961  0.017460  5  0.010254 -0.001942\nUBE2Q2P6         0.004156  0.001423  0.001419  5  0.007085  0.001227\nLINC01579        0.004156  0.003485  0.028000  5  0.011331 -0.003019\nRP11-378J18.9    0.004156  0.002961  0.017460  5  0.010254 -0.001942\nEXTL1            0.004156  0.001423  0.001419  5  0.007085  0.001227\nRPGRIP1          0.003636  0.003939  0.053969  5  0.011747 -0.004474\nGPR158           0.003636  0.002323  0.012448  5  0.008420 -0.001147\nCOLEC11          0.003636  0.003939  0.053969  5  0.011747 -0.004474\n\n\n\n\n# Display the top 20 rows of the Leaderboard DataFrame\nprint(\"\\nTop 20 rows of Leaderboard DataFrame:\")\nprint(leaderboard.head(20))\n\n\nTop 20 rows of Leaderboard DataFrame:\n                  model  score_test  score_val  pred_time_test  pred_time_val  \\\n0   WeightedEnsemble_L2    0.631169   0.744444        0.654799       0.408600   \n1       NeuralNetFastAI    0.628571   0.677778        0.052823       0.027496   \n2        NeuralNetTorch    0.628571   0.705556        0.151266       0.159752   \n3        ExtraTreesGini    0.620779   0.611111        0.075752       0.055813   \n4        ExtraTreesEntr    0.618182   0.650000        0.074750       0.052823   \n5              LightGBM    0.615584   0.672222        0.025913       0.018937   \n6        KNeighborsUnif    0.607792   0.600000        0.077740       0.206310   \n7        KNeighborsDist    0.607792   0.600000        0.083721       0.061445   \n8      RandomForestEntr    0.605195   0.638889        0.073635       0.062790   \n9              CatBoost    0.602597   0.672222        0.226454       0.057837   \n10           LightGBMXT    0.597403   0.666667        0.024916       0.018937   \n11              XGBoost    0.594805   0.677778        0.048837       0.024916   \n12     RandomForestGini    0.594805   0.650000        0.072757       0.067774   \n13        LightGBMLarge    0.561039   0.688889        0.038871       0.019933   \n\n      fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0   239.843281                 0.003986                0.000997   \n1     4.104610                 0.052823                0.027496   \n2    60.267400                 0.151266                0.159752   \n3     1.884982                 0.075752                0.055813   \n4     1.916070                 0.074750                0.052823   \n5    15.993874                 0.025913                0.018937   \n6     2.868490                 0.077740                0.206310   \n7     0.486563                 0.083721                0.061445   \n8     5.339290                 0.073635                0.062790   \n9   122.261497                 0.226454                0.057837   \n10    6.290214                 0.024916                0.018937   \n11   31.652874                 0.048837                0.024916   \n12    4.969003                 0.072757                0.067774   \n13   52.339370                 0.038871                0.019933   \n\n    fit_time_marginal  stack_level  can_infer  fit_order  \n0            0.687398            2       True         14  \n1            4.104610            1       True         10  \n2           60.267400            1       True         12  \n3            1.884982            1       True          8  \n4            1.916070            1       True          9  \n5           15.993874            1       True          4  \n6            2.868490            1       True          1  \n7            0.486563            1       True          2  \n8            5.339290            1       True          6  \n9          122.261497            1       True          7  \n10           6.290214            1       True          3  \n11          31.652874            1       True         11  \n12           4.969003            1       True          5  \n13          52.339370            1       True         13  \n\n\n\n\n\n5.4.5 Save Data\n\n# Save the Importance DataFrame to a CSV file\nimportance.to_csv('../test_TransProPy/data/Insignificant_correlation_Autogluon_TimeLimit_importance.csv', index=False)\n\n# Save the Leaderboard DataFrame to a CSV file\nleaderboard.to_csv('../test_TransProPy/data/Insignificant_correlation_Autogluon_TimeLimit_leaderboard.csv', index=False)"
  },
  {
    "objectID": "AutogluonTimeLimit.html#significant-correlation",
    "href": "AutogluonTimeLimit.html#significant-correlation",
    "title": "5  AutogluonTimeLimit.py",
    "section": "5.5 Significant Correlation",
    "text": "5.5 Significant Correlation\n\n\nPlease note:Data characteristics: Features have strong correlation with the classification.\n\n\n\n5.5.1 Import the corresponding module\n\nfrom TransProPy.AutogluonTimeLimit import Autogluon_TimeLimit\n\n\n\n\n5.5.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/all_degs_count_exp.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n            Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0                 A1BG          6.754888          4.000000          5.044394   \n1                  A2M         16.808499         16.506184         17.143433   \n2                A2ML1          1.584963          9.517669          7.434628   \n3                AADAC          4.000000          2.584963          1.584963   \n4              AADACL2          1.000000          1.000000          0.000000   \n5              AADACL3          0.000000          0.000000          0.000000   \n6              AADACL4          0.000000          0.000000          0.000000   \n7          AB019440.50          0.000000          0.000000          0.000000   \n8          AB019441.29          6.392317          4.954196          6.629357   \n9  ABC12-47964100C23.1          0.000000          0.000000          0.000000   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0          5.247928          5.977280          5.044394          5.491853   \n1         17.760739         14.766839         16.263691         16.035207   \n2          2.584963          1.584963          2.584963          5.285402   \n3          0.000000          0.000000          0.000000          3.321928   \n4          0.000000          1.000000          0.000000          0.000000   \n5          0.000000          1.000000          0.000000          0.000000   \n6          0.000000          0.000000          0.000000          0.000000   \n7          0.000000          0.000000          0.000000          0.000000   \n8          6.988685          8.625709          6.614710          6.845490   \n9          0.000000          0.000000          0.000000          0.000000   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0          5.754888          6.357552  \n1         18.355114         16.959379  \n2          2.584963          3.584963  \n3          1.000000          4.584963  \n4          0.000000          1.000000  \n5          4.169925          0.000000  \n6          0.000000          0.000000  \n7          0.000000          0.000000  \n8          7.845490          6.507795  \n9          1.584963          0.000000  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      2\n5  TCGA-GN-A26A-06A      2\n6  TCGA-D3-A3BZ-06A      2\n7  TCGA-D3-A51G-06A      2\n8  TCGA-EE-A29R-06A      2\n9  TCGA-D3-A2JE-06A      2\n\n\n\n\n\n5.5.3 Autogluon_TimeLimit\n\nimportance, leaderboard = Autogluon_TimeLimit(\n    gene_data_path='../test_TransProPy/data/all_degs_count_exp.csv', \n    class_data_path='../test_TransProPy/data/class.csv', \n    label_column='class',  \n    test_size=0.3, \n    threshold=0.9, \n    random_feature=None, \n    num_bag_folds=None, \n    num_stack_levels=None, \n    time_limit=1000, \n    random_state=42\n    )\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231214_061648\\\"\n\n\nBeginning AutoGluon training ... Time limit = 1000s\n\n\nAutoGluon will save models to \"AutogluonModels\\ag-20231214_061648\\\"\n\n\nAutoGluon Version:  0.8.2\n\n\nPython Version:     3.10.11\n\n\nOperating System:   Windows\n\n\nPlatform Machine:   AMD64\n\n\nPlatform Version:   10.0.19044\n\n\nDisk Space Avail:   246.09 GB / 925.93 GB (26.6%)\n\n\nTrain Data Rows:    896\n\n\nTrain Data Columns: 2697\n\n\nLabel Column: class\n\n\nPreprocessing data ...\n\n\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\n\n    2 unique label values:  [2, 1]\n\n\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n\n\nSelected class &lt;--&gt; label mapping:  class 1 = 2, class 0 = 1\n\n\n    Note: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (2) vs negative (1) class.\n    To explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n\n\nUsing Feature Generators to preprocess the data ...\n\n\nFitting AutoMLPipelineFeatureGenerator...\n\n\n    Available Memory:                    21956.49 MB\n\n\n    Train Data (Original)  Memory Usage: 19.33 MB (0.1% of available memory)\n\n\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\n\n    Stage 1 Generators:\n\n\n        Fitting AsTypeFeatureGenerator...\n\n\n    Stage 2 Generators:\n\n\n        Fitting FillNaFeatureGenerator...\n\n\n    Stage 3 Generators:\n\n\n        Fitting IdentityFeatureGenerator...\n\n\n    Stage 4 Generators:\n\n\n        Fitting DropUniqueFeatureGenerator...\n\n\n    Stage 5 Generators:\n\n\n        Fitting DropDuplicatesFeatureGenerator...\n\n\n    Types of features in original data (raw dtype, special dtypes):\n\n\n        ('float', []) : 2697 | ['A1BG', 'A2M', 'A2ML1', 'AB019441.29', 'ABCA10', ...]\n\n\n    Types of features in processed data (raw dtype, special dtypes):\n\n\n        ('float', []) : 2697 | ['A1BG', 'A2M', 'A2ML1', 'AB019441.29', 'ABCA10', ...]\n\n\n    4.5s = Fit runtime\n\n\n    2697 features in original data used to generate 2697 features in processed data.\n\n\n    Train Data (Processed) Memory Usage: 19.33 MB (0.1% of available memory)\n\n\nData preprocessing and feature engineering runtime = 4.77s ...\n\n\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\n\n    To change this, specify the eval_metric parameter of Predictor()\n\n\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 716, Val Rows: 180\n\n\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\n\n\nFitting 13 L1 models ...\n\n\nFitting model: KNeighborsUnif ... Training model for up to 995.22s of the 995.17s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.46s    = Training   runtime\n\n\n    0.06s    = Validation runtime\n\n\nFitting model: KNeighborsDist ... Training model for up to 994.64s of the 994.59s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.49s    = Training   runtime\n\n\n    0.09s    = Validation runtime\n\n\nFitting model: LightGBMXT ... Training model for up to 994.01s of the 993.95s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    3.66s    = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: LightGBM ... Training model for up to 990.26s of the 990.21s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    2.85s    = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: RandomForestGini ... Training model for up to 987.33s of the 987.28s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    1.84s    = Training   runtime\n\n\n    0.05s    = Validation runtime\n\n\nFitting model: RandomForestEntr ... Training model for up to 985.37s of the 985.32s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    1.95s    = Training   runtime\n\n\n    0.05s    = Validation runtime\n\n\nFitting model: CatBoost ... Training model for up to 983.29s of the 983.24s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    95.71s   = Training   runtime\n\n\n    0.06s    = Validation runtime\n\n\nFitting model: ExtraTreesGini ... Training model for up to 887.46s of the 887.41s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    1.64s    = Training   runtime\n\n\n    0.05s    = Validation runtime\n\n\nFitting model: ExtraTreesEntr ... Training model for up to 885.7s of the 885.65s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    1.73s    = Training   runtime\n\n\n    0.05s    = Validation runtime\n\n\nFitting model: NeuralNetFastAI ... Training model for up to 883.85s of the 883.8s of remaining time.\n\n\nNo improvement since epoch 0: early stopping\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    2.98s    = Training   runtime\n\n\n    0.03s    = Validation runtime\n\n\nFitting model: XGBoost ... Training model for up to 880.72s of the 880.67s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    7.89s    = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: NeuralNetTorch ... Training model for up to 872.75s of the 872.69s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    31.17s   = Training   runtime\n\n\n    0.18s    = Validation runtime\n\n\nFitting model: LightGBMLarge ... Training model for up to 841.33s of the 841.28s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    3.8s     = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 837.32s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.64s    = Training   runtime\n\n\n    0.0s     = Validation runtime\n\n\nAutoGluon training complete, total runtime = 163.37s ... Best model: \"WeightedEnsemble_L2\"\n\n\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231214_061648\\\")\n\n\nComputing feature importance via permutation shuffling for 2697 features using 385 rows with 5 shuffle sets...\n\n\n    430.24s = Expected runtime (86.05s per shuffle set)\n\n\n    172.95s = Actual runtime (Completed 5 of 5 shuffle sets)\n\n\n                  model  score_test  score_val  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0              LightGBM    1.000000        1.0        0.022921       0.018937   2.848882                 0.022921                0.018937           2.848882            1       True          4\n1         LightGBMLarge    1.000000        1.0        0.023920       0.018937   3.804882                 0.023920                0.018937           3.804882            1       True         13\n2            LightGBMXT    1.000000        1.0        0.023922       0.018937   3.662523                 0.023922                0.018937           3.662523            1       True          3\n3   WeightedEnsemble_L2    1.000000        1.0        0.025913       0.019933   4.446029                 0.001993                0.000997           0.641147            2       True         14\n4       NeuralNetFastAI    1.000000        1.0        0.053153       0.027907   2.980451                 0.053153                0.027907           2.980451            1       True         10\n5      RandomForestGini    1.000000        1.0        0.068770       0.050830   1.835159                 0.068770                0.050830           1.835159            1       True          5\n6        ExtraTreesEntr    1.000000        1.0        0.069370       0.051827   1.727780                 0.069370                0.051827           1.727780            1       True          9\n7      RandomForestEntr    1.000000        1.0        0.069767       0.052823   1.953339                 0.069767                0.052823           1.953339            1       True          6\n8        ExtraTreesGini    1.000000        1.0        0.069767       0.052823   1.635236                 0.069767                0.052823           1.635236            1       True          8\n9        KNeighborsUnif    1.000000        1.0        0.079075       0.056810   0.464058                 0.079075                0.056810           0.464058            1       True          1\n10       KNeighborsDist    1.000000        1.0        0.082723       0.089700   0.488944                 0.082723                0.089700           0.488944            1       True          2\n11             CatBoost    1.000000        1.0        0.132557       0.057807  95.714712                 0.132557                0.057807          95.714712            1       True          7\n12       NeuralNetTorch    1.000000        1.0        0.145193       0.182417  31.166128                 0.145193                0.182417          31.166128            1       True         12\n13              XGBoost    0.994805        1.0        0.046843       0.022924   7.887082                 0.046843                0.022924           7.887082            1       True         11\n\n\n\n\n\n5.5.4 Return\n\n# Filtering the DataFrame to get features where importance &gt; 0 and p_value &lt; 0.05\nfiltered_importance = importance[(importance['importance'] &gt; 0) & (importance['p_value'] &lt; 0.05)]\n# Counting the number of such features\nnumber_of_features = filtered_importance.shape[0]\n# Printing the result\nprint(f\"Number of features with importance &gt; 0 and p_value &lt; 0.05: {number_of_features}\")\n\nNumber of features with importance &gt; 0 and p_value &lt; 0.05: 2\n\n\n\n\n# Display the top 20 rows of the Importance DataFrame\nprint(\"Top 20 rows of Importance DataFrame:\")\nprint(importance.head(20))\n\nTop 20 rows of Importance DataFrame:\n               importance    stddev   p_value  n  p99_high   p99_low\nRP11-231C14.4    0.235844  0.017463  0.000004  5    0.2718  0.199888\nISY1-RAB43       0.235844  0.017463  0.000004  5    0.2718  0.199888\nA1BG             0.000000  0.000000  0.500000  5    0.0000  0.000000\nPTX3             0.000000  0.000000  0.500000  5    0.0000  0.000000\nPVRL1            0.000000  0.000000  0.500000  5    0.0000  0.000000\nPVRL4            0.000000  0.000000  0.500000  5    0.0000  0.000000\nPYCR1            0.000000  0.000000  0.500000  5    0.0000  0.000000\nPYGM             0.000000  0.000000  0.500000  5    0.0000  0.000000\nPYHIN1           0.000000  0.000000  0.500000  5    0.0000  0.000000\nPYURF            0.000000  0.000000  0.500000  5    0.0000  0.000000\nPYY2             0.000000  0.000000  0.500000  5    0.0000  0.000000\nQPCT             0.000000  0.000000  0.500000  5    0.0000  0.000000\nPTRH1            0.000000  0.000000  0.500000  5    0.0000  0.000000\nQPRT             0.000000  0.000000  0.500000  5    0.0000  0.000000\nRAB11FIP1P1      0.000000  0.000000  0.500000  5    0.0000  0.000000\nRAB17            0.000000  0.000000  0.500000  5    0.0000  0.000000\nRAB25            0.000000  0.000000  0.500000  5    0.0000  0.000000\nRAB27A           0.000000  0.000000  0.500000  5    0.0000  0.000000\nRAB27B           0.000000  0.000000  0.500000  5    0.0000  0.000000\nRAB41            0.000000  0.000000  0.500000  5    0.0000  0.000000\n\n\n\n\n# Display the top 20 rows of the Leaderboard DataFrame\nprint(\"\\nTop 20 rows of Leaderboard DataFrame:\")\nprint(leaderboard.head(20))\n\n\nTop 20 rows of Leaderboard DataFrame:\n                  model  score_test  score_val  pred_time_test  pred_time_val  \\\n0              LightGBM    1.000000        1.0        0.022921       0.018937   \n1         LightGBMLarge    1.000000        1.0        0.023920       0.018937   \n2            LightGBMXT    1.000000        1.0        0.023922       0.018937   \n3   WeightedEnsemble_L2    1.000000        1.0        0.025913       0.019933   \n4       NeuralNetFastAI    1.000000        1.0        0.053153       0.027907   \n5      RandomForestGini    1.000000        1.0        0.068770       0.050830   \n6        ExtraTreesEntr    1.000000        1.0        0.069370       0.051827   \n7      RandomForestEntr    1.000000        1.0        0.069767       0.052823   \n8        ExtraTreesGini    1.000000        1.0        0.069767       0.052823   \n9        KNeighborsUnif    1.000000        1.0        0.079075       0.056810   \n10       KNeighborsDist    1.000000        1.0        0.082723       0.089700   \n11             CatBoost    1.000000        1.0        0.132557       0.057807   \n12       NeuralNetTorch    1.000000        1.0        0.145193       0.182417   \n13              XGBoost    0.994805        1.0        0.046843       0.022924   \n\n     fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0    2.848882                 0.022921                0.018937   \n1    3.804882                 0.023920                0.018937   \n2    3.662523                 0.023922                0.018937   \n3    4.446029                 0.001993                0.000997   \n4    2.980451                 0.053153                0.027907   \n5    1.835159                 0.068770                0.050830   \n6    1.727780                 0.069370                0.051827   \n7    1.953339                 0.069767                0.052823   \n8    1.635236                 0.069767                0.052823   \n9    0.464058                 0.079075                0.056810   \n10   0.488944                 0.082723                0.089700   \n11  95.714712                 0.132557                0.057807   \n12  31.166128                 0.145193                0.182417   \n13   7.887082                 0.046843                0.022924   \n\n    fit_time_marginal  stack_level  can_infer  fit_order  \n0            2.848882            1       True          4  \n1            3.804882            1       True         13  \n2            3.662523            1       True          3  \n3            0.641147            2       True         14  \n4            2.980451            1       True         10  \n5            1.835159            1       True          5  \n6            1.727780            1       True          9  \n7            1.953339            1       True          6  \n8            1.635236            1       True          8  \n9            0.464058            1       True          1  \n10           0.488944            1       True          2  \n11          95.714712            1       True          7  \n12          31.166128            1       True         12  \n13           7.887082            1       True         11  \n\n\n\n\n\n5.5.5 Save Data\n\n# Save the Importance DataFrame to a CSV file\nimportance.to_csv('../test_TransProPy/data/significant_correlation_Autogluon_TimeLimit_importance.csv', index=False)\n\n# Save the Leaderboard DataFrame to a CSV file\nleaderboard.to_csv('../test_TransProPy/data/significant_correlation_Autogluon_TimeLimit_leaderboard.csv', index=False)"
  },
  {
    "objectID": "AutogluonTimeLimit.html#references",
    "href": "AutogluonTimeLimit.html#references",
    "title": "5  AutogluonTimeLimit.py",
    "section": "5.6 References",
    "text": "5.6 References\n\n5.6.1 Scientific Publications\n\n\nErickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., & Smola, A. (2020). AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data. arXiv preprint arXiv:2003.06505.\nFakoor, R., Mueller, J., Erickson, N., Chaudhari, P., & Smola, A. J. (2020). Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation. arXiv preprint arXiv:2006.14284. \nShi, X., Mueller, J., Erickson, N., Li, M., & Smola, A. (2021). Multimodal AutoML on Structured Tables with Text Fields. In AutoML@ICML 2021. \n\n\n\n\n5.6.2 Articles\n\n\nPrasanna, S. (2020, March 31). Machine learning with AutoGluon, an open source AutoML library. AWS Open Source Blog. \nSun, Y., Wu, C., Zhang, Z., He, T., Mueller, J., & Zhang, H. (n.d.). (2020). Image classification on Kaggle using AutoGluon. Medium.\nErickson, N., Mueller, J., Zhang, H., & Kamakoti, B. (2019). AutoGluon: Deep Learning AutoML. Medium.\n\n\n\n\n5.6.3 Documentation\n\n\nAutoGluon Predictors – AutoGluon Documentation 0.1.0 documentation"
  },
  {
    "objectID": "AutoGluonSelectML.html#parameters",
    "href": "AutoGluonSelectML.html#parameters",
    "title": "6  AutoGluonSelectML.py",
    "section": "6.1 Parameters",
    "text": "6.1 Parameters\n\n\ngene_data_path (str):\n\nPath to the gene expression data CSV file.\nFor example: ‘../data/gene_tpm.csv’\n\nclass_data_path (str):\n\nPath to the class data CSV file.\nFor example: ‘../data/tumor_class.csv’\n\nlabel_column (str):\n\nName of the column in the dataset that is the target label for prediction.\n\ntest_size (float):\n\nProportion of the data to be used as the test set.\n\nthreshold (float):\n\nThe threshold used to filter out rows based on the proportion of non-zero values.\n\nhyperparameters (dict, optional):\n\nDictionary of hyperparameters for the models.\nFor example: {‘GBM’: {}, ‘RF’: {}}\n\nrandom_feature (int, optional):\n\nThe number of random feature to select. If None, no random feature selection is performed.\nDefault is None.\n\nnum_bag_folds (int, optional)\n\nPlease note: This parameter annotation source can be referred to the documentation link in References.\nNumber of folds used for bagging of models. When num_bag_folds = k, training time is roughly increased by a factor of k (set = 0 to disable bagging). Disabled by default (0), but we recommend values between 5-10 to maximize predictive performance. Increasing num_bag_folds will result in models with lower bias but that are more prone to overfitting. num_bag_folds = 1 is an invalid value, and will raise a ValueError. Values &gt; 10 may produce diminishing returns, and can even harm overall results due to overfitting. To further improve predictions, avoid increasing num_bag_folds much beyond 10 and instead increase num_bag_sets.\ndefault = None\n\nnum_stack_levels (int, optional)\n\nPlease note: This parameter annotation source can be referred to the documentation link in References.\nNumber of stacking levels to use in stack ensemble. Roughly increases model training time by factor of num_stack_levels+1 (set = 0 to disable stack ensembling). Disabled by default (0), but we recommend values between 1-3 to maximize predictive performance. To prevent overfitting, num_bag_folds &gt;= 2 must also be set or else a ValueError will be raised.\ndefault = None\n\ntime_limit (int, optional):\n\nTime limit for training in seconds.\ndefault is 120.\n\nrandom_state (int, optional):\n\nThe seed used by the random number generator.\ndefault is 42."
  },
  {
    "objectID": "AutoGluonSelectML.html#return",
    "href": "AutoGluonSelectML.html#return",
    "title": "6  AutoGluonSelectML.py",
    "section": "6.2 Return",
    "text": "6.2 Return\n\n\nimportance (DataFrame):\n\nDataFrame containing feature importance.\n\nleaderboard (DataFrame):\n\nDataFrame containing model performance on the test data."
  },
  {
    "objectID": "AutoGluonSelectML.html#usage-of-autogluon_selectml",
    "href": "AutoGluonSelectML.html#usage-of-autogluon_selectml",
    "title": "6  AutoGluonSelectML.py",
    "section": "6.3 Usage of Autogluon_SelectML",
    "text": "6.3 Usage of Autogluon_SelectML\nPerforming training and prediction tasks on tabular data using Autogluon.\n\n6.3.1 Objectives\n\n6.3.1.1 Model Training and Selection\n\nAutogluon will attempt various models and hyperparameter combinations within a given time limit to find the best-performing model on the test data. During training, Autogluon may output training logs displaying performance metrics and progress information for different models. The goal is to select the best-performing model for use in subsequent prediction tasks.\n\n\n\n6.3.1.2 Leaderboard\n\nThe leaderboard displays performance scores of different models on the test data, typically including metrics like accuracy, precision, recall, and more. The purpose is to assist users in understanding the performance of different models to choose the most suitable model for predictions.\n\n\n\n6.3.1.3 Importance\n\nFeature importance indicates which features are most critical for the model’s prediction performance. The purpose is to help users understand the importance of specific features in the data, which can be used for feature selection or further data analysis.\n\n\n\n\n6.3.2 Note\n\nPlease note that Autogluon’s output results may vary depending on your data and task. You can review the generated model leaderboard and feature importance to understand model performance and the significance of specific features in the data. These results can aid you in making better predictions and decisions."
  },
  {
    "objectID": "AutoGluonSelectML.html#insignificant-correlation",
    "href": "AutoGluonSelectML.html#insignificant-correlation",
    "title": "6  AutoGluonSelectML.py",
    "section": "6.4 Insignificant Correlation",
    "text": "6.4 Insignificant Correlation\n\n\nPlease note:Data characteristics: Features have weak correlation with the classification.\nRandomly shuffling the class labels to a certain extent simulates reducing the correlation.\n\n\n\n6.4.1 Import the corresponding module\n\nfrom TransProPy.AutogluonSelectML import AutoGluon_SelectML\n\n\n\n\n6.4.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/all_degs_count_exp.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n            Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0                 A1BG          6.754888          4.000000          5.044394   \n1                  A2M         16.808499         16.506184         17.143433   \n2                A2ML1          1.584963          9.517669          7.434628   \n3                AADAC          4.000000          2.584963          1.584963   \n4              AADACL2          1.000000          1.000000          0.000000   \n5              AADACL3          0.000000          0.000000          0.000000   \n6              AADACL4          0.000000          0.000000          0.000000   \n7          AB019440.50          0.000000          0.000000          0.000000   \n8          AB019441.29          6.392317          4.954196          6.629357   \n9  ABC12-47964100C23.1          0.000000          0.000000          0.000000   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0          5.247928          5.977280          5.044394          5.491853   \n1         17.760739         14.766839         16.263691         16.035207   \n2          2.584963          1.584963          2.584963          5.285402   \n3          0.000000          0.000000          0.000000          3.321928   \n4          0.000000          1.000000          0.000000          0.000000   \n5          0.000000          1.000000          0.000000          0.000000   \n6          0.000000          0.000000          0.000000          0.000000   \n7          0.000000          0.000000          0.000000          0.000000   \n8          6.988685          8.625709          6.614710          6.845490   \n9          0.000000          0.000000          0.000000          0.000000   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0          5.754888          6.357552  \n1         18.355114         16.959379  \n2          2.584963          3.584963  \n3          1.000000          4.584963  \n4          0.000000          1.000000  \n5          4.169925          0.000000  \n6          0.000000          0.000000  \n7          0.000000          0.000000  \n8          7.845490          6.507795  \n9          1.584963          0.000000  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/random_classification_class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      1\n5  TCGA-GN-A26A-06A      1\n6  TCGA-D3-A3BZ-06A      1\n7  TCGA-D3-A51G-06A      1\n8  TCGA-EE-A29R-06A      1\n9  TCGA-D3-A2JE-06A      1\n\n\n\n\n\n6.4.3 Autogluon_SelectML\n\n\nThe core purpose of choosing Autogluon_SelectML — to select a larger feature set in AutoGluon that includes both important and secondary features — is reflected in the following custom hyperparameters configuration. This setup is designed to utilize multiple model types so that the models can consider a broader range of features.\nThis configuration encompasses neural networks (using PyTorch and FastAI), gradient boosting machines (LightGBM, XGBoost, and CatBoost), random forests (RF), extremely randomized trees (XT), K-nearest neighbors (KNN), and linear regression (LR).\n\n\n\nimportance, leaderboard = AutoGluon_SelectML(\n    gene_data_path='../test_TransProPy/data/all_degs_count_exp.csv', \n    class_data_path='../test_TransProPy/data/random_classification_class.csv', \n    label_column='class', \n    test_size=0.3, \n    threshold=0.9, \n    hyperparameters={\n        'GBM': {}, \n        'RF': {},\n        'CAT': {}, \n        'XGB' : {},\n        # 'NN_TORCH': {}, \n        # 'FASTAI': {},\n        'XT': {}, \n        'KNN': {},\n        'LR': {}\n        },\n    random_feature=None, \n    num_bag_folds=None, \n    num_stack_levels=None, \n    time_limit=1000, \n    random_state=42\n    )\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231214_062237\\\"\n\n\nBeginning AutoGluon training ... Time limit = 1000s\n\n\nAutoGluon will save models to \"AutogluonModels\\ag-20231214_062237\\\"\n\n\nAutoGluon Version:  0.8.2\n\n\nPython Version:     3.10.11\n\n\nOperating System:   Windows\n\n\nPlatform Machine:   AMD64\n\n\nPlatform Version:   10.0.19044\n\n\nDisk Space Avail:   246.05 GB / 925.93 GB (26.6%)\n\n\nTrain Data Rows:    896\n\n\nTrain Data Columns: 2697\n\n\nLabel Column: class\n\n\nPreprocessing data ...\n\n\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\n\n    2 unique label values:  [1, 2]\n\n\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n\n\nSelected class &lt;--&gt; label mapping:  class 1 = 2, class 0 = 1\n\n\n    Note: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (2) vs negative (1) class.\n    To explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n\n\nUsing Feature Generators to preprocess the data ...\n\n\nFitting AutoMLPipelineFeatureGenerator...\n\n\n    Available Memory:                    22238.13 MB\n\n\n    Train Data (Original)  Memory Usage: 19.33 MB (0.1% of available memory)\n\n\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\n\n    Stage 1 Generators:\n\n\n        Fitting AsTypeFeatureGenerator...\n\n\n    Stage 2 Generators:\n\n\n        Fitting FillNaFeatureGenerator...\n\n\n    Stage 3 Generators:\n\n\n        Fitting IdentityFeatureGenerator...\n\n\n    Stage 4 Generators:\n\n\n        Fitting DropUniqueFeatureGenerator...\n\n\n    Stage 5 Generators:\n\n\n        Fitting DropDuplicatesFeatureGenerator...\n\n\n    Types of features in original data (raw dtype, special dtypes):\n\n\n        ('float', []) : 2697 | ['A1BG', 'A2M', 'A2ML1', 'AB019441.29', 'ABCA10', ...]\n\n\n    Types of features in processed data (raw dtype, special dtypes):\n\n\n        ('float', []) : 2697 | ['A1BG', 'A2M', 'A2ML1', 'AB019441.29', 'ABCA10', ...]\n\n\n    2.6s = Fit runtime\n\n\n    2697 features in original data used to generate 2697 features in processed data.\n\n\n    Train Data (Processed) Memory Usage: 19.33 MB (0.1% of available memory)\n\n\nData preprocessing and feature engineering runtime = 2.83s ...\n\n\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\n\n    To change this, specify the eval_metric parameter of Predictor()\n\n\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 716, Val Rows: 180\n\n\nUser-specified model hyperparameters to be fit:\n{\n    'GBM': {},\n    'RF': {},\n    'CAT': {},\n    'XGB': {},\n    'XT': {},\n    'KNN': {},\n    'LR': {},\n}\n\n\nFitting 7 L1 models ...\n\n\nFitting model: KNeighbors ... Training model for up to 997.17s of the 997.14s of remaining time.\n\n\n    0.6  = Validation score   (accuracy)\n\n\n    1.88s    = Training   runtime\n\n\n    0.21s    = Validation runtime\n\n\nFitting model: LightGBM ... Training model for up to 995.04s of the 994.99s of remaining time.\n\n\n    0.6722   = Validation score   (accuracy)\n\n\n    16.34s   = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: RandomForest ... Training model for up to 978.62s of the 978.56s of remaining time.\n\n\n    0.65     = Validation score   (accuracy)\n\n\n    4.86s    = Training   runtime\n\n\n    0.06s    = Validation runtime\n\n\nFitting model: CatBoost ... Training model for up to 973.62s of the 973.57s of remaining time.\n\n\n    0.6722   = Validation score   (accuracy)\n\n\n    122.27s  = Training   runtime\n\n\n    0.06s    = Validation runtime\n\n\nFitting model: ExtraTrees ... Training model for up to 851.23s of the 851.18s of remaining time.\n\n\n    0.6111   = Validation score   (accuracy)\n\n\n    1.86s    = Training   runtime\n\n\n    0.05s    = Validation runtime\n\n\nFitting model: XGBoost ... Training model for up to 849.24s of the 849.19s of remaining time.\n\n\n    0.6778   = Validation score   (accuracy)\n\n\n    31.63s   = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: LinearModel ... Training model for up to 817.52s of the 817.47s of remaining time.\n\n\nE:\\Anaconda\\Anaconda\\envs\\TransPro\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning:\n\nn_quantiles (1000) is greater than the total number of samples (716). n_quantiles is set to n_samples.\n\n\n\n    0.6611   = Validation score   (accuracy)\n\n\n    3.98s    = Training   runtime\n\n\n    0.13s    = Validation runtime\n\n\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 813.25s of remaining time.\n\n\n    0.7  = Validation score   (accuracy)\n\n\n    0.37s    = Training   runtime\n\n\n    0.0s     = Validation runtime\n\n\nAutoGluon training complete, total runtime = 187.18s ... Best model: \"WeightedEnsemble_L2\"\n\n\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231214_062237\\\")\n\n\nComputing feature importance via permutation shuffling for 2697 features using 385 rows with 5 shuffle sets...\n\n\n    4060.4s = Expected runtime (812.08s per shuffle set)\n\n\n    1365.81s    = Actual runtime (Completed 5 of 5 shuffle sets)\n\n\n                 model  score_test  score_val  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0  WeightedEnsemble_L2    0.623377   0.700000        0.291541       0.229141   57.174159                 0.002990                0.000997           0.372444            2       True          8\n1           ExtraTrees    0.620779   0.611111        0.075747       0.053820    1.861452                 0.075747                0.053820           1.861452            1       True          5\n2             LightGBM    0.615584   0.672222        0.028021       0.018937   16.341586                 0.028021                0.018937          16.341586            1       True          2\n3           KNeighbors    0.607792   0.600000        0.073753       0.211911    1.878197                 0.073753                0.211911           1.878197            1       True          1\n4             CatBoost    0.602597   0.672222        0.135547       0.060796  122.266869                 0.135547                0.060796         122.266869            1       True          4\n5          LinearModel    0.597403   0.661111        0.139934       0.125487    3.979682                 0.139934                0.125487           3.979682            1       True          7\n6              XGBoost    0.594805   0.677778        0.047840       0.023920   31.625202                 0.047840                0.023920          31.625202            1       True          6\n7         RandomForest    0.594805   0.650000        0.072756       0.059800    4.855244                 0.072756                0.059800           4.855244            1       True          3\n\n\n\n\n\n6.4.4 Return\n\n# Filtering the DataFrame to get features where importance &gt; 0 and p_value &lt; 0.05\nfiltered_importance = importance[(importance['importance'] &gt; 0) & (importance['p_value'] &lt; 0.05)]\n# Counting the number of such features\nnumber_of_features = filtered_importance.shape[0]\n# Printing the result\nprint(f\"Number of features with importance &gt; 0 and p_value &lt; 0.05: {number_of_features}\")\n\nNumber of features with importance &gt; 0 and p_value &lt; 0.05: 1147\n\n\n\n\n# Display the top 20 rows of the Importance DataFrame\nprint(\"Top 20 rows of Importance DataFrame:\")\nprint(importance.head(20))\n\nTop 20 rows of Importance DataFrame:\n                importance    stddev   p_value  n  p99_high   p99_low\nRP11-422P24.10    0.019221  0.004346  0.000293  5  0.028170  0.010272\nRP11-495P10.1     0.016104  0.005631  0.001535  5  0.027698  0.004510\nLRRIQ3            0.015584  0.004107  0.000529  5  0.024040  0.007128\nKCNT1             0.014545  0.003939  0.000587  5  0.022656  0.006435\nKY                0.014026  0.004718  0.001330  5  0.023741  0.004311\nFAM129C           0.014026  0.004346  0.000978  5  0.022975  0.005077\nHBA2              0.014026  0.007018  0.005541  5  0.028476 -0.000424\nLGALS4            0.014026  0.006255  0.003709  5  0.026906  0.001146\nIGLV1-51          0.013506  0.008496  0.011850  5  0.031001 -0.003988\nENSAP2            0.012987  0.003673  0.000692  5  0.020550  0.005424\nTMEM182           0.012987  0.002597  0.000182  5  0.018335  0.007639\nVSNL1             0.012987  0.001837  0.000047  5  0.016769  0.009205\nADORA3            0.012468  0.011081  0.032822  5  0.035283 -0.010348\nCTA-253N17.1      0.012468  0.003387  0.000594  5  0.019441  0.005494\nPNMAL1            0.012468  0.003387  0.000594  5  0.019441  0.005494\nC15orf59          0.012468  0.002845  0.000304  5  0.018326  0.006609\nHIST1H2AG         0.012468  0.001162  0.000009  5  0.014859  0.010076\nC3orf67           0.012468  0.002845  0.000304  5  0.018326  0.006609\nAQP7              0.011948  0.006255  0.006470  5  0.024828 -0.000932\nMFSD2B            0.011948  0.002961  0.000418  5  0.018046  0.005850\n\n\n\n\n# Display the top 20 rows of the Leaderboard DataFrame\nprint(\"\\nTop 20 rows of Leaderboard DataFrame:\")\nprint(leaderboard.head(20))\n\n\nTop 20 rows of Leaderboard DataFrame:\n                 model  score_test  score_val  pred_time_test  pred_time_val  \\\n0  WeightedEnsemble_L2    0.623377   0.700000        0.291541       0.229141   \n1           ExtraTrees    0.620779   0.611111        0.075747       0.053820   \n2             LightGBM    0.615584   0.672222        0.028021       0.018937   \n3           KNeighbors    0.607792   0.600000        0.073753       0.211911   \n4             CatBoost    0.602597   0.672222        0.135547       0.060796   \n5          LinearModel    0.597403   0.661111        0.139934       0.125487   \n6              XGBoost    0.594805   0.677778        0.047840       0.023920   \n7         RandomForest    0.594805   0.650000        0.072756       0.059800   \n\n     fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0   57.174159                 0.002990                0.000997   \n1    1.861452                 0.075747                0.053820   \n2   16.341586                 0.028021                0.018937   \n3    1.878197                 0.073753                0.211911   \n4  122.266869                 0.135547                0.060796   \n5    3.979682                 0.139934                0.125487   \n6   31.625202                 0.047840                0.023920   \n7    4.855244                 0.072756                0.059800   \n\n   fit_time_marginal  stack_level  can_infer  fit_order  \n0           0.372444            2       True          8  \n1           1.861452            1       True          5  \n2          16.341586            1       True          2  \n3           1.878197            1       True          1  \n4         122.266869            1       True          4  \n5           3.979682            1       True          7  \n6          31.625202            1       True          6  \n7           4.855244            1       True          3  \n\n\n\n\n\n6.4.5 Save Data\n\n# Save the Importance DataFrame to a CSV file\nimportance.to_csv('../test_TransProPy/data/Insignificant_correlation_Autogluon_SelectML_importance.csv', index=False)\n\n# Save the Leaderboard DataFrame to a CSV file\nleaderboard.to_csv('../test_TransProPy/data/Insignificant_correlation_Autogluon_SelectML_leaderboard.csv', index=False)"
  },
  {
    "objectID": "AutoGluonSelectML.html#significant-correlation",
    "href": "AutoGluonSelectML.html#significant-correlation",
    "title": "6  AutoGluonSelectML.py",
    "section": "6.5 Significant Correlation",
    "text": "6.5 Significant Correlation\n\n\nPlease note:Data characteristics: Features have strong correlation with the classification.\n\n\n\n6.5.1 Import the corresponding module\n\nfrom TransProPy.AutogluonSelectML import AutoGluon_SelectML\n\n\n\n\n6.5.2 Data\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/all_degs_count_exp.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n            Unnamed: 0  TCGA-D9-A4Z2-01A  TCGA-ER-A2NH-06A  TCGA-BF-A5EO-01A  \\\n0                 A1BG          6.754888          4.000000          5.044394   \n1                  A2M         16.808499         16.506184         17.143433   \n2                A2ML1          1.584963          9.517669          7.434628   \n3                AADAC          4.000000          2.584963          1.584963   \n4              AADACL2          1.000000          1.000000          0.000000   \n5              AADACL3          0.000000          0.000000          0.000000   \n6              AADACL4          0.000000          0.000000          0.000000   \n7          AB019440.50          0.000000          0.000000          0.000000   \n8          AB019441.29          6.392317          4.954196          6.629357   \n9  ABC12-47964100C23.1          0.000000          0.000000          0.000000   \n\n   TCGA-D9-A6EA-06A  TCGA-D9-A4Z3-01A  TCGA-GN-A26A-06A  TCGA-D3-A3BZ-06A  \\\n0          5.247928          5.977280          5.044394          5.491853   \n1         17.760739         14.766839         16.263691         16.035207   \n2          2.584963          1.584963          2.584963          5.285402   \n3          0.000000          0.000000          0.000000          3.321928   \n4          0.000000          1.000000          0.000000          0.000000   \n5          0.000000          1.000000          0.000000          0.000000   \n6          0.000000          0.000000          0.000000          0.000000   \n7          0.000000          0.000000          0.000000          0.000000   \n8          6.988685          8.625709          6.614710          6.845490   \n9          0.000000          0.000000          0.000000          0.000000   \n\n   TCGA-D3-A51G-06A  TCGA-EE-A29R-06A  \n0          5.754888          6.357552  \n1         18.355114         16.959379  \n2          2.584963          3.584963  \n3          1.000000          4.584963  \n4          0.000000          1.000000  \n5          4.169925          0.000000  \n6          0.000000          0.000000  \n7          0.000000          0.000000  \n8          7.845490          6.507795  \n9          1.584963          0.000000  \n\n\n\n\nimport pandas as pd\ndata_path = '../test_TransProPy/data/class.csv'  \ndata = pd.read_csv(data_path)\nprint(data.iloc[:10, :10]) \n\n         Unnamed: 0  class\n0  TCGA-D9-A4Z2-01A      2\n1  TCGA-ER-A2NH-06A      2\n2  TCGA-BF-A5EO-01A      2\n3  TCGA-D9-A6EA-06A      2\n4  TCGA-D9-A4Z3-01A      2\n5  TCGA-GN-A26A-06A      2\n6  TCGA-D3-A3BZ-06A      2\n7  TCGA-D3-A51G-06A      2\n8  TCGA-EE-A29R-06A      2\n9  TCGA-D3-A2JE-06A      2\n\n\n\n\n\n6.5.3 Autogluon_SelectML\n\n\nThe core purpose of choosing Autogluon_SelectML — to select a larger feature set in AutoGluon that includes both important and secondary features — is reflected in the following custom hyperparameters configuration. This setup is designed to utilize multiple model types so that the models can consider a broader range of features.\nThis configuration encompasses neural networks (using PyTorch and FastAI), gradient boosting machines (LightGBM, XGBoost, and CatBoost), random forests (RF), extremely randomized trees (XT), K-nearest neighbors (KNN), and linear regression (LR).\n\n\n\nimportance, leaderboard = AutoGluon_SelectML(\n    gene_data_path='../test_TransProPy/data/all_degs_count_exp.csv', \n    class_data_path='../test_TransProPy/data/class.csv', \n    label_column='class', \n    test_size=0.3, \n    threshold=0.9, \n    hyperparameters={\n        'GBM': {}, \n        'RF': {},\n        'CAT': {}, \n        'XGB' : {},\n        # 'NN_TORCH': {}, \n        # 'FASTAI': {},\n        'XT': {}, \n        'KNN': {},\n        'LR': {}\n        },\n    random_feature=None, \n    num_bag_folds=None, \n    num_stack_levels=None, \n    time_limit=1000, \n    random_state=42\n    )\n\nNo path specified. Models will be saved in: \"AutogluonModels\\ag-20231214_064835\\\"\n\n\nBeginning AutoGluon training ... Time limit = 1000s\n\n\nAutoGluon will save models to \"AutogluonModels\\ag-20231214_064835\\\"\n\n\nAutoGluon Version:  0.8.2\n\n\nPython Version:     3.10.11\n\n\nOperating System:   Windows\n\n\nPlatform Machine:   AMD64\n\n\nPlatform Version:   10.0.19044\n\n\nDisk Space Avail:   246.39 GB / 925.93 GB (26.6%)\n\n\nTrain Data Rows:    896\n\n\nTrain Data Columns: 2697\n\n\nLabel Column: class\n\n\nPreprocessing data ...\n\n\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\n\n    2 unique label values:  [2, 1]\n\n\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n\n\nSelected class &lt;--&gt; label mapping:  class 1 = 2, class 0 = 1\n\n\n    Note: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (2) vs negative (1) class.\n    To explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n\n\nUsing Feature Generators to preprocess the data ...\n\n\nFitting AutoMLPipelineFeatureGenerator...\n\n\n    Available Memory:                    22659.34 MB\n\n\n    Train Data (Original)  Memory Usage: 19.33 MB (0.1% of available memory)\n\n\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\n\n    Stage 1 Generators:\n\n\n        Fitting AsTypeFeatureGenerator...\n\n\n    Stage 2 Generators:\n\n\n        Fitting FillNaFeatureGenerator...\n\n\n    Stage 3 Generators:\n\n\n        Fitting IdentityFeatureGenerator...\n\n\n    Stage 4 Generators:\n\n\n        Fitting DropUniqueFeatureGenerator...\n\n\n    Stage 5 Generators:\n\n\n        Fitting DropDuplicatesFeatureGenerator...\n\n\n    Types of features in original data (raw dtype, special dtypes):\n\n\n        ('float', []) : 2697 | ['A1BG', 'A2M', 'A2ML1', 'AB019441.29', 'ABCA10', ...]\n\n\n    Types of features in processed data (raw dtype, special dtypes):\n\n\n        ('float', []) : 2697 | ['A1BG', 'A2M', 'A2ML1', 'AB019441.29', 'ABCA10', ...]\n\n\n    4.5s = Fit runtime\n\n\n    2697 features in original data used to generate 2697 features in processed data.\n\n\n    Train Data (Processed) Memory Usage: 19.33 MB (0.1% of available memory)\n\n\nData preprocessing and feature engineering runtime = 4.81s ...\n\n\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\n\n    To change this, specify the eval_metric parameter of Predictor()\n\n\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 716, Val Rows: 180\n\n\nUser-specified model hyperparameters to be fit:\n{\n    'GBM': {},\n    'RF': {},\n    'CAT': {},\n    'XGB': {},\n    'XT': {},\n    'KNN': {},\n    'LR': {},\n}\n\n\nFitting 7 L1 models ...\n\n\nFitting model: KNeighbors ... Training model for up to 995.18s of the 995.13s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.47s    = Training   runtime\n\n\n    0.05s    = Validation runtime\n\n\nFitting model: LightGBM ... Training model for up to 994.61s of the 994.56s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    2.92s    = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: RandomForest ... Training model for up to 991.61s of the 991.55s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    1.85s    = Training   runtime\n\n\n    0.05s    = Validation runtime\n\n\nFitting model: CatBoost ... Training model for up to 989.64s of the 989.59s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    95.72s   = Training   runtime\n\n\n    0.06s    = Validation runtime\n\n\nFitting model: ExtraTrees ... Training model for up to 893.79s of the 893.74s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    1.68s    = Training   runtime\n\n\n    0.05s    = Validation runtime\n\n\nFitting model: XGBoost ... Training model for up to 891.99s of the 891.94s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    7.73s    = Training   runtime\n\n\n    0.02s    = Validation runtime\n\n\nFitting model: LinearModel ... Training model for up to 884.18s of the 884.12s of remaining time.\n\n\nE:\\Anaconda\\Anaconda\\envs\\TransPro\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2663: UserWarning:\n\nn_quantiles (1000) is greater than the total number of samples (716). n_quantiles is set to n_samples.\n\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    3.56s    = Training   runtime\n\n\n    0.11s    = Validation runtime\n\n\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 880.34s of remaining time.\n\n\n    1.0  = Validation score   (accuracy)\n\n\n    0.38s    = Training   runtime\n\n\n    0.0s     = Validation runtime\n\n\nAutoGluon training complete, total runtime = 120.11s ... Best model: \"WeightedEnsemble_L2\"\n\n\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231214_064835\\\")\n\n\nComputing feature importance via permutation shuffling for 2697 features using 385 rows with 5 shuffle sets...\n\n\n    1196.62s    = Expected runtime (239.32s per shuffle set)\n\n\n    145.56s = Actual runtime (Completed 5 of 5 shuffle sets)\n\n\n                 model  score_test  score_val  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0             LightGBM    1.000000        1.0        0.025913       0.019933   2.920317                 0.025913                0.019933           2.920317            1       True          2\n1         RandomForest    1.000000        1.0        0.068770       0.049833   1.846311                 0.068770                0.049833           1.846311            1       True          3\n2           ExtraTrees    1.000000        1.0        0.069767       0.051603   1.678658                 0.069767                0.051603           1.678658            1       True          5\n3  WeightedEnsemble_L2    1.000000        1.0        0.070763       0.053597   2.058318                 0.000996                0.001994           0.379660            2       True          8\n4           KNeighbors    1.000000        1.0        0.074539       0.049833   0.466127                 0.074539                0.049833           0.466127            1       True          1\n5          LinearModel    1.000000        1.0        0.133068       0.107640   3.559300                 0.133068                0.107640           3.559300            1       True          7\n6             CatBoost    1.000000        1.0        0.218687       0.061146  95.724333                 0.218687                0.061146          95.724333            1       True          4\n7              XGBoost    0.994805        1.0        0.046843       0.021926   7.730929                 0.046843                0.021926           7.730929            1       True          6\n\n\n\n\n\n6.5.4 Return\n\n# Filtering the DataFrame to get features where importance &gt; 0 and p_value &lt; 0.05\nfiltered_importance = importance[(importance['importance'] &gt; 0) & (importance['p_value'] &lt; 0.05)]\n# Counting the number of such features\nnumber_of_features = filtered_importance.shape[0]\n# Printing the result\nprint(f\"Number of features with importance &gt; 0 and p_value &lt; 0.05: {number_of_features}\")\n\nNumber of features with importance &gt; 0 and p_value &lt; 0.05: 0\n\n\n\n\n# Display the top 20 rows of the Importance DataFrame\nprint(\"Top 20 rows of Importance DataFrame:\")\nprint(importance.head(20))\n\nTop 20 rows of Importance DataFrame:\n             importance  stddev  p_value  n  p99_high  p99_low\nA1BG                0.0     0.0      0.5  5       0.0      0.0\nQPCT                0.0     0.0      0.5  5       0.0      0.0\nPTX3                0.0     0.0      0.5  5       0.0      0.0\nPVRL1               0.0     0.0      0.5  5       0.0      0.0\nPVRL4               0.0     0.0      0.5  5       0.0      0.0\nPYCR1               0.0     0.0      0.5  5       0.0      0.0\nPYGM                0.0     0.0      0.5  5       0.0      0.0\nPYHIN1              0.0     0.0      0.5  5       0.0      0.0\nPYURF               0.0     0.0      0.5  5       0.0      0.0\nPYY2                0.0     0.0      0.5  5       0.0      0.0\nQPRT                0.0     0.0      0.5  5       0.0      0.0\nRHPN1-AS1           0.0     0.0      0.5  5       0.0      0.0\nRAB11FIP1P1         0.0     0.0      0.5  5       0.0      0.0\nRAB17               0.0     0.0      0.5  5       0.0      0.0\nRAB25               0.0     0.0      0.5  5       0.0      0.0\nRAB27A              0.0     0.0      0.5  5       0.0      0.0\nRAB27B              0.0     0.0      0.5  5       0.0      0.0\nRAB41               0.0     0.0      0.5  5       0.0      0.0\nRAB42               0.0     0.0      0.5  5       0.0      0.0\nRAB44               0.0     0.0      0.5  5       0.0      0.0\n\n\n\n\n# Display the top 20 rows of the Leaderboard DataFrame\nprint(\"\\nTop 20 rows of Leaderboard DataFrame:\")\nprint(leaderboard.head(20))\n\n\nTop 20 rows of Leaderboard DataFrame:\n                 model  score_test  score_val  pred_time_test  pred_time_val  \\\n0             LightGBM    1.000000        1.0        0.025913       0.019933   \n1         RandomForest    1.000000        1.0        0.068770       0.049833   \n2           ExtraTrees    1.000000        1.0        0.069767       0.051603   \n3  WeightedEnsemble_L2    1.000000        1.0        0.070763       0.053597   \n4           KNeighbors    1.000000        1.0        0.074539       0.049833   \n5          LinearModel    1.000000        1.0        0.133068       0.107640   \n6             CatBoost    1.000000        1.0        0.218687       0.061146   \n7              XGBoost    0.994805        1.0        0.046843       0.021926   \n\n    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0   2.920317                 0.025913                0.019933   \n1   1.846311                 0.068770                0.049833   \n2   1.678658                 0.069767                0.051603   \n3   2.058318                 0.000996                0.001994   \n4   0.466127                 0.074539                0.049833   \n5   3.559300                 0.133068                0.107640   \n6  95.724333                 0.218687                0.061146   \n7   7.730929                 0.046843                0.021926   \n\n   fit_time_marginal  stack_level  can_infer  fit_order  \n0           2.920317            1       True          2  \n1           1.846311            1       True          3  \n2           1.678658            1       True          5  \n3           0.379660            2       True          8  \n4           0.466127            1       True          1  \n5           3.559300            1       True          7  \n6          95.724333            1       True          4  \n7           7.730929            1       True          6  \n\n\n\n\n\n6.5.5 Save Data\n\n# Save the Importance DataFrame to a CSV file\nimportance.to_csv('../test_TransProPy/data/significant_correlation_Autogluon_SelectML_importance.csv', index=False)\n\n# Save the Leaderboard DataFrame to a CSV file\nleaderboard.to_csv('../test_TransProPy/data/significant_correlation_Autogluon_SelectML_leaderboard.csv', index=False)"
  },
  {
    "objectID": "AutoGluonSelectML.html#references",
    "href": "AutoGluonSelectML.html#references",
    "title": "6  AutoGluonSelectML.py",
    "section": "6.6 References",
    "text": "6.6 References\n\n6.6.1 Scientific Publications\n\n\nErickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., & Smola, A. (2020). AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data. arXiv preprint arXiv:2003.06505.\nFakoor, R., Mueller, J., Erickson, N., Chaudhari, P., & Smola, A. J. (2020). Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation. arXiv preprint arXiv:2006.14284. \nShi, X., Mueller, J., Erickson, N., Li, M., & Smola, A. (2021). Multimodal AutoML on Structured Tables with Text Fields. In AutoML@ICML 2021. \n\n\n\n\n6.6.2 Articles\n\n\nPrasanna, S. (2020, March 31). Machine learning with AutoGluon, an open source AutoML library. AWS Open Source Blog. \nSun, Y., Wu, C., Zhang, Z., He, T., Mueller, J., & Zhang, H. (n.d.). (2020). Image classification on Kaggle using AutoGluon. Medium.\nErickson, N., Mueller, J., Zhang, H., & Kamakoti, B. (2019). AutoGluon: Deep Learning AutoML. Medium.\n\n\n\n\n6.6.3 Documentation\n\n\nAutoGluon Predictors –AutoGluon Documentation 0.1.0 documentation"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n\n\n\n\n\nNote\n\n\n\nPolars works fine with Matplotlib, Plotly, and Altair, but it doesn’t work with Seaborn, nor does it have its own .plot method. The latter two are very convenient for exploratory data analysis, so for these examples we’ll just use to_pandas followed by plot or a Seaborn function after doing all the data manipulation in Polars.\n\n\n\n\n\n\n\n\nTip\n\n\n\nPolars works fine with Matplotlib, Plotly, and Altair, but it doesn’t work with Seaborn, nor does it have its own .plot method. The latter two are very convenient for exploratory data analysis, so for these examples we’ll just use to_pandas followed by plot or a Seaborn function after doing all the data manipulation in Polars.\n\n\n\nTab 1Tab 2\n\n\nContent for the first tab.\n\n\nContent for the second tab."
  }
]