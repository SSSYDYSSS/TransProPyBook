# AutoGluonSelectML.py
Trains a model using AutoGluon on provided data path and returns feature importance and model leaderboard.

## Parameters {#parameters-d}

>- <b style="color: #43799a;">gene_data_path</b> (str): 
>   - Path to the gene expression data CSV file. 
>   - <b style="color: #54467d;">For example</b>: '../data/gene_tpm.csv'
>- <b style="color: #43799a;">class_data_path</b> (str):
>   - Path to the class data CSV file. 
>   - <b style="color: #54467d;">For example</b>: '../data/tumor_class.csv'
>- <b style="color: #43799a;">label_column</b> (str): 
>   - Name of the column in the dataset that is the target label for prediction.
>- <b style="color: #43799a;">test_size</b> (float):
>   - Proportion of the data to be used as the test set.
>- <b style="color: #43799a;">threshold</b> (float): 
>   - The threshold used to filter out rows based on the proportion of non-zero values.
>- <b style="color: #43799a;">hyperparameters</b> (dict, <b style="color: #406e1a;">optional</b>): 
>   - Dictionary of hyperparameters for the models. 
>   - <b style="color: #54467d;">For example</b>: {'GBM': {}, 'RF': {}}
>- <b style="color: #43799a;">random_feature</b> (int, <b style="color: #406e1a;">optional</b>): 
>   - The number of random feature to select. If None, no random feature selection is performed. 
>   - <b style="color: #406e1a;">Default is None</b>.
>- <b style="color: #43799a;">num_bag_folds</b> (int, <b style="color: #406e1a;">optional</b>)
>   - <b style="color: #54467d;">Please note</b>: This parameter annotation source can be referred to the documentation link in References. 
>   - Number of folds used for bagging of models. When `num_bag_folds = k`, training time is roughly increased by a factor of `k` (set = 0 to disable bagging). Disabled by default (0), but we recommend values between 5-10 to maximize predictive performance. Increasing `num_bag_folds` will result in models with lower bias but that are more prone to overfitting. `num_bag_folds = 1` is an invalid value, and will raise a ValueError. Values > 10 may produce diminishing returns, and can even harm overall results due to overfitting. To further improve predictions, avoid increasing `num_bag_folds` much beyond 10 and instead increase `num_bag_sets`.
>   - <b style="color: #406e1a;">default = None</b>
>- <b style="color: #43799a;">num_stack_levels</b> (int, <b style="color: #406e1a;">optional</b>)
>   - <b style="color: #54467d;">Please note</b>: This parameter annotation source can be referred to the documentation link in References. 
>   - Number of stacking levels to use in stack ensemble. Roughly increases model training time by factor of `num_stack_levels+1` (set = 0 to disable stack ensembling). Disabled by default (0), but we recommend values between 1-3 to maximize predictive performance. To prevent overfitting, `num_bag_folds >= 2` must also be set or else a ValueError will be raised. 
>   - <b style="color: #406e1a;">default = None</b>
>- <b style="color: #43799a;">time_limit</b> (int, <b style="color: #406e1a;">optional</b>): 
>   - Time limit for training in seconds. 
>   - <b style="color: #406e1a;">default is 120</b>.
>- <b style="color: #43799a;">random_state</b> (int, <b style="color: #406e1a;">optional</b>): 
>   - The seed used by the random number generator. 
>   - <b style="color: #406e1a;">default is 42</b>.

## Return {#returns-d}

>- <b style="color: #43799a;">importance</b> (DataFrame): 
>   - DataFrame containing feature importance.
>- <b style="color: #43799a;">leaderboard</b> (DataFrame):
>   - DataFrame containing model performance on the test data. 

## Usage of Autogluon_SelectML

Performing training and prediction tasks on tabular data using Autogluon.

### Objectives

#### Model Training and Selection

>Autogluon will attempt various models and hyperparameter combinations within a given time limit to find the best-performing model on the test data. During training, Autogluon may output training logs displaying performance metrics and progress information for different models. 
>The goal is to select the best-performing model for use in subsequent prediction tasks.

#### Leaderboard

>The leaderboard displays performance scores of different models on the test data, typically including metrics like accuracy, precision, recall, and more. 
>The purpose is to assist users in understanding the performance of different models to choose the most suitable model for predictions.

#### Importance

>Feature importance indicates which features are most critical for the model's prediction performance. 
>The purpose is to help users understand the importance of specific features in the data, which can be used for feature selection or further data analysis.

### Note

>Please note that Autogluon's output results may vary depending on your data and task. You can review the generated model leaderboard and feature importance to understand model performance and the significance of specific features in the data. These results can aid you in making better predictions and decisions.

## Insignificant Correlation {#insignificant-correlation-d}

>- <b style="color: #54467d;">Please note</b>:Data characteristics: Features have weak correlation with the classification.
>- Randomly shuffling the class labels to a certain extent simulates reducing the correlation.

### Import the corresponding module

```{python}
from TransProPy.AutogluonSelectML import AutoGluon_SelectML
```

---

### Data
```{python}
import pandas as pd
data_path = '../test_TransProPy/data/four_methods_degs_intersection.csv'  
data = pd.read_csv(data_path)
print(data.iloc[:10, :10]) 

```

----

```{python}
import pandas as pd
data_path = '../test_TransProPy/data/random_classification_class.csv'  
data = pd.read_csv(data_path)
print(data.iloc[:10, :10]) 

```

----

### Autogluon_SelectML
>- The core purpose of choosing Autogluon_SelectML — to select a larger feature set in AutoGluon that includes both important and secondary features — is reflected in the following custom hyperparameters configuration. This setup is designed to utilize multiple model types so that the models can consider a broader range of features.
>- This configuration encompasses neural networks (using PyTorch and FastAI), gradient boosting machines (LightGBM, XGBoost, and CatBoost), random forests (RF), extremely randomized trees (XT), K-nearest neighbors (KNN), and linear regression (LR).


```{python}
importance, leaderboard = AutoGluon_SelectML(
    gene_data_path='../test_TransProPy/data/four_methods_degs_intersection.csv', 
    class_data_path='../test_TransProPy/data/random_classification_class.csv', 
    label_column='class', 
    test_size=0.3, 
    threshold=0.9, 
    hyperparameters={
        'GBM': {}, 
        'RF': {},
        'CAT': {}, 
        'XGB' : {},
        # 'NN_TORCH': {}, 
        # 'FASTAI': {},
        'XT': {}, 
        'KNN': {},
        'LR': {}
        },
    random_feature=None, 
    num_bag_folds=None, 
    num_stack_levels=None, 
    time_limit=1000, 
    random_state=42
    )
```

----

### Return

```{python}
# Filtering the DataFrame to get features where importance > 0 and p_value < 0.05
filtered_importance = importance[(importance['importance'] > 0) & (importance['p_value'] < 0.05)]
# Counting the number of such features
number_of_features = filtered_importance.shape[0]
# Printing the result
print(f"Number of features with importance > 0 and p_value < 0.05: {number_of_features}")
```

----

```{python}
# Display the top 20 rows of the Importance DataFrame
print("Top 20 rows of Importance DataFrame:")
print(importance.head(20))
```

----

```{python}
# Display the top 20 rows of the Leaderboard DataFrame
print("\nTop 20 rows of Leaderboard DataFrame:")
print(leaderboard.head(20))
```

----

### Save Data

```{python}
# Save the Importance DataFrame to a CSV file
importance.to_csv('../test_TransProPy/data/Insignificant_correlation_Autogluon_SelectML_importance.csv', index=False)

# Save the Leaderboard DataFrame to a CSV file
leaderboard.to_csv('../test_TransProPy/data/Insignificant_correlation_Autogluon_SelectML_leaderboard.csv', index=False)

```

----

## Significant Correlation {#significant-correlation-d}

>- <b style="color: #54467d;">Please note</b>:Data characteristics: Features have strong correlation with the classification.

### Import the corresponding module

```{python}
from TransProPy.AutogluonSelectML import AutoGluon_SelectML
```

---

### Data
```{python}
import pandas as pd
data_path = '../test_TransProPy/data/four_methods_degs_intersection.csv'  
data = pd.read_csv(data_path)
print(data.iloc[:10, :10]) 

```

----

```{python}
import pandas as pd
data_path = '../test_TransProPy/data/class.csv'  
data = pd.read_csv(data_path)
print(data.iloc[:10, :10]) 

```

----

### Autogluon_SelectML

>- The core purpose of choosing Autogluon_SelectML — to select a larger feature set in AutoGluon that includes both important and secondary features — is reflected in the following custom hyperparameters configuration. This setup is designed to utilize multiple model types so that the models can consider a broader range of features.
>- This configuration encompasses neural networks (using PyTorch and FastAI), gradient boosting machines (LightGBM, XGBoost, and CatBoost), random forests (RF), extremely randomized trees (XT), K-nearest neighbors (KNN), and linear regression (LR).


```{python}
importance, leaderboard = AutoGluon_SelectML(
    gene_data_path='../test_TransProPy/data/four_methods_degs_intersection.csv', 
    class_data_path='../test_TransProPy/data/class.csv', 
    label_column='class', 
    test_size=0.3, 
    threshold=0.9, 
    hyperparameters={
        'GBM': {}, 
        'RF': {},
        'CAT': {}, 
        'XGB' : {},
        # 'NN_TORCH': {}, 
        # 'FASTAI': {},
        'XT': {}, 
        'KNN': {},
        'LR': {}
        },
    random_feature=None, 
    num_bag_folds=None, 
    num_stack_levels=None, 
    time_limit=1000, 
    random_state=42
    )
```

----

### Return

```{python}
# Filtering the DataFrame to get features where importance > 0 and p_value < 0.05
filtered_importance = importance[(importance['importance'] > 0) & (importance['p_value'] < 0.05)]
# Counting the number of such features
number_of_features = filtered_importance.shape[0]
# Printing the result
print(f"Number of features with importance > 0 and p_value < 0.05: {number_of_features}")
```

----

```{python}
# Display the top 20 rows of the Importance DataFrame
print("Top 20 rows of Importance DataFrame:")
print(importance.head(20))
```

----

```{python}
# Display the top 20 rows of the Leaderboard DataFrame
print("\nTop 20 rows of Leaderboard DataFrame:")
print(leaderboard.head(20))
```

----

### Save Data

```{python}
# Save the Importance DataFrame to a CSV file
importance.to_csv('../test_TransProPy/data/significant_correlation_Autogluon_SelectML_importance.csv', index=False)

# Save the Leaderboard DataFrame to a CSV file
leaderboard.to_csv('../test_TransProPy/data/significant_correlation_Autogluon_SelectML_leaderboard.csv', index=False)

```

----

## References {#references-d}


### Scientific Publications

>- <a href="https://arxiv.org/abs/2003.06505" style="color: #43799a; font-weight: bold;">Erickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., & Smola, A. (2020). AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data. arXiv preprint arXiv:2003.06505.</a>
>
>- <a href="https://arxiv.org/abs/2006.14284" style="color: #43799a; font-weight: bold;">Fakoor, R., Mueller, J., Erickson, N., Chaudhari, P., & Smola, A. J. (2020). Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation. arXiv preprint arXiv:2006.14284. </a>
>
>- <a href="https://openreview.net/forum?id=OHAIVOOl7Vl" style="color: #43799a; font-weight: bold;">Shi, X., Mueller, J., Erickson, N., Li, M., & Smola, A. (2021). Multimodal AutoML on Structured Tables with Text Fields. In AutoML@ICML 2021. </a>

### Articles

>- <a href="https://aws.amazon.com/blogs/opensource/machine-learning-with-autogluon-an-open-source-automl-library/" style="color: #43799a; font-weight: bold;">Prasanna, S. (2020, March 31). Machine learning with AutoGluon, an open source AutoML library. AWS Open Source Blog. </a>
>
>- <a href="https://medium.com/@zhanghang0704/image-classification-on-kaggle-using-autogluon-fc896e74d7e8" style="color: #43799a; font-weight: bold;">Sun, Y., Wu, C., Zhang, Z., He, T., Mueller, J., & Zhang, H. (n.d.). (2020). Image classification on Kaggle using AutoGluon. Medium.</a>
>
>- <a href="https://towardsdatascience.com/autogluon-deep-learning-automl-5cdb4e2388ec" style="color: #43799a; font-weight: bold;">Erickson, N., Mueller, J., Zhang, H., & Kamakoti, B. (2019). AutoGluon: Deep Learning AutoML. Medium.</a>


### Documentation

>- <a href="https://auto.gluon.ai/0.1.0/api/autogluon.predictor.html?highlight=num_bag_folds" style="color: #43799a; font-weight: bold;">AutoGluon Predictors --AutoGluon Documentation 0.1.0 documentation</a>

